{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is a coding practice to train a simple MLP for MINST. Aug 7th\n",
        "Motivation: I've written down numerious code snippets since I took ML/DL courses. But never had I ever executed a complete coding task from the first line to the last line of code.\n",
        "\n",
        "The code is written in a explicit manner. I hope it particularly helps new learners."
      ],
      "metadata": {
        "id": "gETEyLruHcU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LBPNUm2h_NUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZViYOg9b_OCr",
        "outputId": "e95630ae-55b4-4467-a47e-20d5eb5ae153"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m41.0/42.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m720.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up environment"
      ],
      "metadata": {
        "id": "STJ0r3DFIhHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets # Import datasets where you may load MNIST\n",
        "import torch.optim as optim # Where you get optimization function\n",
        "import torchvision.transforms as transforms # Where you get those data transformation techniques\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "m3M9yo1uIu_M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ],
      "metadata": {
        "id": "twhUjsRbJI0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the training data\n",
        "\n",
        "mnist_trainset = datasets.MNIST(root='./sample_data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_dataloader = DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "# Loading the test data\n",
        "mnist_testset = datasets.MNIST(root='./sample_data', train=False, download=True, transform=transforms.ToTensor())\n",
        "test_dataloader = DataLoader(mnist_testset, batch_size=64, shuffle=False)\n",
        "\n",
        "'''\n",
        "Learning:\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    # Add other necessary transforms here, e.g., normalization\n",
        "])\n",
        "This transforms image to tensor. (Notice that transforms.Compose can collect other transformations as well)\n",
        "Otherwise it would raise the issue that nn.Module can not process \"PIL image\"\n",
        "'''\n"
      ],
      "metadata": {
        "id": "oQKMENQDJMAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "0f9a54e7-80e4-4d56-dbc7-01c7856e874a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./sample_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 90235642.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./sample_data/MNIST/raw/train-images-idx3-ubyte.gz to ./sample_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./sample_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 76138085.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./sample_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./sample_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./sample_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 23364419.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./sample_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./sample_data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./sample_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3433765.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./sample_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./sample_data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nLearning:\\ntransform = transforms.Compose([\\n    transforms.ToTensor(),  # Convert PIL image to tensor\\n    # Add other necessary transforms here, e.g., normalization\\n])\\nThis transforms image to tensor. (Notice that transforms.Compose can collect other transformations as well)\\nOtherwise it would raise the issue that nn.Module can not process \"PIL image\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing the data (If necessary)\n",
        "\n",
        "\n",
        "\n",
        "*   It should include things such as transformations, which was included when data was loaded\n",
        "\n",
        "\n",
        "*   Viewing data is also good to have.\n",
        "\n"
      ],
      "metadata": {
        "id": "evYE9dZuJRtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing data\n",
        "\n",
        "# Get one batch of data (images and labels)\n",
        "data_iter = iter(train_dataloader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# Plot the images\n",
        "fig = plt.figure(figsize=(25,4))  # The size of the figure\n",
        "for idx in range(20): # Show the first 20 images in the batch\n",
        "  ax = fig.add_subplot(2,10,idx + 1, xticks=[], yticks=[])\n",
        "  ax.imshow(images[idx].squeeze(), cmap='gray') # squeeze() is used to remove any unnecessary dimensions, e.g., (1, 28, 28) -> (28, 28)\n",
        "  ax.set_title(str(labels[idx].item()))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "-YEN68AYdyVe",
        "outputId": "b54ab919-3ad1-41ea-b880-1443d3e31816"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2500x400 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Layer\n",
        "\n",
        "\n",
        "\n",
        "Structuree:\n",
        "\n",
        "*   Fully connected layer 1\n",
        "\n",
        "*   Activation function (specific choice does not matter)\n",
        "\n",
        "*   FC layer 2\n",
        "\n",
        "Notice\n",
        "*   Softmax not included here bc it is implictly included in the Adam optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H-dTYzTpJhM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):  #\n",
        "  def __init__(self, input_size, hidden_dim, output_size):\n",
        "    super(MLP, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_dim)    # (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "    self.G = nn.GELU()\n",
        "    self.fc2 = nn.Linear(hidden_dim, output_size)\n",
        "    # self.S = nn.Softmax2d() # Isn't SM needed? - Yes - it is implicitly included in the CrossEntropyLoss.\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.G(x)\n",
        "    prediction = self.fc2(x)\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# When you want to execute the model\n",
        "input_size = 784 # MNIST Image: 28*28=784\n",
        "hidden_dim = 500 # A random but appropriate number for h_d\n",
        "output_size = 10 # output size for mnist classfication task\n",
        "model = MLP(input_size, hidden_dim, output_size)\n",
        "print('model:', type(model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZYn75qSJxYp",
        "outputId": "580d3e28-944e-41c9-d42a-91186c7e8faa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model: <class '__main__.MLP'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper-parameters"
      ],
      "metadata": {
        "id": "PuvffWXJOSLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Hyper-parameters to be defined:\n",
        "- lr\n",
        "- choice of optimizer\n",
        "- choice of loss function\n",
        "\n",
        "'''\n",
        "\n",
        "# Choice of Loss function\n",
        "criterion = nn.CrossEntropyLoss()  # The softmax function is contained in this CEL function.\n",
        "# Choice of optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001) # '.parameters()' is a method of nnModule\n",
        "\n"
      ],
      "metadata": {
        "id": "uF0gqNvBOR3P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation and training loop\n",
        "You will define a class for backpropation where gradient is calculated\n",
        "\n",
        "Structure\n",
        "*   Iterate through every epoch, then every batch\n",
        "*   Flatten your data: 'b c h w -> (b c)(h w)'\n",
        "*   Call your model function (forward) to cal predictions (y)\n",
        "*   Cost & weights 三部曲：cost, zero grad, gradients, weights\n",
        "*   Graph: If you want to plot cost-epoch graph:\n",
        "  losses.append(loss.item())  #.item(): from a single-value tensor to a python float\n",
        "\n",
        "\n",
        "Notice:\n",
        "*   I didn't use either class or function because we don't need to reuse this.\n",
        "*   only use \"enumerate\" when you need to use the index for batch\n",
        "*   Things per batch: cal cost, zero grad, cal gradients, update weights.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tlSkpObmUJJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHFFXnYzODrf",
        "outputId": "fc47d580-b4b6-42b6-9ff5-8948ffafde04"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "BP 这里我有点问题：\n",
        "- 每一个batch计算一次cost吗？ yes\n",
        "- 每一次计算cost都要更新gradient吗？ yes\n",
        "- 更新gradient之后如何更新不同的weights & biases? optimizer.step()\n",
        "- 循环算法在def里面吗？\n",
        "\n",
        "'''\n",
        "\n",
        "losses = [] # Difine a empty dictionary for loss. This is for plotting the graph later.\n",
        "epoches = 10 # For a toy model, maybe running through the dataset 10 times is a good idea\n",
        "# def backward(x): Let's see what would happen if we don't even define a function here\n",
        "for epoch in range(epoches):\n",
        "  for batch_idx, (data, label) in enumerate(train_dataloader):\n",
        "    # print('dtype:', data.dtype, data.shape)\n",
        "    data_flatten = rearrange(data,'b c h w -> (b c)(h w)')\n",
        "    # print('data_flatten:', data_flatten.dtype)\n",
        "    prediction = model(data_flatten)\n",
        "    loss = criterion(prediction, label)\n",
        "    optimizer.zero_grad() # We call zero_grad before each run because you want gradient to be updated for each mini training run (each batch). https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "    loss.backward() # We call \".backward\" to calculate gradients for each training run\n",
        "    optimizer.step() # We update the model weights according to the gradients\n",
        "    print(f'The {batch_idx} batch, training loss is {loss}')\n",
        "  print(f'The {epoch} epoch, training loss is {loss}')\n",
        "\n",
        "  # Yes we want a loss-epoch graph\n",
        "  losses.append(loss.item())  #.item(): from a single-value tensor to a python float\n"
      ],
      "metadata": {
        "id": "QXOWY0mIUg9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "990e400f-edde-4ae4-f6ed-19e673fe75f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "The 634 batch, training loss is 0.09162287414073944\n",
            "The 635 batch, training loss is 0.18906784057617188\n",
            "The 636 batch, training loss is 0.09615503251552582\n",
            "The 637 batch, training loss is 0.04665713757276535\n",
            "The 638 batch, training loss is 0.38649293780326843\n",
            "The 639 batch, training loss is 0.23225833475589752\n",
            "The 640 batch, training loss is 0.09243569523096085\n",
            "The 641 batch, training loss is 0.16631346940994263\n",
            "The 642 batch, training loss is 0.1441897600889206\n",
            "The 643 batch, training loss is 0.23657187819480896\n",
            "The 644 batch, training loss is 0.26735952496528625\n",
            "The 645 batch, training loss is 0.13479001820087433\n",
            "The 646 batch, training loss is 0.16520661115646362\n",
            "The 647 batch, training loss is 0.08063704520463943\n",
            "The 648 batch, training loss is 0.12280645221471786\n",
            "The 649 batch, training loss is 0.16447018086910248\n",
            "The 650 batch, training loss is 0.3549378514289856\n",
            "The 651 batch, training loss is 0.1725669950246811\n",
            "The 652 batch, training loss is 0.17473465204238892\n",
            "The 653 batch, training loss is 0.28518441319465637\n",
            "The 654 batch, training loss is 0.1937732845544815\n",
            "The 655 batch, training loss is 0.23975788056850433\n",
            "The 656 batch, training loss is 0.18030695617198944\n",
            "The 657 batch, training loss is 0.1761547178030014\n",
            "The 658 batch, training loss is 0.13938187062740326\n",
            "The 659 batch, training loss is 0.20857089757919312\n",
            "The 660 batch, training loss is 0.10157006233930588\n",
            "The 661 batch, training loss is 0.19690363109111786\n",
            "The 662 batch, training loss is 0.1333996057510376\n",
            "The 663 batch, training loss is 0.27033156156539917\n",
            "The 664 batch, training loss is 0.2081693857908249\n",
            "The 665 batch, training loss is 0.14038969576358795\n",
            "The 666 batch, training loss is 0.09046633541584015\n",
            "The 667 batch, training loss is 0.17923112213611603\n",
            "The 668 batch, training loss is 0.14182305335998535\n",
            "The 669 batch, training loss is 0.2338588535785675\n",
            "The 670 batch, training loss is 0.2159946858882904\n",
            "The 671 batch, training loss is 0.2409183830022812\n",
            "The 672 batch, training loss is 0.20376141369342804\n",
            "The 673 batch, training loss is 0.27861472964286804\n",
            "The 674 batch, training loss is 0.20024512708187103\n",
            "The 675 batch, training loss is 0.23693861067295074\n",
            "The 676 batch, training loss is 0.19533255696296692\n",
            "The 677 batch, training loss is 0.1838321089744568\n",
            "The 678 batch, training loss is 0.3040785491466522\n",
            "The 679 batch, training loss is 0.5398346185684204\n",
            "The 680 batch, training loss is 0.3325892388820648\n",
            "The 681 batch, training loss is 0.24343140423297882\n",
            "The 682 batch, training loss is 0.09646252542734146\n",
            "The 683 batch, training loss is 0.27291110157966614\n",
            "The 684 batch, training loss is 0.28368332982063293\n",
            "The 685 batch, training loss is 0.14638525247573853\n",
            "The 686 batch, training loss is 0.2348865121603012\n",
            "The 687 batch, training loss is 0.1534097045660019\n",
            "The 688 batch, training loss is 0.13312575221061707\n",
            "The 689 batch, training loss is 0.15718883275985718\n",
            "The 690 batch, training loss is 0.14191152155399323\n",
            "The 691 batch, training loss is 0.17384181916713715\n",
            "The 692 batch, training loss is 0.08616656810045242\n",
            "The 693 batch, training loss is 0.22646111249923706\n",
            "The 694 batch, training loss is 0.23644517362117767\n",
            "The 695 batch, training loss is 0.15491673350334167\n",
            "The 696 batch, training loss is 0.21431371569633484\n",
            "The 697 batch, training loss is 0.16086727380752563\n",
            "The 698 batch, training loss is 0.0821709930896759\n",
            "The 699 batch, training loss is 0.23676720261573792\n",
            "The 700 batch, training loss is 0.12543423473834991\n",
            "The 701 batch, training loss is 0.317207932472229\n",
            "The 702 batch, training loss is 0.12876717746257782\n",
            "The 703 batch, training loss is 0.12666814029216766\n",
            "The 704 batch, training loss is 0.25473275780677795\n",
            "The 705 batch, training loss is 0.1933552622795105\n",
            "The 706 batch, training loss is 0.23692861199378967\n",
            "The 707 batch, training loss is 0.18520621955394745\n",
            "The 708 batch, training loss is 0.32816213369369507\n",
            "The 709 batch, training loss is 0.20253445208072662\n",
            "The 710 batch, training loss is 0.054832249879837036\n",
            "The 711 batch, training loss is 0.3973180949687958\n",
            "The 712 batch, training loss is 0.15276870131492615\n",
            "The 713 batch, training loss is 0.11180098354816437\n",
            "The 714 batch, training loss is 0.29385024309158325\n",
            "The 715 batch, training loss is 0.17236042022705078\n",
            "The 716 batch, training loss is 0.22242161631584167\n",
            "The 717 batch, training loss is 0.22072498500347137\n",
            "The 718 batch, training loss is 0.2816488444805145\n",
            "The 719 batch, training loss is 0.2149641215801239\n",
            "The 720 batch, training loss is 0.25631123781204224\n",
            "The 721 batch, training loss is 0.173124298453331\n",
            "The 722 batch, training loss is 0.16857509315013885\n",
            "The 723 batch, training loss is 0.11716285347938538\n",
            "The 724 batch, training loss is 0.14753252267837524\n",
            "The 725 batch, training loss is 0.22822007536888123\n",
            "The 726 batch, training loss is 0.2786596715450287\n",
            "The 727 batch, training loss is 0.23128481209278107\n",
            "The 728 batch, training loss is 0.15435832738876343\n",
            "The 729 batch, training loss is 0.12271900475025177\n",
            "The 730 batch, training loss is 0.22418111562728882\n",
            "The 731 batch, training loss is 0.20765681564807892\n",
            "The 732 batch, training loss is 0.2300158590078354\n",
            "The 733 batch, training loss is 0.17276285588741302\n",
            "The 734 batch, training loss is 0.17256803810596466\n",
            "The 735 batch, training loss is 0.22392086684703827\n",
            "The 736 batch, training loss is 0.18396146595478058\n",
            "The 737 batch, training loss is 0.14744196832180023\n",
            "The 738 batch, training loss is 0.18054845929145813\n",
            "The 739 batch, training loss is 0.08051367104053497\n",
            "The 740 batch, training loss is 0.23653867840766907\n",
            "The 741 batch, training loss is 0.1444404274225235\n",
            "The 742 batch, training loss is 0.27420127391815186\n",
            "The 743 batch, training loss is 0.24205069243907928\n",
            "The 744 batch, training loss is 0.24188899993896484\n",
            "The 745 batch, training loss is 0.1633310765028\n",
            "The 746 batch, training loss is 0.22606191039085388\n",
            "The 747 batch, training loss is 0.2433602213859558\n",
            "The 748 batch, training loss is 0.2075342833995819\n",
            "The 749 batch, training loss is 0.2646912634372711\n",
            "The 750 batch, training loss is 0.211086243391037\n",
            "The 751 batch, training loss is 0.12097420543432236\n",
            "The 752 batch, training loss is 0.14211827516555786\n",
            "The 753 batch, training loss is 0.288684606552124\n",
            "The 754 batch, training loss is 0.12901167571544647\n",
            "The 755 batch, training loss is 0.2852250933647156\n",
            "The 756 batch, training loss is 0.27368995547294617\n",
            "The 757 batch, training loss is 0.15919451415538788\n",
            "The 758 batch, training loss is 0.13748617470264435\n",
            "The 759 batch, training loss is 0.07323551923036575\n",
            "The 760 batch, training loss is 0.19171911478042603\n",
            "The 761 batch, training loss is 0.30454227328300476\n",
            "The 762 batch, training loss is 0.19470447301864624\n",
            "The 763 batch, training loss is 0.2404092252254486\n",
            "The 764 batch, training loss is 0.15338681638240814\n",
            "The 765 batch, training loss is 0.15834686160087585\n",
            "The 766 batch, training loss is 0.08216685801744461\n",
            "The 767 batch, training loss is 0.1366044580936432\n",
            "The 768 batch, training loss is 0.3377282917499542\n",
            "The 769 batch, training loss is 0.2640496790409088\n",
            "The 770 batch, training loss is 0.1301390677690506\n",
            "The 771 batch, training loss is 0.06976260989904404\n",
            "The 772 batch, training loss is 0.1828301101922989\n",
            "The 773 batch, training loss is 0.24521082639694214\n",
            "The 774 batch, training loss is 0.22234396636486053\n",
            "The 775 batch, training loss is 0.2182052880525589\n",
            "The 776 batch, training loss is 0.19972994923591614\n",
            "The 777 batch, training loss is 0.1693149358034134\n",
            "The 778 batch, training loss is 0.28782209753990173\n",
            "The 779 batch, training loss is 0.23202094435691833\n",
            "The 780 batch, training loss is 0.21997150778770447\n",
            "The 781 batch, training loss is 0.18369907140731812\n",
            "The 782 batch, training loss is 0.21642149984836578\n",
            "The 783 batch, training loss is 0.11266285181045532\n",
            "The 784 batch, training loss is 0.10392440855503082\n",
            "The 785 batch, training loss is 0.15303701162338257\n",
            "The 786 batch, training loss is 0.25769883394241333\n",
            "The 787 batch, training loss is 0.19190920889377594\n",
            "The 788 batch, training loss is 0.21816250681877136\n",
            "The 789 batch, training loss is 0.30297139286994934\n",
            "The 790 batch, training loss is 0.11547408252954483\n",
            "The 791 batch, training loss is 0.18257229030132294\n",
            "The 792 batch, training loss is 0.29330599308013916\n",
            "The 793 batch, training loss is 0.14276565611362457\n",
            "The 794 batch, training loss is 0.14944683015346527\n",
            "The 795 batch, training loss is 0.24101996421813965\n",
            "The 796 batch, training loss is 0.1772359311580658\n",
            "The 797 batch, training loss is 0.06381025910377502\n",
            "The 798 batch, training loss is 0.11532021313905716\n",
            "The 799 batch, training loss is 0.2516404688358307\n",
            "The 800 batch, training loss is 0.14861902594566345\n",
            "The 801 batch, training loss is 0.19103413820266724\n",
            "The 802 batch, training loss is 0.3455773591995239\n",
            "The 803 batch, training loss is 0.29490262269973755\n",
            "The 804 batch, training loss is 0.16793400049209595\n",
            "The 805 batch, training loss is 0.17997461557388306\n",
            "The 806 batch, training loss is 0.14833271503448486\n",
            "The 807 batch, training loss is 0.16258661448955536\n",
            "The 808 batch, training loss is 0.1114250048995018\n",
            "The 809 batch, training loss is 0.11974617838859558\n",
            "The 810 batch, training loss is 0.41415882110595703\n",
            "The 811 batch, training loss is 0.5634823441505432\n",
            "The 812 batch, training loss is 0.19712457060813904\n",
            "The 813 batch, training loss is 0.18915212154388428\n",
            "The 814 batch, training loss is 0.26558661460876465\n",
            "The 815 batch, training loss is 0.15194863080978394\n",
            "The 816 batch, training loss is 0.22614817321300507\n",
            "The 817 batch, training loss is 0.3526741862297058\n",
            "The 818 batch, training loss is 0.12696601450443268\n",
            "The 819 batch, training loss is 0.3127318322658539\n",
            "The 820 batch, training loss is 0.18747001886367798\n",
            "The 821 batch, training loss is 0.12626735866069794\n",
            "The 822 batch, training loss is 0.1081179678440094\n",
            "The 823 batch, training loss is 0.15285898745059967\n",
            "The 824 batch, training loss is 0.14709027111530304\n",
            "The 825 batch, training loss is 0.23201417922973633\n",
            "The 826 batch, training loss is 0.19812531769275665\n",
            "The 827 batch, training loss is 0.3838973641395569\n",
            "The 828 batch, training loss is 0.30396443605422974\n",
            "The 829 batch, training loss is 0.2567102015018463\n",
            "The 830 batch, training loss is 0.28915128111839294\n",
            "The 831 batch, training loss is 0.15978339314460754\n",
            "The 832 batch, training loss is 0.165557399392128\n",
            "The 833 batch, training loss is 0.18687786161899567\n",
            "The 834 batch, training loss is 0.14751002192497253\n",
            "The 835 batch, training loss is 0.17892280220985413\n",
            "The 836 batch, training loss is 0.33994901180267334\n",
            "The 837 batch, training loss is 0.30703675746917725\n",
            "The 838 batch, training loss is 0.19159206748008728\n",
            "The 839 batch, training loss is 0.19020532071590424\n",
            "The 840 batch, training loss is 0.14318755269050598\n",
            "The 841 batch, training loss is 0.16569405794143677\n",
            "The 842 batch, training loss is 0.2359044998884201\n",
            "The 843 batch, training loss is 0.12889264523983002\n",
            "The 844 batch, training loss is 0.31720781326293945\n",
            "The 845 batch, training loss is 0.1807834804058075\n",
            "The 846 batch, training loss is 0.3291774392127991\n",
            "The 847 batch, training loss is 0.13271965086460114\n",
            "The 848 batch, training loss is 0.12449774146080017\n",
            "The 849 batch, training loss is 0.4204544723033905\n",
            "The 850 batch, training loss is 0.09616384655237198\n",
            "The 851 batch, training loss is 0.299897164106369\n",
            "The 852 batch, training loss is 0.2691570520401001\n",
            "The 853 batch, training loss is 0.09817997366189957\n",
            "The 854 batch, training loss is 0.22302836179733276\n",
            "The 855 batch, training loss is 0.14064888656139374\n",
            "The 856 batch, training loss is 0.22166794538497925\n",
            "The 857 batch, training loss is 0.473701149225235\n",
            "The 858 batch, training loss is 0.24286304414272308\n",
            "The 859 batch, training loss is 0.21912656724452972\n",
            "The 860 batch, training loss is 0.13638341426849365\n",
            "The 861 batch, training loss is 0.16399286687374115\n",
            "The 862 batch, training loss is 0.25038737058639526\n",
            "The 863 batch, training loss is 0.1702147126197815\n",
            "The 864 batch, training loss is 0.1762436330318451\n",
            "The 865 batch, training loss is 0.2727036774158478\n",
            "The 866 batch, training loss is 0.06402089446783066\n",
            "The 867 batch, training loss is 0.2005378156900406\n",
            "The 868 batch, training loss is 0.15107305347919464\n",
            "The 869 batch, training loss is 0.09690169990062714\n",
            "The 870 batch, training loss is 0.2483961582183838\n",
            "The 871 batch, training loss is 0.23173730075359344\n",
            "The 872 batch, training loss is 0.10876954346895218\n",
            "The 873 batch, training loss is 0.23093323409557343\n",
            "The 874 batch, training loss is 0.19100002944469452\n",
            "The 875 batch, training loss is 0.1670359969139099\n",
            "The 876 batch, training loss is 0.29532918334007263\n",
            "The 877 batch, training loss is 0.1617814004421234\n",
            "The 878 batch, training loss is 0.10340937227010727\n",
            "The 879 batch, training loss is 0.21412484347820282\n",
            "The 880 batch, training loss is 0.20026181638240814\n",
            "The 881 batch, training loss is 0.11471354961395264\n",
            "The 882 batch, training loss is 0.11353587359189987\n",
            "The 883 batch, training loss is 0.06984861195087433\n",
            "The 884 batch, training loss is 0.25999826192855835\n",
            "The 885 batch, training loss is 0.2243688404560089\n",
            "The 886 batch, training loss is 0.21708481013774872\n",
            "The 887 batch, training loss is 0.12051113694906235\n",
            "The 888 batch, training loss is 0.09044120460748672\n",
            "The 889 batch, training loss is 0.12116040289402008\n",
            "The 890 batch, training loss is 0.3877429664134979\n",
            "The 891 batch, training loss is 0.16836118698120117\n",
            "The 892 batch, training loss is 0.2687954604625702\n",
            "The 893 batch, training loss is 0.23979447782039642\n",
            "The 894 batch, training loss is 0.1831417679786682\n",
            "The 895 batch, training loss is 0.26698189973831177\n",
            "The 896 batch, training loss is 0.24325482547283173\n",
            "The 897 batch, training loss is 0.2997095286846161\n",
            "The 898 batch, training loss is 0.07359938323497772\n",
            "The 899 batch, training loss is 0.14370617270469666\n",
            "The 900 batch, training loss is 0.17501164972782135\n",
            "The 901 batch, training loss is 0.10972029715776443\n",
            "The 902 batch, training loss is 0.177521213889122\n",
            "The 903 batch, training loss is 0.23063847422599792\n",
            "The 904 batch, training loss is 0.16551178693771362\n",
            "The 905 batch, training loss is 0.11608368903398514\n",
            "The 906 batch, training loss is 0.318026602268219\n",
            "The 907 batch, training loss is 0.297452449798584\n",
            "The 908 batch, training loss is 0.23888887465000153\n",
            "The 909 batch, training loss is 0.19861698150634766\n",
            "The 910 batch, training loss is 0.0626608356833458\n",
            "The 911 batch, training loss is 0.1354425549507141\n",
            "The 912 batch, training loss is 0.1658361703157425\n",
            "The 913 batch, training loss is 0.2821575105190277\n",
            "The 914 batch, training loss is 0.18331316113471985\n",
            "The 915 batch, training loss is 0.07257569581270218\n",
            "The 916 batch, training loss is 0.11386649310588837\n",
            "The 917 batch, training loss is 0.22401079535484314\n",
            "The 918 batch, training loss is 0.23869094252586365\n",
            "The 919 batch, training loss is 0.13854752480983734\n",
            "The 920 batch, training loss is 0.37020471692085266\n",
            "The 921 batch, training loss is 0.14491800963878632\n",
            "The 922 batch, training loss is 0.286492258310318\n",
            "The 923 batch, training loss is 0.34946906566619873\n",
            "The 924 batch, training loss is 0.2715965211391449\n",
            "The 925 batch, training loss is 0.1715124100446701\n",
            "The 926 batch, training loss is 0.3173878490924835\n",
            "The 927 batch, training loss is 0.09947045892477036\n",
            "The 928 batch, training loss is 0.1536562740802765\n",
            "The 929 batch, training loss is 0.21425050497055054\n",
            "The 930 batch, training loss is 0.1757311075925827\n",
            "The 931 batch, training loss is 0.10403285175561905\n",
            "The 932 batch, training loss is 0.3838363289833069\n",
            "The 933 batch, training loss is 0.0693976879119873\n",
            "The 934 batch, training loss is 0.266385555267334\n",
            "The 935 batch, training loss is 0.10948290675878525\n",
            "The 936 batch, training loss is 0.2780402600765228\n",
            "The 937 batch, training loss is 0.15813523530960083\n",
            "The 4 epoch, training loss is 0.15813523530960083\n",
            "The 0 batch, training loss is 0.24922548234462738\n",
            "The 1 batch, training loss is 0.3698543310165405\n",
            "The 2 batch, training loss is 0.2275235652923584\n",
            "The 3 batch, training loss is 0.17036272585391998\n",
            "The 4 batch, training loss is 0.12573474645614624\n",
            "The 5 batch, training loss is 0.169753298163414\n",
            "The 6 batch, training loss is 0.275650292634964\n",
            "The 7 batch, training loss is 0.49107038974761963\n",
            "The 8 batch, training loss is 0.231243297457695\n",
            "The 9 batch, training loss is 0.20324650406837463\n",
            "The 10 batch, training loss is 0.17276331782341003\n",
            "The 11 batch, training loss is 0.38713550567626953\n",
            "The 12 batch, training loss is 0.17951048910617828\n",
            "The 13 batch, training loss is 0.0700104609131813\n",
            "The 14 batch, training loss is 0.29825150966644287\n",
            "The 15 batch, training loss is 0.14535896480083466\n",
            "The 16 batch, training loss is 0.10152699053287506\n",
            "The 17 batch, training loss is 0.12457723915576935\n",
            "The 18 batch, training loss is 0.23111799359321594\n",
            "The 19 batch, training loss is 0.14787986874580383\n",
            "The 20 batch, training loss is 0.10061083734035492\n",
            "The 21 batch, training loss is 0.14894835650920868\n",
            "The 22 batch, training loss is 0.3445022404193878\n",
            "The 23 batch, training loss is 0.43077075481414795\n",
            "The 24 batch, training loss is 0.13473476469516754\n",
            "The 25 batch, training loss is 0.14159752428531647\n",
            "The 26 batch, training loss is 0.1299186646938324\n",
            "The 27 batch, training loss is 0.2062000334262848\n",
            "The 28 batch, training loss is 0.3096964955329895\n",
            "The 29 batch, training loss is 0.19477716088294983\n",
            "The 30 batch, training loss is 0.2529048025608063\n",
            "The 31 batch, training loss is 0.13631226122379303\n",
            "The 32 batch, training loss is 0.1441633254289627\n",
            "The 33 batch, training loss is 0.24821147322654724\n",
            "The 34 batch, training loss is 0.19831867516040802\n",
            "The 35 batch, training loss is 0.3672025203704834\n",
            "The 36 batch, training loss is 0.2674294412136078\n",
            "The 37 batch, training loss is 0.16676945984363556\n",
            "The 38 batch, training loss is 0.3738306760787964\n",
            "The 39 batch, training loss is 0.21404963731765747\n",
            "The 40 batch, training loss is 0.14769311249256134\n",
            "The 41 batch, training loss is 0.16735616326332092\n",
            "The 42 batch, training loss is 0.28609544038772583\n",
            "The 43 batch, training loss is 0.1756836324930191\n",
            "The 44 batch, training loss is 0.20830994844436646\n",
            "The 45 batch, training loss is 0.0762990415096283\n",
            "The 46 batch, training loss is 0.20619536936283112\n",
            "The 47 batch, training loss is 0.23050223290920258\n",
            "The 48 batch, training loss is 0.15601159632205963\n",
            "The 49 batch, training loss is 0.19016613066196442\n",
            "The 50 batch, training loss is 0.16854868829250336\n",
            "The 51 batch, training loss is 0.16142146289348602\n",
            "The 52 batch, training loss is 0.12168720364570618\n",
            "The 53 batch, training loss is 0.11834335327148438\n",
            "The 54 batch, training loss is 0.23099903762340546\n",
            "The 55 batch, training loss is 0.2558228671550751\n",
            "The 56 batch, training loss is 0.12879006564617157\n",
            "The 57 batch, training loss is 0.1863347291946411\n",
            "The 58 batch, training loss is 0.15411695837974548\n",
            "The 59 batch, training loss is 0.2063693404197693\n",
            "The 60 batch, training loss is 0.10389930754899979\n",
            "The 61 batch, training loss is 0.10918300598859787\n",
            "The 62 batch, training loss is 0.15902893245220184\n",
            "The 63 batch, training loss is 0.11512941867113113\n",
            "The 64 batch, training loss is 0.2640387713909149\n",
            "The 65 batch, training loss is 0.09639820456504822\n",
            "The 66 batch, training loss is 0.126271054148674\n",
            "The 67 batch, training loss is 0.2495730072259903\n",
            "The 68 batch, training loss is 0.1583424210548401\n",
            "The 69 batch, training loss is 0.14042463898658752\n",
            "The 70 batch, training loss is 0.15350531041622162\n",
            "The 71 batch, training loss is 0.26797303557395935\n",
            "The 72 batch, training loss is 0.2578044831752777\n",
            "The 73 batch, training loss is 0.19165445864200592\n",
            "The 74 batch, training loss is 0.2991009056568146\n",
            "The 75 batch, training loss is 0.2893560230731964\n",
            "The 76 batch, training loss is 0.27193498611450195\n",
            "The 77 batch, training loss is 0.11959166079759598\n",
            "The 78 batch, training loss is 0.1516566425561905\n",
            "The 79 batch, training loss is 0.11729590594768524\n",
            "The 80 batch, training loss is 0.2756634056568146\n",
            "The 81 batch, training loss is 0.2777528464794159\n",
            "The 82 batch, training loss is 0.22307732701301575\n",
            "The 83 batch, training loss is 0.11736425012350082\n",
            "The 84 batch, training loss is 0.08296666294336319\n",
            "The 85 batch, training loss is 0.2392941266298294\n",
            "The 86 batch, training loss is 0.17826369404792786\n",
            "The 87 batch, training loss is 0.08553864061832428\n",
            "The 88 batch, training loss is 0.21212118864059448\n",
            "The 89 batch, training loss is 0.19113323092460632\n",
            "The 90 batch, training loss is 0.42282792925834656\n",
            "The 91 batch, training loss is 0.13604286313056946\n",
            "The 92 batch, training loss is 0.16890127956867218\n",
            "The 93 batch, training loss is 0.07816417515277863\n",
            "The 94 batch, training loss is 0.1567411720752716\n",
            "The 95 batch, training loss is 0.14558173716068268\n",
            "The 96 batch, training loss is 0.0920218676328659\n",
            "The 97 batch, training loss is 0.18782633543014526\n",
            "The 98 batch, training loss is 0.16661493480205536\n",
            "The 99 batch, training loss is 0.10275530815124512\n",
            "The 100 batch, training loss is 0.2654857635498047\n",
            "The 101 batch, training loss is 0.2860358655452728\n",
            "The 102 batch, training loss is 0.2368663251399994\n",
            "The 103 batch, training loss is 0.20426255464553833\n",
            "The 104 batch, training loss is 0.08775258809328079\n",
            "The 105 batch, training loss is 0.16210207343101501\n",
            "The 106 batch, training loss is 0.13889780640602112\n",
            "The 107 batch, training loss is 0.2947359085083008\n",
            "The 108 batch, training loss is 0.11093210428953171\n",
            "The 109 batch, training loss is 0.11811943352222443\n",
            "The 110 batch, training loss is 0.16269178688526154\n",
            "The 111 batch, training loss is 0.224847674369812\n",
            "The 112 batch, training loss is 0.1851413995027542\n",
            "The 113 batch, training loss is 0.16165481507778168\n",
            "The 114 batch, training loss is 0.31152263283729553\n",
            "The 115 batch, training loss is 0.14371085166931152\n",
            "The 116 batch, training loss is 0.2645719647407532\n",
            "The 117 batch, training loss is 0.2330453246831894\n",
            "The 118 batch, training loss is 0.3176000416278839\n",
            "The 119 batch, training loss is 0.2185676544904709\n",
            "The 120 batch, training loss is 0.15303044021129608\n",
            "The 121 batch, training loss is 0.31240028142929077\n",
            "The 122 batch, training loss is 0.20374269783496857\n",
            "The 123 batch, training loss is 0.04025483876466751\n",
            "The 124 batch, training loss is 0.11192520707845688\n",
            "The 125 batch, training loss is 0.02996639907360077\n",
            "The 126 batch, training loss is 0.14683373272418976\n",
            "The 127 batch, training loss is 0.13295549154281616\n",
            "The 128 batch, training loss is 0.09014892578125\n",
            "The 129 batch, training loss is 0.37186455726623535\n",
            "The 130 batch, training loss is 0.20330536365509033\n",
            "The 131 batch, training loss is 0.07641176879405975\n",
            "The 132 batch, training loss is 0.2453017383813858\n",
            "The 133 batch, training loss is 0.11290639638900757\n",
            "The 134 batch, training loss is 0.13228456676006317\n",
            "The 135 batch, training loss is 0.3535805940628052\n",
            "The 136 batch, training loss is 0.13313348591327667\n",
            "The 137 batch, training loss is 0.15615178644657135\n",
            "The 138 batch, training loss is 0.21971344947814941\n",
            "The 139 batch, training loss is 0.07568258047103882\n",
            "The 140 batch, training loss is 0.08984929323196411\n",
            "The 141 batch, training loss is 0.08446299284696579\n",
            "The 142 batch, training loss is 0.13948650658130646\n",
            "The 143 batch, training loss is 0.2145119160413742\n",
            "The 144 batch, training loss is 0.14844629168510437\n",
            "The 145 batch, training loss is 0.14701193571090698\n",
            "The 146 batch, training loss is 0.16033139824867249\n",
            "The 147 batch, training loss is 0.16276493668556213\n",
            "The 148 batch, training loss is 0.1114249974489212\n",
            "The 149 batch, training loss is 0.22708407044410706\n",
            "The 150 batch, training loss is 0.09303931146860123\n",
            "The 151 batch, training loss is 0.19377608597278595\n",
            "The 152 batch, training loss is 0.23043757677078247\n",
            "The 153 batch, training loss is 0.34485721588134766\n",
            "The 154 batch, training loss is 0.3270038962364197\n",
            "The 155 batch, training loss is 0.15349555015563965\n",
            "The 156 batch, training loss is 0.2671687602996826\n",
            "The 157 batch, training loss is 0.16180409491062164\n",
            "The 158 batch, training loss is 0.2812044620513916\n",
            "The 159 batch, training loss is 0.14138160645961761\n",
            "The 160 batch, training loss is 0.18214033544063568\n",
            "The 161 batch, training loss is 0.1655787080526352\n",
            "The 162 batch, training loss is 0.166719451546669\n",
            "The 163 batch, training loss is 0.13100434839725494\n",
            "The 164 batch, training loss is 0.12222202867269516\n",
            "The 165 batch, training loss is 0.36651620268821716\n",
            "The 166 batch, training loss is 0.15282690525054932\n",
            "The 167 batch, training loss is 0.1125616654753685\n",
            "The 168 batch, training loss is 0.19889889657497406\n",
            "The 169 batch, training loss is 0.30116769671440125\n",
            "The 170 batch, training loss is 0.12930095195770264\n",
            "The 171 batch, training loss is 0.22253857553005219\n",
            "The 172 batch, training loss is 0.11595454812049866\n",
            "The 173 batch, training loss is 0.4085303843021393\n",
            "The 174 batch, training loss is 0.09457529336214066\n",
            "The 175 batch, training loss is 0.23175019025802612\n",
            "The 176 batch, training loss is 0.23493856191635132\n",
            "The 177 batch, training loss is 0.2659344971179962\n",
            "The 178 batch, training loss is 0.23197826743125916\n",
            "The 179 batch, training loss is 0.0857851579785347\n",
            "The 180 batch, training loss is 0.09445789456367493\n",
            "The 181 batch, training loss is 0.19510196149349213\n",
            "The 182 batch, training loss is 0.27226728200912476\n",
            "The 183 batch, training loss is 0.2569883167743683\n",
            "The 184 batch, training loss is 0.11581070721149445\n",
            "The 185 batch, training loss is 0.17357446253299713\n",
            "The 186 batch, training loss is 0.1822391003370285\n",
            "The 187 batch, training loss is 0.2575492858886719\n",
            "The 188 batch, training loss is 0.20723427832126617\n",
            "The 189 batch, training loss is 0.2573280930519104\n",
            "The 190 batch, training loss is 0.1251886785030365\n",
            "The 191 batch, training loss is 0.17634806036949158\n",
            "The 192 batch, training loss is 0.22888977825641632\n",
            "The 193 batch, training loss is 0.2840762436389923\n",
            "The 194 batch, training loss is 0.14262078702449799\n",
            "The 195 batch, training loss is 0.11905854195356369\n",
            "The 196 batch, training loss is 0.1629665195941925\n",
            "The 197 batch, training loss is 0.2661452293395996\n",
            "The 198 batch, training loss is 0.24900314211845398\n",
            "The 199 batch, training loss is 0.14321480691432953\n",
            "The 200 batch, training loss is 0.1692761331796646\n",
            "The 201 batch, training loss is 0.17133846879005432\n",
            "The 202 batch, training loss is 0.14793811738491058\n",
            "The 203 batch, training loss is 0.18377850949764252\n",
            "The 204 batch, training loss is 0.31634122133255005\n",
            "The 205 batch, training loss is 0.14364087581634521\n",
            "The 206 batch, training loss is 0.17898516356945038\n",
            "The 207 batch, training loss is 0.07147037982940674\n",
            "The 208 batch, training loss is 0.1987733691930771\n",
            "The 209 batch, training loss is 0.3346216082572937\n",
            "The 210 batch, training loss is 0.3267245292663574\n",
            "The 211 batch, training loss is 0.28307080268859863\n",
            "The 212 batch, training loss is 0.31291788816452026\n",
            "The 213 batch, training loss is 0.09690842777490616\n",
            "The 214 batch, training loss is 0.15511305630207062\n",
            "The 215 batch, training loss is 0.0957999974489212\n",
            "The 216 batch, training loss is 0.1706094741821289\n",
            "The 217 batch, training loss is 0.09168454259634018\n",
            "The 218 batch, training loss is 0.09906811267137527\n",
            "The 219 batch, training loss is 0.24225443601608276\n",
            "The 220 batch, training loss is 0.11970368027687073\n",
            "The 221 batch, training loss is 0.21493865549564362\n",
            "The 222 batch, training loss is 0.11611879616975784\n",
            "The 223 batch, training loss is 0.18966232240200043\n",
            "The 224 batch, training loss is 0.15776345133781433\n",
            "The 225 batch, training loss is 0.32533323764801025\n",
            "The 226 batch, training loss is 0.23459221422672272\n",
            "The 227 batch, training loss is 0.17865034937858582\n",
            "The 228 batch, training loss is 0.12681567668914795\n",
            "The 229 batch, training loss is 0.1081879511475563\n",
            "The 230 batch, training loss is 0.09236065298318863\n",
            "The 231 batch, training loss is 0.10119837522506714\n",
            "The 232 batch, training loss is 0.1319025754928589\n",
            "The 233 batch, training loss is 0.1461353302001953\n",
            "The 234 batch, training loss is 0.20851349830627441\n",
            "The 235 batch, training loss is 0.1439359039068222\n",
            "The 236 batch, training loss is 0.2575005292892456\n",
            "The 237 batch, training loss is 0.25275707244873047\n",
            "The 238 batch, training loss is 0.23422178626060486\n",
            "The 239 batch, training loss is 0.06364547461271286\n",
            "The 240 batch, training loss is 0.3392810523509979\n",
            "The 241 batch, training loss is 0.09345795959234238\n",
            "The 242 batch, training loss is 0.1741851419210434\n",
            "The 243 batch, training loss is 0.15080995857715607\n",
            "The 244 batch, training loss is 0.11847866326570511\n",
            "The 245 batch, training loss is 0.22744791209697723\n",
            "The 246 batch, training loss is 0.4005472958087921\n",
            "The 247 batch, training loss is 0.14192470908164978\n",
            "The 248 batch, training loss is 0.22008295357227325\n",
            "The 249 batch, training loss is 0.2556501030921936\n",
            "The 250 batch, training loss is 0.4333840608596802\n",
            "The 251 batch, training loss is 0.06903929263353348\n",
            "The 252 batch, training loss is 0.1409176141023636\n",
            "The 253 batch, training loss is 0.17643588781356812\n",
            "The 254 batch, training loss is 0.1756933629512787\n",
            "The 255 batch, training loss is 0.26233747601509094\n",
            "The 256 batch, training loss is 0.1727730631828308\n",
            "The 257 batch, training loss is 0.16307176649570465\n",
            "The 258 batch, training loss is 0.06812357157468796\n",
            "The 259 batch, training loss is 0.14538660645484924\n",
            "The 260 batch, training loss is 0.17829358577728271\n",
            "The 261 batch, training loss is 0.11971646547317505\n",
            "The 262 batch, training loss is 0.30751824378967285\n",
            "The 263 batch, training loss is 0.08784523606300354\n",
            "The 264 batch, training loss is 0.2805138826370239\n",
            "The 265 batch, training loss is 0.13009919226169586\n",
            "The 266 batch, training loss is 0.1573728621006012\n",
            "The 267 batch, training loss is 0.1972658336162567\n",
            "The 268 batch, training loss is 0.07487734407186508\n",
            "The 269 batch, training loss is 0.23147575557231903\n",
            "The 270 batch, training loss is 0.18301479518413544\n",
            "The 271 batch, training loss is 0.18839503824710846\n",
            "The 272 batch, training loss is 0.16748175024986267\n",
            "The 273 batch, training loss is 0.16907387971878052\n",
            "The 274 batch, training loss is 0.21170586347579956\n",
            "The 275 batch, training loss is 0.3089578151702881\n",
            "The 276 batch, training loss is 0.32011744379997253\n",
            "The 277 batch, training loss is 0.16885888576507568\n",
            "The 278 batch, training loss is 0.2572856843471527\n",
            "The 279 batch, training loss is 0.2013920694589615\n",
            "The 280 batch, training loss is 0.20717398822307587\n",
            "The 281 batch, training loss is 0.11000519245862961\n",
            "The 282 batch, training loss is 0.08690239489078522\n",
            "The 283 batch, training loss is 0.19597336649894714\n",
            "The 284 batch, training loss is 0.3920392692089081\n",
            "The 285 batch, training loss is 0.169441819190979\n",
            "The 286 batch, training loss is 0.06695505231618881\n",
            "The 287 batch, training loss is 0.16934411227703094\n",
            "The 288 batch, training loss is 0.1659414917230606\n",
            "The 289 batch, training loss is 0.2246684730052948\n",
            "The 290 batch, training loss is 0.21682222187519073\n",
            "The 291 batch, training loss is 0.12593640387058258\n",
            "The 292 batch, training loss is 0.21081127226352692\n",
            "The 293 batch, training loss is 0.10815354436635971\n",
            "The 294 batch, training loss is 0.13407345116138458\n",
            "The 295 batch, training loss is 0.0993022695183754\n",
            "The 296 batch, training loss is 0.22856958210468292\n",
            "The 297 batch, training loss is 0.15853653848171234\n",
            "The 298 batch, training loss is 0.19599410891532898\n",
            "The 299 batch, training loss is 0.2952936887741089\n",
            "The 300 batch, training loss is 0.10554706305265427\n",
            "The 301 batch, training loss is 0.07879066467285156\n",
            "The 302 batch, training loss is 0.09438736736774445\n",
            "The 303 batch, training loss is 0.15745028853416443\n",
            "The 304 batch, training loss is 0.27160021662712097\n",
            "The 305 batch, training loss is 0.1891980767250061\n",
            "The 306 batch, training loss is 0.1736963987350464\n",
            "The 307 batch, training loss is 0.04780040308833122\n",
            "The 308 batch, training loss is 0.1661377251148224\n",
            "The 309 batch, training loss is 0.3672751784324646\n",
            "The 310 batch, training loss is 0.13230346143245697\n",
            "The 311 batch, training loss is 0.16837455332279205\n",
            "The 312 batch, training loss is 0.1973329782485962\n",
            "The 313 batch, training loss is 0.13801833987236023\n",
            "The 314 batch, training loss is 0.32778200507164\n",
            "The 315 batch, training loss is 0.13447313010692596\n",
            "The 316 batch, training loss is 0.1455182433128357\n",
            "The 317 batch, training loss is 0.1547142118215561\n",
            "The 318 batch, training loss is 0.2816248834133148\n",
            "The 319 batch, training loss is 0.2805040180683136\n",
            "The 320 batch, training loss is 0.15856005251407623\n",
            "The 321 batch, training loss is 0.09756645560264587\n",
            "The 322 batch, training loss is 0.27346646785736084\n",
            "The 323 batch, training loss is 0.3075650930404663\n",
            "The 324 batch, training loss is 0.15850992500782013\n",
            "The 325 batch, training loss is 0.2113274335861206\n",
            "The 326 batch, training loss is 0.14454247057437897\n",
            "The 327 batch, training loss is 0.16714300215244293\n",
            "The 328 batch, training loss is 0.134133979678154\n",
            "The 329 batch, training loss is 0.1552603840827942\n",
            "The 330 batch, training loss is 0.0984027162194252\n",
            "The 331 batch, training loss is 0.16732950508594513\n",
            "The 332 batch, training loss is 0.14560182392597198\n",
            "The 333 batch, training loss is 0.11585405468940735\n",
            "The 334 batch, training loss is 0.16085615754127502\n",
            "The 335 batch, training loss is 0.30933451652526855\n",
            "The 336 batch, training loss is 0.14555735886096954\n",
            "The 337 batch, training loss is 0.14113783836364746\n",
            "The 338 batch, training loss is 0.28179699182510376\n",
            "The 339 batch, training loss is 0.21224331855773926\n",
            "The 340 batch, training loss is 0.15562935173511505\n",
            "The 341 batch, training loss is 0.16024595499038696\n",
            "The 342 batch, training loss is 0.290141761302948\n",
            "The 343 batch, training loss is 0.14154495298862457\n",
            "The 344 batch, training loss is 0.3475515842437744\n",
            "The 345 batch, training loss is 0.2197810560464859\n",
            "The 346 batch, training loss is 0.11895841360092163\n",
            "The 347 batch, training loss is 0.13210958242416382\n",
            "The 348 batch, training loss is 0.19979508221149445\n",
            "The 349 batch, training loss is 0.25651735067367554\n",
            "The 350 batch, training loss is 0.0760861486196518\n",
            "The 351 batch, training loss is 0.18002529442310333\n",
            "The 352 batch, training loss is 0.30724650621414185\n",
            "The 353 batch, training loss is 0.20436212420463562\n",
            "The 354 batch, training loss is 0.16214324533939362\n",
            "The 355 batch, training loss is 0.1967911422252655\n",
            "The 356 batch, training loss is 0.11578195542097092\n",
            "The 357 batch, training loss is 0.20787720382213593\n",
            "The 358 batch, training loss is 0.08059710264205933\n",
            "The 359 batch, training loss is 0.24811136722564697\n",
            "The 360 batch, training loss is 0.1397654414176941\n",
            "The 361 batch, training loss is 0.09774929285049438\n",
            "The 362 batch, training loss is 0.2146298885345459\n",
            "The 363 batch, training loss is 0.10354746133089066\n",
            "The 364 batch, training loss is 0.1924729198217392\n",
            "The 365 batch, training loss is 0.08763343840837479\n",
            "The 366 batch, training loss is 0.23655250668525696\n",
            "The 367 batch, training loss is 0.13168829679489136\n",
            "The 368 batch, training loss is 0.09419476985931396\n",
            "The 369 batch, training loss is 0.1634051352739334\n",
            "The 370 batch, training loss is 0.07120164483785629\n",
            "The 371 batch, training loss is 0.0902840793132782\n",
            "The 372 batch, training loss is 0.1521342545747757\n",
            "The 373 batch, training loss is 0.13024193048477173\n",
            "The 374 batch, training loss is 0.14401066303253174\n",
            "The 375 batch, training loss is 0.11584719270467758\n",
            "The 376 batch, training loss is 0.33274418115615845\n",
            "The 377 batch, training loss is 0.13932788372039795\n",
            "The 378 batch, training loss is 0.06871705502271652\n",
            "The 379 batch, training loss is 0.16189761459827423\n",
            "The 380 batch, training loss is 0.06058162450790405\n",
            "The 381 batch, training loss is 0.13132090866565704\n",
            "The 382 batch, training loss is 0.08190370351076126\n",
            "The 383 batch, training loss is 0.16454347968101501\n",
            "The 384 batch, training loss is 0.12452269345521927\n",
            "The 385 batch, training loss is 0.11137063801288605\n",
            "The 386 batch, training loss is 0.2188197672367096\n",
            "The 387 batch, training loss is 0.3541905879974365\n",
            "The 388 batch, training loss is 0.08048521727323532\n",
            "The 389 batch, training loss is 0.09888461977243423\n",
            "The 390 batch, training loss is 0.0917806625366211\n",
            "The 391 batch, training loss is 0.12461265176534653\n",
            "The 392 batch, training loss is 0.16373653709888458\n",
            "The 393 batch, training loss is 0.17873512208461761\n",
            "The 394 batch, training loss is 0.1819358766078949\n",
            "The 395 batch, training loss is 0.11765953153371811\n",
            "The 396 batch, training loss is 0.1194911077618599\n",
            "The 397 batch, training loss is 0.14371651411056519\n",
            "The 398 batch, training loss is 0.13377656042575836\n",
            "The 399 batch, training loss is 0.17756512761116028\n",
            "The 400 batch, training loss is 0.18680816888809204\n",
            "The 401 batch, training loss is 0.09974701702594757\n",
            "The 402 batch, training loss is 0.10432634502649307\n",
            "The 403 batch, training loss is 0.18403713405132294\n",
            "The 404 batch, training loss is 0.1935095489025116\n",
            "The 405 batch, training loss is 0.05292421579360962\n",
            "The 406 batch, training loss is 0.1888866275548935\n",
            "The 407 batch, training loss is 0.21767191588878632\n",
            "The 408 batch, training loss is 0.31136247515678406\n",
            "The 409 batch, training loss is 0.16430650651454926\n",
            "The 410 batch, training loss is 0.17108869552612305\n",
            "The 411 batch, training loss is 0.07367835938930511\n",
            "The 412 batch, training loss is 0.27633219957351685\n",
            "The 413 batch, training loss is 0.10782299935817719\n",
            "The 414 batch, training loss is 0.2300460934638977\n",
            "The 415 batch, training loss is 0.16492486000061035\n",
            "The 416 batch, training loss is 0.1099269911646843\n",
            "The 417 batch, training loss is 0.20740634202957153\n",
            "The 418 batch, training loss is 0.16404233872890472\n",
            "The 419 batch, training loss is 0.2308097779750824\n",
            "The 420 batch, training loss is 0.15244965255260468\n",
            "The 421 batch, training loss is 0.1810629665851593\n",
            "The 422 batch, training loss is 0.20709432661533356\n",
            "The 423 batch, training loss is 0.19356180727481842\n",
            "The 424 batch, training loss is 0.0825786367058754\n",
            "The 425 batch, training loss is 0.2285110354423523\n",
            "The 426 batch, training loss is 0.14296671748161316\n",
            "The 427 batch, training loss is 0.41811805963516235\n",
            "The 428 batch, training loss is 0.2644045054912567\n",
            "The 429 batch, training loss is 0.2314850389957428\n",
            "The 430 batch, training loss is 0.15020321309566498\n",
            "The 431 batch, training loss is 0.13960707187652588\n",
            "The 432 batch, training loss is 0.19222018122673035\n",
            "The 433 batch, training loss is 0.1388646811246872\n",
            "The 434 batch, training loss is 0.1979139745235443\n",
            "The 435 batch, training loss is 0.1663297563791275\n",
            "The 436 batch, training loss is 0.18096809089183807\n",
            "The 437 batch, training loss is 0.21934208273887634\n",
            "The 438 batch, training loss is 0.06384548544883728\n",
            "The 439 batch, training loss is 0.10966645926237106\n",
            "The 440 batch, training loss is 0.18274849653244019\n",
            "The 441 batch, training loss is 0.14230278134346008\n",
            "The 442 batch, training loss is 0.22122034430503845\n",
            "The 443 batch, training loss is 0.29480862617492676\n",
            "The 444 batch, training loss is 0.26930102705955505\n",
            "The 445 batch, training loss is 0.14758220314979553\n",
            "The 446 batch, training loss is 0.1283111423254013\n",
            "The 447 batch, training loss is 0.3054269254207611\n",
            "The 448 batch, training loss is 0.14002595841884613\n",
            "The 449 batch, training loss is 0.23741702735424042\n",
            "The 450 batch, training loss is 0.08738292753696442\n",
            "The 451 batch, training loss is 0.12257257103919983\n",
            "The 452 batch, training loss is 0.47057339549064636\n",
            "The 453 batch, training loss is 0.23218247294425964\n",
            "The 454 batch, training loss is 0.27397438883781433\n",
            "The 455 batch, training loss is 0.23243272304534912\n",
            "The 456 batch, training loss is 0.1026296615600586\n",
            "The 457 batch, training loss is 0.2139001190662384\n",
            "The 458 batch, training loss is 0.10619310289621353\n",
            "The 459 batch, training loss is 0.1322910040616989\n",
            "The 460 batch, training loss is 0.2039431780576706\n",
            "The 461 batch, training loss is 0.20114129781723022\n",
            "The 462 batch, training loss is 0.17866884171962738\n",
            "The 463 batch, training loss is 0.3870468735694885\n",
            "The 464 batch, training loss is 0.31967994570732117\n",
            "The 465 batch, training loss is 0.19590038061141968\n",
            "The 466 batch, training loss is 0.1322176605463028\n",
            "The 467 batch, training loss is 0.1481323093175888\n",
            "The 468 batch, training loss is 0.2451411485671997\n",
            "The 469 batch, training loss is 0.26961377263069153\n",
            "The 470 batch, training loss is 0.13794228434562683\n",
            "The 471 batch, training loss is 0.1366521269083023\n",
            "The 472 batch, training loss is 0.14223627746105194\n",
            "The 473 batch, training loss is 0.23625023663043976\n",
            "The 474 batch, training loss is 0.16091234982013702\n",
            "The 475 batch, training loss is 0.08992517739534378\n",
            "The 476 batch, training loss is 0.4133768379688263\n",
            "The 477 batch, training loss is 0.04230835661292076\n",
            "The 478 batch, training loss is 0.40027955174446106\n",
            "The 479 batch, training loss is 0.15851952135562897\n",
            "The 480 batch, training loss is 0.27898234128952026\n",
            "The 481 batch, training loss is 0.17901872098445892\n",
            "The 482 batch, training loss is 0.15948441624641418\n",
            "The 483 batch, training loss is 0.22527338564395905\n",
            "The 484 batch, training loss is 0.22768089175224304\n",
            "The 485 batch, training loss is 0.0907365009188652\n",
            "The 486 batch, training loss is 0.19673246145248413\n",
            "The 487 batch, training loss is 0.06552398949861526\n",
            "The 488 batch, training loss is 0.15029045939445496\n",
            "The 489 batch, training loss is 0.24190489947795868\n",
            "The 490 batch, training loss is 0.36498045921325684\n",
            "The 491 batch, training loss is 0.3917371928691864\n",
            "The 492 batch, training loss is 0.2587670385837555\n",
            "The 493 batch, training loss is 0.33428144454956055\n",
            "The 494 batch, training loss is 0.275730162858963\n",
            "The 495 batch, training loss is 0.2692800462245941\n",
            "The 496 batch, training loss is 0.09730805456638336\n",
            "The 497 batch, training loss is 0.14749334752559662\n",
            "The 498 batch, training loss is 0.211810901761055\n",
            "The 499 batch, training loss is 0.12667331099510193\n",
            "The 500 batch, training loss is 0.21334008872509003\n",
            "The 501 batch, training loss is 0.07544174045324326\n",
            "The 502 batch, training loss is 0.3632892370223999\n",
            "The 503 batch, training loss is 0.14200611412525177\n",
            "The 504 batch, training loss is 0.2224597930908203\n",
            "The 505 batch, training loss is 0.14642775058746338\n",
            "The 506 batch, training loss is 0.28545722365379333\n",
            "The 507 batch, training loss is 0.09650448709726334\n",
            "The 508 batch, training loss is 0.24235428869724274\n",
            "The 509 batch, training loss is 0.30044782161712646\n",
            "The 510 batch, training loss is 0.24531115591526031\n",
            "The 511 batch, training loss is 0.2538144886493683\n",
            "The 512 batch, training loss is 0.16864031553268433\n",
            "The 513 batch, training loss is 0.2201232612133026\n",
            "The 514 batch, training loss is 0.1929028034210205\n",
            "The 515 batch, training loss is 0.14525927603244781\n",
            "The 516 batch, training loss is 0.2559553384780884\n",
            "The 517 batch, training loss is 0.11978396028280258\n",
            "The 518 batch, training loss is 0.1261540800333023\n",
            "The 519 batch, training loss is 0.37333786487579346\n",
            "The 520 batch, training loss is 0.2543659210205078\n",
            "The 521 batch, training loss is 0.12078029662370682\n",
            "The 522 batch, training loss is 0.159543976187706\n",
            "The 523 batch, training loss is 0.07449343055486679\n",
            "The 524 batch, training loss is 0.19916345179080963\n",
            "The 525 batch, training loss is 0.22539299726486206\n",
            "The 526 batch, training loss is 0.12973669171333313\n",
            "The 527 batch, training loss is 0.1305140256881714\n",
            "The 528 batch, training loss is 0.13641171157360077\n",
            "The 529 batch, training loss is 0.22471939027309418\n",
            "The 530 batch, training loss is 0.12382268905639648\n",
            "The 531 batch, training loss is 0.14914438128471375\n",
            "The 532 batch, training loss is 0.20764581859111786\n",
            "The 533 batch, training loss is 0.19462522864341736\n",
            "The 534 batch, training loss is 0.1249823272228241\n",
            "The 535 batch, training loss is 0.23932592570781708\n",
            "The 536 batch, training loss is 0.08562754839658737\n",
            "The 537 batch, training loss is 0.10874921083450317\n",
            "The 538 batch, training loss is 0.15582771599292755\n",
            "The 539 batch, training loss is 0.36253800988197327\n",
            "The 540 batch, training loss is 0.2500426769256592\n",
            "The 541 batch, training loss is 0.22474461793899536\n",
            "The 542 batch, training loss is 0.2714332044124603\n",
            "The 543 batch, training loss is 0.20989632606506348\n",
            "The 544 batch, training loss is 0.21664032340049744\n",
            "The 545 batch, training loss is 0.2242635041475296\n",
            "The 546 batch, training loss is 0.22795777022838593\n",
            "The 547 batch, training loss is 0.1137673482298851\n",
            "The 548 batch, training loss is 0.22107823193073273\n",
            "The 549 batch, training loss is 0.2273794561624527\n",
            "The 550 batch, training loss is 0.34438225626945496\n",
            "The 551 batch, training loss is 0.19904227554798126\n",
            "The 552 batch, training loss is 0.31576722860336304\n",
            "The 553 batch, training loss is 0.1830078810453415\n",
            "The 554 batch, training loss is 0.09380963444709778\n",
            "The 555 batch, training loss is 0.11163313686847687\n",
            "The 556 batch, training loss is 0.5505714416503906\n",
            "The 557 batch, training loss is 0.08089248836040497\n",
            "The 558 batch, training loss is 0.24272844195365906\n",
            "The 559 batch, training loss is 0.23291124403476715\n",
            "The 560 batch, training loss is 0.16195708513259888\n",
            "The 561 batch, training loss is 0.0924321711063385\n",
            "The 562 batch, training loss is 0.21859881281852722\n",
            "The 563 batch, training loss is 0.16650062799453735\n",
            "The 564 batch, training loss is 0.10178424417972565\n",
            "The 565 batch, training loss is 0.17005115747451782\n",
            "The 566 batch, training loss is 0.19894523918628693\n",
            "The 567 batch, training loss is 0.18135559558868408\n",
            "The 568 batch, training loss is 0.2779165208339691\n",
            "The 569 batch, training loss is 0.16889458894729614\n",
            "The 570 batch, training loss is 0.25920554995536804\n",
            "The 571 batch, training loss is 0.14665552973747253\n",
            "The 572 batch, training loss is 0.20556308329105377\n",
            "The 573 batch, training loss is 0.13468053936958313\n",
            "The 574 batch, training loss is 0.3376346826553345\n",
            "The 575 batch, training loss is 0.32139551639556885\n",
            "The 576 batch, training loss is 0.05284005403518677\n",
            "The 577 batch, training loss is 0.1369069516658783\n",
            "The 578 batch, training loss is 0.13319537043571472\n",
            "The 579 batch, training loss is 0.29490453004837036\n",
            "The 580 batch, training loss is 0.12999454140663147\n",
            "The 581 batch, training loss is 0.253913015127182\n",
            "The 582 batch, training loss is 0.3105818033218384\n",
            "The 583 batch, training loss is 0.25678449869155884\n",
            "The 584 batch, training loss is 0.1873392015695572\n",
            "The 585 batch, training loss is 0.11068198084831238\n",
            "The 586 batch, training loss is 0.2974146604537964\n",
            "The 587 batch, training loss is 0.04612001031637192\n",
            "The 588 batch, training loss is 0.2589121162891388\n",
            "The 589 batch, training loss is 0.18900729715824127\n",
            "The 590 batch, training loss is 0.14402985572814941\n",
            "The 591 batch, training loss is 0.10349611192941666\n",
            "The 592 batch, training loss is 0.09070917963981628\n",
            "The 593 batch, training loss is 0.2446283996105194\n",
            "The 594 batch, training loss is 0.2136641889810562\n",
            "The 595 batch, training loss is 0.2126730978488922\n",
            "The 596 batch, training loss is 0.04402955248951912\n",
            "The 597 batch, training loss is 0.17147986590862274\n",
            "The 598 batch, training loss is 0.12048519402742386\n",
            "The 599 batch, training loss is 0.09736821800470352\n",
            "The 600 batch, training loss is 0.15676896274089813\n",
            "The 601 batch, training loss is 0.1825030893087387\n",
            "The 602 batch, training loss is 0.10820014029741287\n",
            "The 603 batch, training loss is 0.16977287828922272\n",
            "The 604 batch, training loss is 0.2696217894554138\n",
            "The 605 batch, training loss is 0.1470775157213211\n",
            "The 606 batch, training loss is 0.1945902556180954\n",
            "The 607 batch, training loss is 0.24549587070941925\n",
            "The 608 batch, training loss is 0.29901719093322754\n",
            "The 609 batch, training loss is 0.2914543151855469\n",
            "The 610 batch, training loss is 0.24639782309532166\n",
            "The 611 batch, training loss is 0.2654319405555725\n",
            "The 612 batch, training loss is 0.1508292257785797\n",
            "The 613 batch, training loss is 0.1092875599861145\n",
            "The 614 batch, training loss is 0.3253684639930725\n",
            "The 615 batch, training loss is 0.14365126192569733\n",
            "The 616 batch, training loss is 0.07041780650615692\n",
            "The 617 batch, training loss is 0.1341145783662796\n",
            "The 618 batch, training loss is 0.10317491739988327\n",
            "The 619 batch, training loss is 0.1308840811252594\n",
            "The 620 batch, training loss is 0.17379796504974365\n",
            "The 621 batch, training loss is 0.17094996571540833\n",
            "The 622 batch, training loss is 0.2478369027376175\n",
            "The 623 batch, training loss is 0.06768692284822464\n",
            "The 624 batch, training loss is 0.12477847933769226\n",
            "The 625 batch, training loss is 0.3227938413619995\n",
            "The 626 batch, training loss is 0.1291930079460144\n",
            "The 627 batch, training loss is 0.21848461031913757\n",
            "The 628 batch, training loss is 0.25237035751342773\n",
            "The 629 batch, training loss is 0.36885449290275574\n",
            "The 630 batch, training loss is 0.2222883254289627\n",
            "The 631 batch, training loss is 0.10723071545362473\n",
            "The 632 batch, training loss is 0.26890283823013306\n",
            "The 633 batch, training loss is 0.2616298198699951\n",
            "The 634 batch, training loss is 0.14579252898693085\n",
            "The 635 batch, training loss is 0.13742120563983917\n",
            "The 636 batch, training loss is 0.16165076196193695\n",
            "The 637 batch, training loss is 0.22652298212051392\n",
            "The 638 batch, training loss is 0.09555865824222565\n",
            "The 639 batch, training loss is 0.17032073438167572\n",
            "The 640 batch, training loss is 0.07372274994850159\n",
            "The 641 batch, training loss is 0.18389223515987396\n",
            "The 642 batch, training loss is 0.1762697547674179\n",
            "The 643 batch, training loss is 0.19398897886276245\n",
            "The 644 batch, training loss is 0.2907503843307495\n",
            "The 645 batch, training loss is 0.16933554410934448\n",
            "The 646 batch, training loss is 0.28556644916534424\n",
            "The 647 batch, training loss is 0.20712371170520782\n",
            "The 648 batch, training loss is 0.20485012233257294\n",
            "The 649 batch, training loss is 0.12841494381427765\n",
            "The 650 batch, training loss is 0.20524032413959503\n",
            "The 651 batch, training loss is 0.22553947567939758\n",
            "The 652 batch, training loss is 0.161658376455307\n",
            "The 653 batch, training loss is 0.1319217085838318\n",
            "The 654 batch, training loss is 0.14094802737236023\n",
            "The 655 batch, training loss is 0.15045082569122314\n",
            "The 656 batch, training loss is 0.10329872369766235\n",
            "The 657 batch, training loss is 0.17769479751586914\n",
            "The 658 batch, training loss is 0.180296391248703\n",
            "The 659 batch, training loss is 0.1584482192993164\n",
            "The 660 batch, training loss is 0.2475472092628479\n",
            "The 661 batch, training loss is 0.1511344313621521\n",
            "The 662 batch, training loss is 0.09916020929813385\n",
            "The 663 batch, training loss is 0.07136936485767365\n",
            "The 664 batch, training loss is 0.07564278692007065\n",
            "The 665 batch, training loss is 0.11837708204984665\n",
            "The 666 batch, training loss is 0.12229059636592865\n",
            "The 667 batch, training loss is 0.22884856164455414\n",
            "The 668 batch, training loss is 0.15236152708530426\n",
            "The 669 batch, training loss is 0.17802351713180542\n",
            "The 670 batch, training loss is 0.16291572153568268\n",
            "The 671 batch, training loss is 0.2530753016471863\n",
            "The 672 batch, training loss is 0.5724138021469116\n",
            "The 673 batch, training loss is 0.13272421061992645\n",
            "The 674 batch, training loss is 0.25822675228118896\n",
            "The 675 batch, training loss is 0.21824944019317627\n",
            "The 676 batch, training loss is 0.27037325501441956\n",
            "The 677 batch, training loss is 0.09109806269407272\n",
            "The 678 batch, training loss is 0.1742178499698639\n",
            "The 679 batch, training loss is 0.20768100023269653\n",
            "The 680 batch, training loss is 0.146476611495018\n",
            "The 681 batch, training loss is 0.3323317766189575\n",
            "The 682 batch, training loss is 0.12651704251766205\n",
            "The 683 batch, training loss is 0.21185928583145142\n",
            "The 684 batch, training loss is 0.16217011213302612\n",
            "The 685 batch, training loss is 0.08876819163560867\n",
            "The 686 batch, training loss is 0.1724943220615387\n",
            "The 687 batch, training loss is 0.12657403945922852\n",
            "The 688 batch, training loss is 0.4676227569580078\n",
            "The 689 batch, training loss is 0.10765747725963593\n",
            "The 690 batch, training loss is 0.1159094050526619\n",
            "The 691 batch, training loss is 0.2119663506746292\n",
            "The 692 batch, training loss is 0.1324349194765091\n",
            "The 693 batch, training loss is 0.38805365562438965\n",
            "The 694 batch, training loss is 0.26377439498901367\n",
            "The 695 batch, training loss is 0.1665433794260025\n",
            "The 696 batch, training loss is 0.2638968825340271\n",
            "The 697 batch, training loss is 0.11672952026128769\n",
            "The 698 batch, training loss is 0.14719004929065704\n",
            "The 699 batch, training loss is 0.22641055285930634\n",
            "The 700 batch, training loss is 0.21012073755264282\n",
            "The 701 batch, training loss is 0.09639940410852432\n",
            "The 702 batch, training loss is 0.09398431330919266\n",
            "The 703 batch, training loss is 0.3655675947666168\n",
            "The 704 batch, training loss is 0.25847646594047546\n",
            "The 705 batch, training loss is 0.11803825944662094\n",
            "The 706 batch, training loss is 0.3120921552181244\n",
            "The 707 batch, training loss is 0.2298635095357895\n",
            "The 708 batch, training loss is 0.15986943244934082\n",
            "The 709 batch, training loss is 0.1430722326040268\n",
            "The 710 batch, training loss is 0.16656100749969482\n",
            "The 711 batch, training loss is 0.10964814573526382\n",
            "The 712 batch, training loss is 0.13877959549427032\n",
            "The 713 batch, training loss is 0.18415947258472443\n",
            "The 714 batch, training loss is 0.16689017415046692\n",
            "The 715 batch, training loss is 0.3094712495803833\n",
            "The 716 batch, training loss is 0.2169989049434662\n",
            "The 717 batch, training loss is 0.2971764802932739\n",
            "The 718 batch, training loss is 0.3602461516857147\n",
            "The 719 batch, training loss is 0.15325792133808136\n",
            "The 720 batch, training loss is 0.16319534182548523\n",
            "The 721 batch, training loss is 0.21759848296642303\n",
            "The 722 batch, training loss is 0.1486833095550537\n",
            "The 723 batch, training loss is 0.1004491001367569\n",
            "The 724 batch, training loss is 0.10645957291126251\n",
            "The 725 batch, training loss is 0.14538954198360443\n",
            "The 726 batch, training loss is 0.12635920941829681\n",
            "The 727 batch, training loss is 0.36144018173217773\n",
            "The 728 batch, training loss is 0.1371685415506363\n",
            "The 729 batch, training loss is 0.20782281458377838\n",
            "The 730 batch, training loss is 0.2922581136226654\n",
            "The 731 batch, training loss is 0.16103126108646393\n",
            "The 732 batch, training loss is 0.0620841383934021\n",
            "The 733 batch, training loss is 0.10577823966741562\n",
            "The 734 batch, training loss is 0.11227384209632874\n",
            "The 735 batch, training loss is 0.17123578488826752\n",
            "The 736 batch, training loss is 0.25959649682044983\n",
            "The 737 batch, training loss is 0.08452820777893066\n",
            "The 738 batch, training loss is 0.20055638253688812\n",
            "The 739 batch, training loss is 0.133636936545372\n",
            "The 740 batch, training loss is 0.24507589638233185\n",
            "The 741 batch, training loss is 0.17308872938156128\n",
            "The 742 batch, training loss is 0.1950688660144806\n",
            "The 743 batch, training loss is 0.1795625537633896\n",
            "The 744 batch, training loss is 0.24463365972042084\n",
            "The 745 batch, training loss is 0.22350206971168518\n",
            "The 746 batch, training loss is 0.26642245054244995\n",
            "The 747 batch, training loss is 0.15044237673282623\n",
            "The 748 batch, training loss is 0.22727344930171967\n",
            "The 749 batch, training loss is 0.1723335236310959\n",
            "The 750 batch, training loss is 0.2219492495059967\n",
            "The 751 batch, training loss is 0.16355516016483307\n",
            "The 752 batch, training loss is 0.20271337032318115\n",
            "The 753 batch, training loss is 0.17005014419555664\n",
            "The 754 batch, training loss is 0.09561450034379959\n",
            "The 755 batch, training loss is 0.08592233061790466\n",
            "The 756 batch, training loss is 0.15643109381198883\n",
            "The 757 batch, training loss is 0.12205462157726288\n",
            "The 758 batch, training loss is 0.11449145525693893\n",
            "The 759 batch, training loss is 0.11089560389518738\n",
            "The 760 batch, training loss is 0.33309316635131836\n",
            "The 761 batch, training loss is 0.08038561791181564\n",
            "The 762 batch, training loss is 0.16331636905670166\n",
            "The 763 batch, training loss is 0.10179880261421204\n",
            "The 764 batch, training loss is 0.09033656865358353\n",
            "The 765 batch, training loss is 0.23376864194869995\n",
            "The 766 batch, training loss is 0.1339818388223648\n",
            "The 767 batch, training loss is 0.16685451567173004\n",
            "The 768 batch, training loss is 0.09793680161237717\n",
            "The 769 batch, training loss is 0.08647284656763077\n",
            "The 770 batch, training loss is 0.24531295895576477\n",
            "The 771 batch, training loss is 0.20438683032989502\n",
            "The 772 batch, training loss is 0.10027053207159042\n",
            "The 773 batch, training loss is 0.15739130973815918\n",
            "The 774 batch, training loss is 0.2412455976009369\n",
            "The 775 batch, training loss is 0.13597674667835236\n",
            "The 776 batch, training loss is 0.0995241031050682\n",
            "The 777 batch, training loss is 0.2502697706222534\n",
            "The 778 batch, training loss is 0.0696428194642067\n",
            "The 779 batch, training loss is 0.10689849406480789\n",
            "The 780 batch, training loss is 0.13443875312805176\n",
            "The 781 batch, training loss is 0.13888350129127502\n",
            "The 782 batch, training loss is 0.09414615482091904\n",
            "The 783 batch, training loss is 0.13128593564033508\n",
            "The 784 batch, training loss is 0.24349698424339294\n",
            "The 785 batch, training loss is 0.16702328622341156\n",
            "The 786 batch, training loss is 0.10163494199514389\n",
            "The 787 batch, training loss is 0.15133386850357056\n",
            "The 788 batch, training loss is 0.25337618589401245\n",
            "The 789 batch, training loss is 0.3234536349773407\n",
            "The 790 batch, training loss is 0.12188230454921722\n",
            "The 791 batch, training loss is 0.17482787370681763\n",
            "The 792 batch, training loss is 0.21723423898220062\n",
            "The 793 batch, training loss is 0.2702920436859131\n",
            "The 794 batch, training loss is 0.09620404243469238\n",
            "The 795 batch, training loss is 0.13390813767910004\n",
            "The 796 batch, training loss is 0.3341546356678009\n",
            "The 797 batch, training loss is 0.2059108316898346\n",
            "The 798 batch, training loss is 0.19227026402950287\n",
            "The 799 batch, training loss is 0.11225064843893051\n",
            "The 800 batch, training loss is 0.13074052333831787\n",
            "The 801 batch, training loss is 0.2186925858259201\n",
            "The 802 batch, training loss is 0.1685626208782196\n",
            "The 803 batch, training loss is 0.1364174485206604\n",
            "The 804 batch, training loss is 0.0788954570889473\n",
            "The 805 batch, training loss is 0.15318845212459564\n",
            "The 806 batch, training loss is 0.11941130459308624\n",
            "The 807 batch, training loss is 0.16595172882080078\n",
            "The 808 batch, training loss is 0.19953608512878418\n",
            "The 809 batch, training loss is 0.0823623538017273\n",
            "The 810 batch, training loss is 0.19207866489887238\n",
            "The 811 batch, training loss is 0.11775483191013336\n",
            "The 812 batch, training loss is 0.18428045511245728\n",
            "The 813 batch, training loss is 0.20361438393592834\n",
            "The 814 batch, training loss is 0.1827571839094162\n",
            "The 815 batch, training loss is 0.10462810844182968\n",
            "The 816 batch, training loss is 0.07547424733638763\n",
            "The 817 batch, training loss is 0.14197108149528503\n",
            "The 818 batch, training loss is 0.30069291591644287\n",
            "The 819 batch, training loss is 0.2439999133348465\n",
            "The 820 batch, training loss is 0.19311583042144775\n",
            "The 821 batch, training loss is 0.12911944091320038\n",
            "The 822 batch, training loss is 0.23483122885227203\n",
            "The 823 batch, training loss is 0.1755559742450714\n",
            "The 824 batch, training loss is 0.18463724851608276\n",
            "The 825 batch, training loss is 0.13984717428684235\n",
            "The 826 batch, training loss is 0.31249141693115234\n",
            "The 827 batch, training loss is 0.277107298374176\n",
            "The 828 batch, training loss is 0.1433711051940918\n",
            "The 829 batch, training loss is 0.31633424758911133\n",
            "The 830 batch, training loss is 0.06015922874212265\n",
            "The 831 batch, training loss is 0.20236776769161224\n",
            "The 832 batch, training loss is 0.23663930594921112\n",
            "The 833 batch, training loss is 0.10466961562633514\n",
            "The 834 batch, training loss is 0.27987077832221985\n",
            "The 835 batch, training loss is 0.18156076967716217\n",
            "The 836 batch, training loss is 0.23266619443893433\n",
            "The 837 batch, training loss is 0.34992605447769165\n",
            "The 838 batch, training loss is 0.13261808454990387\n",
            "The 839 batch, training loss is 0.10080577433109283\n",
            "The 840 batch, training loss is 0.17995910346508026\n",
            "The 841 batch, training loss is 0.22587601840496063\n",
            "The 842 batch, training loss is 0.10848857462406158\n",
            "The 843 batch, training loss is 0.257417231798172\n",
            "The 844 batch, training loss is 0.48652470111846924\n",
            "The 845 batch, training loss is 0.18912070989608765\n",
            "The 846 batch, training loss is 0.25730210542678833\n",
            "The 847 batch, training loss is 0.10289249569177628\n",
            "The 848 batch, training loss is 0.3818190395832062\n",
            "The 849 batch, training loss is 0.13503704965114594\n",
            "The 850 batch, training loss is 0.10209880769252777\n",
            "The 851 batch, training loss is 0.2608475983142853\n",
            "The 852 batch, training loss is 0.15596932172775269\n",
            "The 853 batch, training loss is 0.19267365336418152\n",
            "The 854 batch, training loss is 0.15899336338043213\n",
            "The 855 batch, training loss is 0.3540022671222687\n",
            "The 856 batch, training loss is 0.1877957433462143\n",
            "The 857 batch, training loss is 0.0981949046254158\n",
            "The 858 batch, training loss is 0.2399226576089859\n",
            "The 859 batch, training loss is 0.12611514329910278\n",
            "The 860 batch, training loss is 0.0935855284333229\n",
            "The 861 batch, training loss is 0.15774744749069214\n",
            "The 862 batch, training loss is 0.2055298089981079\n",
            "The 863 batch, training loss is 0.1769580990076065\n",
            "The 864 batch, training loss is 0.20506200194358826\n",
            "The 865 batch, training loss is 0.20133401453495026\n",
            "The 866 batch, training loss is 0.07351203262805939\n",
            "The 867 batch, training loss is 0.04457104206085205\n",
            "The 868 batch, training loss is 0.30159133672714233\n",
            "The 869 batch, training loss is 0.2179829478263855\n",
            "The 870 batch, training loss is 0.2169846147298813\n",
            "The 871 batch, training loss is 0.13116635382175446\n",
            "The 872 batch, training loss is 0.11146211624145508\n",
            "The 873 batch, training loss is 0.17635999619960785\n",
            "The 874 batch, training loss is 0.037125226110219955\n",
            "The 875 batch, training loss is 0.1114385649561882\n",
            "The 876 batch, training loss is 0.15779559314250946\n",
            "The 877 batch, training loss is 0.2207285314798355\n",
            "The 878 batch, training loss is 0.11460672318935394\n",
            "The 879 batch, training loss is 0.19802775979042053\n",
            "The 880 batch, training loss is 0.1440049409866333\n",
            "The 881 batch, training loss is 0.15428341925144196\n",
            "The 882 batch, training loss is 0.09299024939537048\n",
            "The 883 batch, training loss is 0.4038790166378021\n",
            "The 884 batch, training loss is 0.29950085282325745\n",
            "The 885 batch, training loss is 0.32825905084609985\n",
            "The 886 batch, training loss is 0.12888404726982117\n",
            "The 887 batch, training loss is 0.38149014115333557\n",
            "The 888 batch, training loss is 0.31815779209136963\n",
            "The 889 batch, training loss is 0.34195148944854736\n",
            "The 890 batch, training loss is 0.14523212611675262\n",
            "The 891 batch, training loss is 0.1671660840511322\n",
            "The 892 batch, training loss is 0.2781939208507538\n",
            "The 893 batch, training loss is 0.19620805978775024\n",
            "The 894 batch, training loss is 0.1973768174648285\n",
            "The 895 batch, training loss is 0.11880628764629364\n",
            "The 896 batch, training loss is 0.0769069716334343\n",
            "The 897 batch, training loss is 0.16948077082633972\n",
            "The 898 batch, training loss is 0.1265653669834137\n",
            "The 899 batch, training loss is 0.2293034940958023\n",
            "The 900 batch, training loss is 0.20788876712322235\n",
            "The 901 batch, training loss is 0.2148956060409546\n",
            "The 902 batch, training loss is 0.313444048166275\n",
            "The 903 batch, training loss is 0.10694941133260727\n",
            "The 904 batch, training loss is 0.1481742560863495\n",
            "The 905 batch, training loss is 0.07893580943346024\n",
            "The 906 batch, training loss is 0.15075832605361938\n",
            "The 907 batch, training loss is 0.2280357927083969\n",
            "The 908 batch, training loss is 0.27671554684638977\n",
            "The 909 batch, training loss is 0.13624590635299683\n",
            "The 910 batch, training loss is 0.0719459280371666\n",
            "The 911 batch, training loss is 0.13489575684070587\n",
            "The 912 batch, training loss is 0.1454436033964157\n",
            "The 913 batch, training loss is 0.18359579145908356\n",
            "The 914 batch, training loss is 0.05557901784777641\n",
            "The 915 batch, training loss is 0.4533802568912506\n",
            "The 916 batch, training loss is 0.10799401253461838\n",
            "The 917 batch, training loss is 0.33433184027671814\n",
            "The 918 batch, training loss is 0.27217897772789\n",
            "The 919 batch, training loss is 0.058032821863889694\n",
            "The 920 batch, training loss is 0.074534572660923\n",
            "The 921 batch, training loss is 0.08835180848836899\n",
            "The 922 batch, training loss is 0.0575287826359272\n",
            "The 923 batch, training loss is 0.32784196734428406\n",
            "The 924 batch, training loss is 0.1129738986492157\n",
            "The 925 batch, training loss is 0.14653654396533966\n",
            "The 926 batch, training loss is 0.26181578636169434\n",
            "The 927 batch, training loss is 0.07779018580913544\n",
            "The 928 batch, training loss is 0.1337888538837433\n",
            "The 929 batch, training loss is 0.395347535610199\n",
            "The 930 batch, training loss is 0.2468279004096985\n",
            "The 931 batch, training loss is 0.13667982816696167\n",
            "The 932 batch, training loss is 0.09260744601488113\n",
            "The 933 batch, training loss is 0.17757439613342285\n",
            "The 934 batch, training loss is 0.08643384277820587\n",
            "The 935 batch, training loss is 0.22530506551265717\n",
            "The 936 batch, training loss is 0.20260858535766602\n",
            "The 937 batch, training loss is 0.4003031253814697\n",
            "The 5 epoch, training loss is 0.4003031253814697\n",
            "The 0 batch, training loss is 0.07986247539520264\n",
            "The 1 batch, training loss is 0.07241938263177872\n",
            "The 2 batch, training loss is 0.0710393562912941\n",
            "The 3 batch, training loss is 0.14869695901870728\n",
            "The 4 batch, training loss is 0.20285092294216156\n",
            "The 5 batch, training loss is 0.17233142256736755\n",
            "The 6 batch, training loss is 0.21113626658916473\n",
            "The 7 batch, training loss is 0.2697000801563263\n",
            "The 8 batch, training loss is 0.42346641421318054\n",
            "The 9 batch, training loss is 0.13833609223365784\n",
            "The 10 batch, training loss is 0.10304845124483109\n",
            "The 11 batch, training loss is 0.14623196423053741\n",
            "The 12 batch, training loss is 0.1099649965763092\n",
            "The 13 batch, training loss is 0.22147607803344727\n",
            "The 14 batch, training loss is 0.13157837092876434\n",
            "The 15 batch, training loss is 0.08691990375518799\n",
            "The 16 batch, training loss is 0.045360494405031204\n",
            "The 17 batch, training loss is 0.258176326751709\n",
            "The 18 batch, training loss is 0.10334517061710358\n",
            "The 19 batch, training loss is 0.09843410551548004\n",
            "The 20 batch, training loss is 0.11339475959539413\n",
            "The 21 batch, training loss is 0.09680085629224777\n",
            "The 22 batch, training loss is 0.09403637796640396\n",
            "The 23 batch, training loss is 0.2082032561302185\n",
            "The 24 batch, training loss is 0.0732775330543518\n",
            "The 25 batch, training loss is 0.16523253917694092\n",
            "The 26 batch, training loss is 0.08824112266302109\n",
            "The 27 batch, training loss is 0.1266467124223709\n",
            "The 28 batch, training loss is 0.21566897630691528\n",
            "The 29 batch, training loss is 0.2907945215702057\n",
            "The 30 batch, training loss is 0.10006740689277649\n",
            "The 31 batch, training loss is 0.11948683112859726\n",
            "The 32 batch, training loss is 0.15709558129310608\n",
            "The 33 batch, training loss is 0.3723965287208557\n",
            "The 34 batch, training loss is 0.1335776299238205\n",
            "The 35 batch, training loss is 0.2168905884027481\n",
            "The 36 batch, training loss is 0.19734354317188263\n",
            "The 37 batch, training loss is 0.275018572807312\n",
            "The 38 batch, training loss is 0.22579099237918854\n",
            "The 39 batch, training loss is 0.16116508841514587\n",
            "The 40 batch, training loss is 0.09078913182020187\n",
            "The 41 batch, training loss is 0.16015812754631042\n",
            "The 42 batch, training loss is 0.11279788613319397\n",
            "The 43 batch, training loss is 0.21387086808681488\n",
            "The 44 batch, training loss is 0.13642190396785736\n",
            "The 45 batch, training loss is 0.13634738326072693\n",
            "The 46 batch, training loss is 0.23051442205905914\n",
            "The 47 batch, training loss is 0.136734738945961\n",
            "The 48 batch, training loss is 0.13207003474235535\n",
            "The 49 batch, training loss is 0.19002975523471832\n",
            "The 50 batch, training loss is 0.16192543506622314\n",
            "The 51 batch, training loss is 0.08613722771406174\n",
            "The 52 batch, training loss is 0.06550723314285278\n",
            "The 53 batch, training loss is 0.1146465316414833\n",
            "The 54 batch, training loss is 0.19370323419570923\n",
            "The 55 batch, training loss is 0.10579371452331543\n",
            "The 56 batch, training loss is 0.2275092601776123\n",
            "The 57 batch, training loss is 0.0928928405046463\n",
            "The 58 batch, training loss is 0.2129184454679489\n",
            "The 59 batch, training loss is 0.10036267340183258\n",
            "The 60 batch, training loss is 0.0656827837228775\n",
            "The 61 batch, training loss is 0.05979793518781662\n",
            "The 62 batch, training loss is 0.19907528162002563\n",
            "The 63 batch, training loss is 0.10089336335659027\n",
            "The 64 batch, training loss is 0.1970253586769104\n",
            "The 65 batch, training loss is 0.20692192018032074\n",
            "The 66 batch, training loss is 0.1646045446395874\n",
            "The 67 batch, training loss is 0.09020639210939407\n",
            "The 68 batch, training loss is 0.06554273515939713\n",
            "The 69 batch, training loss is 0.12125290930271149\n",
            "The 70 batch, training loss is 0.32780957221984863\n",
            "The 71 batch, training loss is 0.22426211833953857\n",
            "The 72 batch, training loss is 0.16884499788284302\n",
            "The 73 batch, training loss is 0.14865925908088684\n",
            "The 74 batch, training loss is 0.12015856057405472\n",
            "The 75 batch, training loss is 0.05494118854403496\n",
            "The 76 batch, training loss is 0.2729471027851105\n",
            "The 77 batch, training loss is 0.4517625570297241\n",
            "The 78 batch, training loss is 0.19608686864376068\n",
            "The 79 batch, training loss is 0.23075701296329498\n",
            "The 80 batch, training loss is 0.12516091763973236\n",
            "The 81 batch, training loss is 0.06411575525999069\n",
            "The 82 batch, training loss is 0.2654099464416504\n",
            "The 83 batch, training loss is 0.21072925627231598\n",
            "The 84 batch, training loss is 0.40397486090660095\n",
            "The 85 batch, training loss is 0.23896630108356476\n",
            "The 86 batch, training loss is 0.08095947653055191\n",
            "The 87 batch, training loss is 0.26104113459587097\n",
            "The 88 batch, training loss is 0.26736146211624146\n",
            "The 89 batch, training loss is 0.11291374266147614\n",
            "The 90 batch, training loss is 0.11302411556243896\n",
            "The 91 batch, training loss is 0.10508041828870773\n",
            "The 92 batch, training loss is 0.14150463044643402\n",
            "The 93 batch, training loss is 0.18510673940181732\n",
            "The 94 batch, training loss is 0.2935357391834259\n",
            "The 95 batch, training loss is 0.27152425050735474\n",
            "The 96 batch, training loss is 0.19383148849010468\n",
            "The 97 batch, training loss is 0.17765924334526062\n",
            "The 98 batch, training loss is 0.17783516645431519\n",
            "The 99 batch, training loss is 0.17686641216278076\n",
            "The 100 batch, training loss is 0.28679731488227844\n",
            "The 101 batch, training loss is 0.14127643406391144\n",
            "The 102 batch, training loss is 0.17647214233875275\n",
            "The 103 batch, training loss is 0.10709116607904434\n",
            "The 104 batch, training loss is 0.12554705142974854\n",
            "The 105 batch, training loss is 0.10103508830070496\n",
            "The 106 batch, training loss is 0.11619814485311508\n",
            "The 107 batch, training loss is 0.4222836494445801\n",
            "The 108 batch, training loss is 0.21801802515983582\n",
            "The 109 batch, training loss is 0.10757425427436829\n",
            "The 110 batch, training loss is 0.20984669029712677\n",
            "The 111 batch, training loss is 0.13793137669563293\n",
            "The 112 batch, training loss is 0.1874476969242096\n",
            "The 113 batch, training loss is 0.14606401324272156\n",
            "The 114 batch, training loss is 0.09014369547367096\n",
            "The 115 batch, training loss is 0.07831352949142456\n",
            "The 116 batch, training loss is 0.21501286327838898\n",
            "The 117 batch, training loss is 0.22126120328903198\n",
            "The 118 batch, training loss is 0.1421097218990326\n",
            "The 119 batch, training loss is 0.15972024202346802\n",
            "The 120 batch, training loss is 0.09465985745191574\n",
            "The 121 batch, training loss is 0.3901909291744232\n",
            "The 122 batch, training loss is 0.13680411875247955\n",
            "The 123 batch, training loss is 0.09495755285024643\n",
            "The 124 batch, training loss is 0.1547475904226303\n",
            "The 125 batch, training loss is 0.24556145071983337\n",
            "The 126 batch, training loss is 0.21214990317821503\n",
            "The 127 batch, training loss is 0.27889546751976013\n",
            "The 128 batch, training loss is 0.11959968507289886\n",
            "The 129 batch, training loss is 0.1210462898015976\n",
            "The 130 batch, training loss is 0.15773247182369232\n",
            "The 131 batch, training loss is 0.2070806920528412\n",
            "The 132 batch, training loss is 0.12240289151668549\n",
            "The 133 batch, training loss is 0.24930110573768616\n",
            "The 134 batch, training loss is 0.15648098289966583\n",
            "The 135 batch, training loss is 0.27288249135017395\n",
            "The 136 batch, training loss is 0.1384809911251068\n",
            "The 137 batch, training loss is 0.20189237594604492\n",
            "The 138 batch, training loss is 0.29475781321525574\n",
            "The 139 batch, training loss is 0.10820003598928452\n",
            "The 140 batch, training loss is 0.20365209877490997\n",
            "The 141 batch, training loss is 0.12541845440864563\n",
            "The 142 batch, training loss is 0.0851471871137619\n",
            "The 143 batch, training loss is 0.15034563839435577\n",
            "The 144 batch, training loss is 0.2543371915817261\n",
            "The 145 batch, training loss is 0.11660763621330261\n",
            "The 146 batch, training loss is 0.20592813193798065\n",
            "The 147 batch, training loss is 0.20238283276557922\n",
            "The 148 batch, training loss is 0.12745530903339386\n",
            "The 149 batch, training loss is 0.19281743466854095\n",
            "The 150 batch, training loss is 0.1823807954788208\n",
            "The 151 batch, training loss is 0.11767198890447617\n",
            "The 152 batch, training loss is 0.25791236758232117\n",
            "The 153 batch, training loss is 0.07632087916135788\n",
            "The 154 batch, training loss is 0.24492564797401428\n",
            "The 155 batch, training loss is 0.27592623233795166\n",
            "The 156 batch, training loss is 0.1193784773349762\n",
            "The 157 batch, training loss is 0.1064387634396553\n",
            "The 158 batch, training loss is 0.07509730011224747\n",
            "The 159 batch, training loss is 0.11740104109048843\n",
            "The 160 batch, training loss is 0.19861570000648499\n",
            "The 161 batch, training loss is 0.14086100459098816\n",
            "The 162 batch, training loss is 0.17370113730430603\n",
            "The 163 batch, training loss is 0.20048007369041443\n",
            "The 164 batch, training loss is 0.10436442494392395\n",
            "The 165 batch, training loss is 0.13247840106487274\n",
            "The 166 batch, training loss is 0.09635494649410248\n",
            "The 167 batch, training loss is 0.17680098116397858\n",
            "The 168 batch, training loss is 0.33494898676872253\n",
            "The 169 batch, training loss is 0.0819707065820694\n",
            "The 170 batch, training loss is 0.2820189595222473\n",
            "The 171 batch, training loss is 0.3196309506893158\n",
            "The 172 batch, training loss is 0.16167095303535461\n",
            "The 173 batch, training loss is 0.08710715919733047\n",
            "The 174 batch, training loss is 0.25776880979537964\n",
            "The 175 batch, training loss is 0.16470368206501007\n",
            "The 176 batch, training loss is 0.09538770467042923\n",
            "The 177 batch, training loss is 0.1801319420337677\n",
            "The 178 batch, training loss is 0.16079194843769073\n",
            "The 179 batch, training loss is 0.1701916605234146\n",
            "The 180 batch, training loss is 0.2902573049068451\n",
            "The 181 batch, training loss is 0.13664506375789642\n",
            "The 182 batch, training loss is 0.21372351050376892\n",
            "The 183 batch, training loss is 0.21784737706184387\n",
            "The 184 batch, training loss is 0.069950170814991\n",
            "The 185 batch, training loss is 0.26100417971611023\n",
            "The 186 batch, training loss is 0.13504506647586823\n",
            "The 187 batch, training loss is 0.27732858061790466\n",
            "The 188 batch, training loss is 0.11076492071151733\n",
            "The 189 batch, training loss is 0.1748371124267578\n",
            "The 190 batch, training loss is 0.08617289364337921\n",
            "The 191 batch, training loss is 0.18667365610599518\n",
            "The 192 batch, training loss is 0.14099003374576569\n",
            "The 193 batch, training loss is 0.2002834528684616\n",
            "The 194 batch, training loss is 0.11476397514343262\n",
            "The 195 batch, training loss is 0.16132612526416779\n",
            "The 196 batch, training loss is 0.22221259772777557\n",
            "The 197 batch, training loss is 0.2970165014266968\n",
            "The 198 batch, training loss is 0.12124232202768326\n",
            "The 199 batch, training loss is 0.23777592182159424\n",
            "The 200 batch, training loss is 0.17741556465625763\n",
            "The 201 batch, training loss is 0.3029242753982544\n",
            "The 202 batch, training loss is 0.049145929515361786\n",
            "The 203 batch, training loss is 0.09571623057126999\n",
            "The 204 batch, training loss is 0.17497098445892334\n",
            "The 205 batch, training loss is 0.09700711816549301\n",
            "The 206 batch, training loss is 0.3105148673057556\n",
            "The 207 batch, training loss is 0.27391529083251953\n",
            "The 208 batch, training loss is 0.14586946368217468\n",
            "The 209 batch, training loss is 0.17329028248786926\n",
            "The 210 batch, training loss is 0.24402070045471191\n",
            "The 211 batch, training loss is 0.20087353885173798\n",
            "The 212 batch, training loss is 0.12241649627685547\n",
            "The 213 batch, training loss is 0.12953869998455048\n",
            "The 214 batch, training loss is 0.28916892409324646\n",
            "The 215 batch, training loss is 0.22254228591918945\n",
            "The 216 batch, training loss is 0.21230986714363098\n",
            "The 217 batch, training loss is 0.08836198598146439\n",
            "The 218 batch, training loss is 0.21241256594657898\n",
            "The 219 batch, training loss is 0.16986362636089325\n",
            "The 220 batch, training loss is 0.12420821934938431\n",
            "The 221 batch, training loss is 0.09946975111961365\n",
            "The 222 batch, training loss is 0.29008784890174866\n",
            "The 223 batch, training loss is 0.24322213232517242\n",
            "The 224 batch, training loss is 0.1748623251914978\n",
            "The 225 batch, training loss is 0.2054714411497116\n",
            "The 226 batch, training loss is 0.08168155699968338\n",
            "The 227 batch, training loss is 0.12603183090686798\n",
            "The 228 batch, training loss is 0.10516227036714554\n",
            "The 229 batch, training loss is 0.08673622459173203\n",
            "The 230 batch, training loss is 0.1884724348783493\n",
            "The 231 batch, training loss is 0.13436976075172424\n",
            "The 232 batch, training loss is 0.2051398605108261\n",
            "The 233 batch, training loss is 0.09356606006622314\n",
            "The 234 batch, training loss is 0.2561037540435791\n",
            "The 235 batch, training loss is 0.2133895307779312\n",
            "The 236 batch, training loss is 0.13855285942554474\n",
            "The 237 batch, training loss is 0.11527165025472641\n",
            "The 238 batch, training loss is 0.1252206712961197\n",
            "The 239 batch, training loss is 0.20638373494148254\n",
            "The 240 batch, training loss is 0.2972659766674042\n",
            "The 241 batch, training loss is 0.13115069270133972\n",
            "The 242 batch, training loss is 0.29391637444496155\n",
            "The 243 batch, training loss is 0.21567672491073608\n",
            "The 244 batch, training loss is 0.12272734940052032\n",
            "The 245 batch, training loss is 0.278369665145874\n",
            "The 246 batch, training loss is 0.10071859508752823\n",
            "The 247 batch, training loss is 0.1692875623703003\n",
            "The 248 batch, training loss is 0.1558115929365158\n",
            "The 249 batch, training loss is 0.16663870215415955\n",
            "The 250 batch, training loss is 0.19654671847820282\n",
            "The 251 batch, training loss is 0.10746543854475021\n",
            "The 252 batch, training loss is 0.2350674271583557\n",
            "The 253 batch, training loss is 0.129607692360878\n",
            "The 254 batch, training loss is 0.21810059249401093\n",
            "The 255 batch, training loss is 0.13977891206741333\n",
            "The 256 batch, training loss is 0.1739833950996399\n",
            "The 257 batch, training loss is 0.09745380282402039\n",
            "The 258 batch, training loss is 0.1268831342458725\n",
            "The 259 batch, training loss is 0.10084835439920425\n",
            "The 260 batch, training loss is 0.10051599144935608\n",
            "The 261 batch, training loss is 0.11521516740322113\n",
            "The 262 batch, training loss is 0.08620225638151169\n",
            "The 263 batch, training loss is 0.08872553706169128\n",
            "The 264 batch, training loss is 0.04164974018931389\n",
            "The 265 batch, training loss is 0.10384900122880936\n",
            "The 266 batch, training loss is 0.20521670579910278\n",
            "The 267 batch, training loss is 0.10189464688301086\n",
            "The 268 batch, training loss is 0.15336591005325317\n",
            "The 269 batch, training loss is 0.12120191752910614\n",
            "The 270 batch, training loss is 0.1404813528060913\n",
            "The 271 batch, training loss is 0.16843490302562714\n",
            "The 272 batch, training loss is 0.2541165351867676\n",
            "The 273 batch, training loss is 0.2524457573890686\n",
            "The 274 batch, training loss is 0.0910477340221405\n",
            "The 275 batch, training loss is 0.42659103870391846\n",
            "The 276 batch, training loss is 0.35219404101371765\n",
            "The 277 batch, training loss is 0.16283930838108063\n",
            "The 278 batch, training loss is 0.07503266632556915\n",
            "The 279 batch, training loss is 0.13197773694992065\n",
            "The 280 batch, training loss is 0.08523727208375931\n",
            "The 281 batch, training loss is 0.08182447403669357\n",
            "The 282 batch, training loss is 0.20040786266326904\n",
            "The 283 batch, training loss is 0.19319266080856323\n",
            "The 284 batch, training loss is 0.09213978797197342\n",
            "The 285 batch, training loss is 0.11014100164175034\n",
            "The 286 batch, training loss is 0.13027159869670868\n",
            "The 287 batch, training loss is 0.19507159292697906\n",
            "The 288 batch, training loss is 0.2457481175661087\n",
            "The 289 batch, training loss is 0.16826578974723816\n",
            "The 290 batch, training loss is 0.11879458278417587\n",
            "The 291 batch, training loss is 0.24566304683685303\n",
            "The 292 batch, training loss is 0.23692524433135986\n",
            "The 293 batch, training loss is 0.08005319535732269\n",
            "The 294 batch, training loss is 0.30387982726097107\n",
            "The 295 batch, training loss is 0.1401006281375885\n",
            "The 296 batch, training loss is 0.10668209940195084\n",
            "The 297 batch, training loss is 0.1964370310306549\n",
            "The 298 batch, training loss is 0.038237325847148895\n",
            "The 299 batch, training loss is 0.08178021758794785\n",
            "The 300 batch, training loss is 0.30996474623680115\n",
            "The 301 batch, training loss is 0.21172913908958435\n",
            "The 302 batch, training loss is 0.07610577344894409\n",
            "The 303 batch, training loss is 0.09365586936473846\n",
            "The 304 batch, training loss is 0.2632017135620117\n",
            "The 305 batch, training loss is 0.12210818380117416\n",
            "The 306 batch, training loss is 0.21794730424880981\n",
            "The 307 batch, training loss is 0.30592185258865356\n",
            "The 308 batch, training loss is 0.21939526498317719\n",
            "The 309 batch, training loss is 0.2374463528394699\n",
            "The 310 batch, training loss is 0.12750783562660217\n",
            "The 311 batch, training loss is 0.342454731464386\n",
            "The 312 batch, training loss is 0.2267421931028366\n",
            "The 313 batch, training loss is 0.10202782601118088\n",
            "The 314 batch, training loss is 0.1364559680223465\n",
            "The 315 batch, training loss is 0.12665468454360962\n",
            "The 316 batch, training loss is 0.12178505957126617\n",
            "The 317 batch, training loss is 0.13247129321098328\n",
            "The 318 batch, training loss is 0.11274903267621994\n",
            "The 319 batch, training loss is 0.2333429902791977\n",
            "The 320 batch, training loss is 0.2152549773454666\n",
            "The 321 batch, training loss is 0.16664443910121918\n",
            "The 322 batch, training loss is 0.05935182049870491\n",
            "The 323 batch, training loss is 0.04790051653981209\n",
            "The 324 batch, training loss is 0.25589028000831604\n",
            "The 325 batch, training loss is 0.16516903042793274\n",
            "The 326 batch, training loss is 0.15757060050964355\n",
            "The 327 batch, training loss is 0.06634537130594254\n",
            "The 328 batch, training loss is 0.15720465779304504\n",
            "The 329 batch, training loss is 0.21032030880451202\n",
            "The 330 batch, training loss is 0.1463097184896469\n",
            "The 331 batch, training loss is 0.41450661420822144\n",
            "The 332 batch, training loss is 0.19614262878894806\n",
            "The 333 batch, training loss is 0.20659585297107697\n",
            "The 334 batch, training loss is 0.39624854922294617\n",
            "The 335 batch, training loss is 0.09197226911783218\n",
            "The 336 batch, training loss is 0.19461019337177277\n",
            "The 337 batch, training loss is 0.0561191625893116\n",
            "The 338 batch, training loss is 0.15111970901489258\n",
            "The 339 batch, training loss is 0.15497635304927826\n",
            "The 340 batch, training loss is 0.10574384778738022\n",
            "The 341 batch, training loss is 0.06932145357131958\n",
            "The 342 batch, training loss is 0.16201509535312653\n",
            "The 343 batch, training loss is 0.12304403632879257\n",
            "The 344 batch, training loss is 0.10672935098409653\n",
            "The 345 batch, training loss is 0.11596989631652832\n",
            "The 346 batch, training loss is 0.29733991622924805\n",
            "The 347 batch, training loss is 0.07402098923921585\n",
            "The 348 batch, training loss is 0.05090547353029251\n",
            "The 349 batch, training loss is 0.2203712910413742\n",
            "The 350 batch, training loss is 0.12881064414978027\n",
            "The 351 batch, training loss is 0.18863430619239807\n",
            "The 352 batch, training loss is 0.38780537247657776\n",
            "The 353 batch, training loss is 0.11738868057727814\n",
            "The 354 batch, training loss is 0.08576109260320663\n",
            "The 355 batch, training loss is 0.12914171814918518\n",
            "The 356 batch, training loss is 0.09787198901176453\n",
            "The 357 batch, training loss is 0.0840039998292923\n",
            "The 358 batch, training loss is 0.2608166038990021\n",
            "The 359 batch, training loss is 0.1578194499015808\n",
            "The 360 batch, training loss is 0.14855051040649414\n",
            "The 361 batch, training loss is 0.2336544692516327\n",
            "The 362 batch, training loss is 0.16858060657978058\n",
            "The 363 batch, training loss is 0.29797497391700745\n",
            "The 364 batch, training loss is 0.16881215572357178\n",
            "The 365 batch, training loss is 0.08323941379785538\n",
            "The 366 batch, training loss is 0.19691219925880432\n",
            "The 367 batch, training loss is 0.17722222208976746\n",
            "The 368 batch, training loss is 0.2146470695734024\n",
            "The 369 batch, training loss is 0.18793562054634094\n",
            "The 370 batch, training loss is 0.1957891285419464\n",
            "The 371 batch, training loss is 0.15744726359844208\n",
            "The 372 batch, training loss is 0.09488969296216965\n",
            "The 373 batch, training loss is 0.14130136370658875\n",
            "The 374 batch, training loss is 0.1343611180782318\n",
            "The 375 batch, training loss is 0.21313512325286865\n",
            "The 376 batch, training loss is 0.11810529977083206\n",
            "The 377 batch, training loss is 0.16867999732494354\n",
            "The 378 batch, training loss is 0.34210044145584106\n",
            "The 379 batch, training loss is 0.10458967834711075\n",
            "The 380 batch, training loss is 0.12790745496749878\n",
            "The 381 batch, training loss is 0.09202844649553299\n",
            "The 382 batch, training loss is 0.22708427906036377\n",
            "The 383 batch, training loss is 0.07601108402013779\n",
            "The 384 batch, training loss is 0.12615254521369934\n",
            "The 385 batch, training loss is 0.263554185628891\n",
            "The 386 batch, training loss is 0.17894010245800018\n",
            "The 387 batch, training loss is 0.13782992959022522\n",
            "The 388 batch, training loss is 0.23991979658603668\n",
            "The 389 batch, training loss is 0.20220360159873962\n",
            "The 390 batch, training loss is 0.18640980124473572\n",
            "The 391 batch, training loss is 0.24309907853603363\n",
            "The 392 batch, training loss is 0.1199895516037941\n",
            "The 393 batch, training loss is 0.15702389180660248\n",
            "The 394 batch, training loss is 0.08968877047300339\n",
            "The 395 batch, training loss is 0.13062627613544464\n",
            "The 396 batch, training loss is 0.1712580770254135\n",
            "The 397 batch, training loss is 0.13718678057193756\n",
            "The 398 batch, training loss is 0.1588451862335205\n",
            "The 399 batch, training loss is 0.14806997776031494\n",
            "The 400 batch, training loss is 0.13640496134757996\n",
            "The 401 batch, training loss is 0.16928313672542572\n",
            "The 402 batch, training loss is 0.23617692291736603\n",
            "The 403 batch, training loss is 0.18636277318000793\n",
            "The 404 batch, training loss is 0.23660536110401154\n",
            "The 405 batch, training loss is 0.20202842354774475\n",
            "The 406 batch, training loss is 0.14772671461105347\n",
            "The 407 batch, training loss is 0.2230454385280609\n",
            "The 408 batch, training loss is 0.17374791204929352\n",
            "The 409 batch, training loss is 0.17287901043891907\n",
            "The 410 batch, training loss is 0.1164734810590744\n",
            "The 411 batch, training loss is 0.2087530493736267\n",
            "The 412 batch, training loss is 0.144331157207489\n",
            "The 413 batch, training loss is 0.1370604932308197\n",
            "The 414 batch, training loss is 0.12498217076063156\n",
            "The 415 batch, training loss is 0.11799780279397964\n",
            "The 416 batch, training loss is 0.20540225505828857\n",
            "The 417 batch, training loss is 0.23239415884017944\n",
            "The 418 batch, training loss is 0.13069505989551544\n",
            "The 419 batch, training loss is 0.1120598241686821\n",
            "The 420 batch, training loss is 0.08572396636009216\n",
            "The 421 batch, training loss is 0.1596655696630478\n",
            "The 422 batch, training loss is 0.12655319273471832\n",
            "The 423 batch, training loss is 0.34899449348449707\n",
            "The 424 batch, training loss is 0.23520946502685547\n",
            "The 425 batch, training loss is 0.15496286749839783\n",
            "The 426 batch, training loss is 0.09487365186214447\n",
            "The 427 batch, training loss is 0.08647777885198593\n",
            "The 428 batch, training loss is 0.10464209318161011\n",
            "The 429 batch, training loss is 0.07552571594715118\n",
            "The 430 batch, training loss is 0.12402604520320892\n",
            "The 431 batch, training loss is 0.17165414988994598\n",
            "The 432 batch, training loss is 0.1525905579328537\n",
            "The 433 batch, training loss is 0.06614905595779419\n",
            "The 434 batch, training loss is 0.15952852368354797\n",
            "The 435 batch, training loss is 0.09940464049577713\n",
            "The 436 batch, training loss is 0.06301024556159973\n",
            "The 437 batch, training loss is 0.18118777871131897\n",
            "The 438 batch, training loss is 0.1994626522064209\n",
            "The 439 batch, training loss is 0.18843863904476166\n",
            "The 440 batch, training loss is 0.055127326399087906\n",
            "The 441 batch, training loss is 0.0591503269970417\n",
            "The 442 batch, training loss is 0.23737257719039917\n",
            "The 443 batch, training loss is 0.15973563492298126\n",
            "The 444 batch, training loss is 0.3317721486091614\n",
            "The 445 batch, training loss is 0.23222124576568604\n",
            "The 446 batch, training loss is 0.13346703350543976\n",
            "The 447 batch, training loss is 0.2786930203437805\n",
            "The 448 batch, training loss is 0.16531169414520264\n",
            "The 449 batch, training loss is 0.1416100114583969\n",
            "The 450 batch, training loss is 0.172999769449234\n",
            "The 451 batch, training loss is 0.26284343004226685\n",
            "The 452 batch, training loss is 0.16453790664672852\n",
            "The 453 batch, training loss is 0.11861278116703033\n",
            "The 454 batch, training loss is 0.09562237560749054\n",
            "The 455 batch, training loss is 0.12564580142498016\n",
            "The 456 batch, training loss is 0.25746122002601624\n",
            "The 457 batch, training loss is 0.06964150071144104\n",
            "The 458 batch, training loss is 0.10946334898471832\n",
            "The 459 batch, training loss is 0.32530295848846436\n",
            "The 460 batch, training loss is 0.3354954719543457\n",
            "The 461 batch, training loss is 0.15703034400939941\n",
            "The 462 batch, training loss is 0.17822252213954926\n",
            "The 463 batch, training loss is 0.0780855193734169\n",
            "The 464 batch, training loss is 0.07095415145158768\n",
            "The 465 batch, training loss is 0.23457571864128113\n",
            "The 466 batch, training loss is 0.16521809995174408\n",
            "The 467 batch, training loss is 0.10001428425312042\n",
            "The 468 batch, training loss is 0.13069909811019897\n",
            "The 469 batch, training loss is 0.12768462300300598\n",
            "The 470 batch, training loss is 0.24215851724147797\n",
            "The 471 batch, training loss is 0.04202890023589134\n",
            "The 472 batch, training loss is 0.17767788469791412\n",
            "The 473 batch, training loss is 0.1840246021747589\n",
            "The 474 batch, training loss is 0.08508525043725967\n",
            "The 475 batch, training loss is 0.2548508048057556\n",
            "The 476 batch, training loss is 0.0977899581193924\n",
            "The 477 batch, training loss is 0.22583571076393127\n",
            "The 478 batch, training loss is 0.10796533524990082\n",
            "The 479 batch, training loss is 0.054623477160930634\n",
            "The 480 batch, training loss is 0.11452672630548477\n",
            "The 481 batch, training loss is 0.2952379286289215\n",
            "The 482 batch, training loss is 0.36174318194389343\n",
            "The 483 batch, training loss is 0.20285086333751678\n",
            "The 484 batch, training loss is 0.07823211699724197\n",
            "The 485 batch, training loss is 0.07117091864347458\n",
            "The 486 batch, training loss is 0.15804921090602875\n",
            "The 487 batch, training loss is 0.12677563726902008\n",
            "The 488 batch, training loss is 0.145516037940979\n",
            "The 489 batch, training loss is 0.19537587463855743\n",
            "The 490 batch, training loss is 0.20590484142303467\n",
            "The 491 batch, training loss is 0.16630135476589203\n",
            "The 492 batch, training loss is 0.0968710333108902\n",
            "The 493 batch, training loss is 0.15882223844528198\n",
            "The 494 batch, training loss is 0.28274571895599365\n",
            "The 495 batch, training loss is 0.06328149139881134\n",
            "The 496 batch, training loss is 0.059090979397296906\n",
            "The 497 batch, training loss is 0.35163235664367676\n",
            "The 498 batch, training loss is 0.10845126211643219\n",
            "The 499 batch, training loss is 0.15409192442893982\n",
            "The 500 batch, training loss is 0.2896932065486908\n",
            "The 501 batch, training loss is 0.13273648917675018\n",
            "The 502 batch, training loss is 0.05176727473735809\n",
            "The 503 batch, training loss is 0.15845026075839996\n",
            "The 504 batch, training loss is 0.16532421112060547\n",
            "The 505 batch, training loss is 0.17320886254310608\n",
            "The 506 batch, training loss is 0.1638704240322113\n",
            "The 507 batch, training loss is 0.19997619092464447\n",
            "The 508 batch, training loss is 0.0961306244134903\n",
            "The 509 batch, training loss is 0.5094095468521118\n",
            "The 510 batch, training loss is 0.1420268714427948\n",
            "The 511 batch, training loss is 0.12432848662137985\n",
            "The 512 batch, training loss is 0.20731452107429504\n",
            "The 513 batch, training loss is 0.13014620542526245\n",
            "The 514 batch, training loss is 0.2622024118900299\n",
            "The 515 batch, training loss is 0.2526819109916687\n",
            "The 516 batch, training loss is 0.23663592338562012\n",
            "The 517 batch, training loss is 0.1015515848994255\n",
            "The 518 batch, training loss is 0.14608193933963776\n",
            "The 519 batch, training loss is 0.1439402848482132\n",
            "The 520 batch, training loss is 0.18260565400123596\n",
            "The 521 batch, training loss is 0.11181812733411789\n",
            "The 522 batch, training loss is 0.14719003438949585\n",
            "The 523 batch, training loss is 0.25919991731643677\n",
            "The 524 batch, training loss is 0.13278520107269287\n",
            "The 525 batch, training loss is 0.12916076183319092\n",
            "The 526 batch, training loss is 0.10482711344957352\n",
            "The 527 batch, training loss is 0.29807165265083313\n",
            "The 528 batch, training loss is 0.06537103652954102\n",
            "The 529 batch, training loss is 0.0679856389760971\n",
            "The 530 batch, training loss is 0.20931090414524078\n",
            "The 531 batch, training loss is 0.14367039501667023\n",
            "The 532 batch, training loss is 0.2599951922893524\n",
            "The 533 batch, training loss is 0.24009756743907928\n",
            "The 534 batch, training loss is 0.20475496351718903\n",
            "The 535 batch, training loss is 0.04010820388793945\n",
            "The 536 batch, training loss is 0.10927165299654007\n",
            "The 537 batch, training loss is 0.17676876485347748\n",
            "The 538 batch, training loss is 0.21603171527385712\n",
            "The 539 batch, training loss is 0.19666807353496552\n",
            "The 540 batch, training loss is 0.19307321310043335\n",
            "The 541 batch, training loss is 0.3283523917198181\n",
            "The 542 batch, training loss is 0.1300736367702484\n",
            "The 543 batch, training loss is 0.20509888231754303\n",
            "The 544 batch, training loss is 0.17693781852722168\n",
            "The 545 batch, training loss is 0.17886696755886078\n",
            "The 546 batch, training loss is 0.08608846366405487\n",
            "The 547 batch, training loss is 0.1573486626148224\n",
            "The 548 batch, training loss is 0.08352018892765045\n",
            "The 549 batch, training loss is 0.12373217940330505\n",
            "The 550 batch, training loss is 0.13662251830101013\n",
            "The 551 batch, training loss is 0.18587709963321686\n",
            "The 552 batch, training loss is 0.08064752072095871\n",
            "The 553 batch, training loss is 0.1667170524597168\n",
            "The 554 batch, training loss is 0.17704853415489197\n",
            "The 555 batch, training loss is 0.11839142441749573\n",
            "The 556 batch, training loss is 0.19033394753932953\n",
            "The 557 batch, training loss is 0.1567174196243286\n",
            "The 558 batch, training loss is 0.13322044909000397\n",
            "The 559 batch, training loss is 0.055638637393713\n",
            "The 560 batch, training loss is 0.0876055434346199\n",
            "The 561 batch, training loss is 0.17772237956523895\n",
            "The 562 batch, training loss is 0.24149882793426514\n",
            "The 563 batch, training loss is 0.3764841556549072\n",
            "The 564 batch, training loss is 0.11988847702741623\n",
            "The 565 batch, training loss is 0.18353679776191711\n",
            "The 566 batch, training loss is 0.23569686710834503\n",
            "The 567 batch, training loss is 0.09313561022281647\n",
            "The 568 batch, training loss is 0.13852407038211823\n",
            "The 569 batch, training loss is 0.16846874356269836\n",
            "The 570 batch, training loss is 0.1243935227394104\n",
            "The 571 batch, training loss is 0.23841679096221924\n",
            "The 572 batch, training loss is 0.2572336196899414\n",
            "The 573 batch, training loss is 0.14981229603290558\n",
            "The 574 batch, training loss is 0.12787629663944244\n",
            "The 575 batch, training loss is 0.3848763704299927\n",
            "The 576 batch, training loss is 0.19301722943782806\n",
            "The 577 batch, training loss is 0.16932691633701324\n",
            "The 578 batch, training loss is 0.20323973894119263\n",
            "The 579 batch, training loss is 0.2826843857765198\n",
            "The 580 batch, training loss is 0.20124632120132446\n",
            "The 581 batch, training loss is 0.12923258543014526\n",
            "The 582 batch, training loss is 0.05790778249502182\n",
            "The 583 batch, training loss is 0.20120279490947723\n",
            "The 584 batch, training loss is 0.10224530845880508\n",
            "The 585 batch, training loss is 0.08328092098236084\n",
            "The 586 batch, training loss is 0.18690422177314758\n",
            "The 587 batch, training loss is 0.07852550595998764\n",
            "The 588 batch, training loss is 0.18700256943702698\n",
            "The 589 batch, training loss is 0.05265552178025246\n",
            "The 590 batch, training loss is 0.09945672750473022\n",
            "The 591 batch, training loss is 0.07856032997369766\n",
            "The 592 batch, training loss is 0.16736266016960144\n",
            "The 593 batch, training loss is 0.1931915134191513\n",
            "The 594 batch, training loss is 0.06929226219654083\n",
            "The 595 batch, training loss is 0.18275384604930878\n",
            "The 596 batch, training loss is 0.1055084615945816\n",
            "The 597 batch, training loss is 0.18561969697475433\n",
            "The 598 batch, training loss is 0.0792158916592598\n",
            "The 599 batch, training loss is 0.1455306112766266\n",
            "The 600 batch, training loss is 0.1886935830116272\n",
            "The 601 batch, training loss is 0.20005929470062256\n",
            "The 602 batch, training loss is 0.08618085831403732\n",
            "The 603 batch, training loss is 0.13631317019462585\n",
            "The 604 batch, training loss is 0.11279910057783127\n",
            "The 605 batch, training loss is 0.24279706180095673\n",
            "The 606 batch, training loss is 0.12168241292238235\n",
            "The 607 batch, training loss is 0.08087575435638428\n",
            "The 608 batch, training loss is 0.10227978974580765\n",
            "The 609 batch, training loss is 0.11865155398845673\n",
            "The 610 batch, training loss is 0.22633390128612518\n",
            "The 611 batch, training loss is 0.2061888575553894\n",
            "The 612 batch, training loss is 0.4559779167175293\n",
            "The 613 batch, training loss is 0.08927801996469498\n",
            "The 614 batch, training loss is 0.1529017835855484\n",
            "The 615 batch, training loss is 0.21789506077766418\n",
            "The 616 batch, training loss is 0.12510783970355988\n",
            "The 617 batch, training loss is 0.07189800590276718\n",
            "The 618 batch, training loss is 0.23989620804786682\n",
            "The 619 batch, training loss is 0.08634094148874283\n",
            "The 620 batch, training loss is 0.13593044877052307\n",
            "The 621 batch, training loss is 0.2216998040676117\n",
            "The 622 batch, training loss is 0.09953237324953079\n",
            "The 623 batch, training loss is 0.09438185393810272\n",
            "The 624 batch, training loss is 0.2698577642440796\n",
            "The 625 batch, training loss is 0.1768459975719452\n",
            "The 626 batch, training loss is 0.2630101442337036\n",
            "The 627 batch, training loss is 0.11694256216287613\n",
            "The 628 batch, training loss is 0.09042824804782867\n",
            "The 629 batch, training loss is 0.05265190079808235\n",
            "The 630 batch, training loss is 0.14501206576824188\n",
            "The 631 batch, training loss is 0.24289722740650177\n",
            "The 632 batch, training loss is 0.09710431843996048\n",
            "The 633 batch, training loss is 0.12386336922645569\n",
            "The 634 batch, training loss is 0.13808217644691467\n",
            "The 635 batch, training loss is 0.12622103095054626\n",
            "The 636 batch, training loss is 0.1617542803287506\n",
            "The 637 batch, training loss is 0.18165037035942078\n",
            "The 638 batch, training loss is 0.076090507209301\n",
            "The 639 batch, training loss is 0.158356711268425\n",
            "The 640 batch, training loss is 0.15425029397010803\n",
            "The 641 batch, training loss is 0.08798491209745407\n",
            "The 642 batch, training loss is 0.059721726924180984\n",
            "The 643 batch, training loss is 0.2034415453672409\n",
            "The 644 batch, training loss is 0.13390196859836578\n",
            "The 645 batch, training loss is 0.20029185712337494\n",
            "The 646 batch, training loss is 0.2300562709569931\n",
            "The 647 batch, training loss is 0.1176731064915657\n",
            "The 648 batch, training loss is 0.27443021535873413\n",
            "The 649 batch, training loss is 0.11269468069076538\n",
            "The 650 batch, training loss is 0.08235514909029007\n",
            "The 651 batch, training loss is 0.16840964555740356\n",
            "The 652 batch, training loss is 0.10824845731258392\n",
            "The 653 batch, training loss is 0.3835708796977997\n",
            "The 654 batch, training loss is 0.09041737020015717\n",
            "The 655 batch, training loss is 0.1937619149684906\n",
            "The 656 batch, training loss is 0.09569527953863144\n",
            "The 657 batch, training loss is 0.06353619694709778\n",
            "The 658 batch, training loss is 0.11718083918094635\n",
            "The 659 batch, training loss is 0.08784747123718262\n",
            "The 660 batch, training loss is 0.15931732952594757\n",
            "The 661 batch, training loss is 0.11557009071111679\n",
            "The 662 batch, training loss is 0.2119259089231491\n",
            "The 663 batch, training loss is 0.17089788615703583\n",
            "The 664 batch, training loss is 0.08201207965612411\n",
            "The 665 batch, training loss is 0.1844947338104248\n",
            "The 666 batch, training loss is 0.1620924323797226\n",
            "The 667 batch, training loss is 0.19162386655807495\n",
            "The 668 batch, training loss is 0.22955352067947388\n",
            "The 669 batch, training loss is 0.15630504488945007\n",
            "The 670 batch, training loss is 0.1315363645553589\n",
            "The 671 batch, training loss is 0.16967551410198212\n",
            "The 672 batch, training loss is 0.10491373389959335\n",
            "The 673 batch, training loss is 0.12342675030231476\n",
            "The 674 batch, training loss is 0.265813410282135\n",
            "The 675 batch, training loss is 0.11434095352888107\n",
            "The 676 batch, training loss is 0.12820656597614288\n",
            "The 677 batch, training loss is 0.11534252762794495\n",
            "The 678 batch, training loss is 0.08764548599720001\n",
            "The 679 batch, training loss is 0.20556166768074036\n",
            "The 680 batch, training loss is 0.14364492893218994\n",
            "The 681 batch, training loss is 0.2116057276725769\n",
            "The 682 batch, training loss is 0.17220894992351532\n",
            "The 683 batch, training loss is 0.04957441985607147\n",
            "The 684 batch, training loss is 0.1615976095199585\n",
            "The 685 batch, training loss is 0.1904153823852539\n",
            "The 686 batch, training loss is 0.11512347310781479\n",
            "The 687 batch, training loss is 0.4144296944141388\n",
            "The 688 batch, training loss is 0.29065245389938354\n",
            "The 689 batch, training loss is 0.20329000055789948\n",
            "The 690 batch, training loss is 0.23899437487125397\n",
            "The 691 batch, training loss is 0.17671170830726624\n",
            "The 692 batch, training loss is 0.1430734544992447\n",
            "The 693 batch, training loss is 0.26648133993148804\n",
            "The 694 batch, training loss is 0.26007577776908875\n",
            "The 695 batch, training loss is 0.1408313661813736\n",
            "The 696 batch, training loss is 0.06848061084747314\n",
            "The 697 batch, training loss is 0.19348691403865814\n",
            "The 698 batch, training loss is 0.08564139157533646\n",
            "The 699 batch, training loss is 0.08472491800785065\n",
            "The 700 batch, training loss is 0.2447614073753357\n",
            "The 701 batch, training loss is 0.08442660421133041\n",
            "The 702 batch, training loss is 0.17752179503440857\n",
            "The 703 batch, training loss is 0.10659008473157883\n",
            "The 704 batch, training loss is 0.17218907177448273\n",
            "The 705 batch, training loss is 0.09730587899684906\n",
            "The 706 batch, training loss is 0.11063902080059052\n",
            "The 707 batch, training loss is 0.03406425192952156\n",
            "The 708 batch, training loss is 0.10513745248317719\n",
            "The 709 batch, training loss is 0.12696553766727448\n",
            "The 710 batch, training loss is 0.06517336517572403\n",
            "The 711 batch, training loss is 0.11476197093725204\n",
            "The 712 batch, training loss is 0.4604911506175995\n",
            "The 713 batch, training loss is 0.10521913319826126\n",
            "The 714 batch, training loss is 0.09893467277288437\n",
            "The 715 batch, training loss is 0.15385650098323822\n",
            "The 716 batch, training loss is 0.1553281843662262\n",
            "The 717 batch, training loss is 0.18987515568733215\n",
            "The 718 batch, training loss is 0.13728781044483185\n",
            "The 719 batch, training loss is 0.16268080472946167\n",
            "The 720 batch, training loss is 0.28538236021995544\n",
            "The 721 batch, training loss is 0.16184002161026\n",
            "The 722 batch, training loss is 0.1467801332473755\n",
            "The 723 batch, training loss is 0.13605676591396332\n",
            "The 724 batch, training loss is 0.07861972600221634\n",
            "The 725 batch, training loss is 0.21946601569652557\n",
            "The 726 batch, training loss is 0.408687025308609\n",
            "The 727 batch, training loss is 0.11937034130096436\n",
            "The 728 batch, training loss is 0.23452505469322205\n",
            "The 729 batch, training loss is 0.12051058560609818\n",
            "The 730 batch, training loss is 0.25967931747436523\n",
            "The 731 batch, training loss is 0.11816508322954178\n",
            "The 732 batch, training loss is 0.06168506294488907\n",
            "The 733 batch, training loss is 0.14364157617092133\n",
            "The 734 batch, training loss is 0.26567405462265015\n",
            "The 735 batch, training loss is 0.09038480371236801\n",
            "The 736 batch, training loss is 0.0698247179389\n",
            "The 737 batch, training loss is 0.18990397453308105\n",
            "The 738 batch, training loss is 0.09657631069421768\n",
            "The 739 batch, training loss is 0.0872984454035759\n",
            "The 740 batch, training loss is 0.06025693193078041\n",
            "The 741 batch, training loss is 0.2776057720184326\n",
            "The 742 batch, training loss is 0.10610900074243546\n",
            "The 743 batch, training loss is 0.1737966686487198\n",
            "The 744 batch, training loss is 0.18470242619514465\n",
            "The 745 batch, training loss is 0.19246958196163177\n",
            "The 746 batch, training loss is 0.10872644931077957\n",
            "The 747 batch, training loss is 0.19809234142303467\n",
            "The 748 batch, training loss is 0.09229713678359985\n",
            "The 749 batch, training loss is 0.11600436270236969\n",
            "The 750 batch, training loss is 0.06261277198791504\n",
            "The 751 batch, training loss is 0.2712579667568207\n",
            "The 752 batch, training loss is 0.18223130702972412\n",
            "The 753 batch, training loss is 0.2642764449119568\n",
            "The 754 batch, training loss is 0.1875440627336502\n",
            "The 755 batch, training loss is 0.10717565566301346\n",
            "The 756 batch, training loss is 0.07241571694612503\n",
            "The 757 batch, training loss is 0.15875138342380524\n",
            "The 758 batch, training loss is 0.17682960629463196\n",
            "The 759 batch, training loss is 0.1545824110507965\n",
            "The 760 batch, training loss is 0.13391326367855072\n",
            "The 761 batch, training loss is 0.14846092462539673\n",
            "The 762 batch, training loss is 0.20487599074840546\n",
            "The 763 batch, training loss is 0.10002968460321426\n",
            "The 764 batch, training loss is 0.23248696327209473\n",
            "The 765 batch, training loss is 0.19476333260536194\n",
            "The 766 batch, training loss is 0.09730853885412216\n",
            "The 767 batch, training loss is 0.09433349967002869\n",
            "The 768 batch, training loss is 0.205704465508461\n",
            "The 769 batch, training loss is 0.12921416759490967\n",
            "The 770 batch, training loss is 0.14408928155899048\n",
            "The 771 batch, training loss is 0.08058731257915497\n",
            "The 772 batch, training loss is 0.1716907024383545\n",
            "The 773 batch, training loss is 0.19666175544261932\n",
            "The 774 batch, training loss is 0.10318338871002197\n",
            "The 775 batch, training loss is 0.16692732274532318\n",
            "The 776 batch, training loss is 0.08028553426265717\n",
            "The 777 batch, training loss is 0.15058894455432892\n",
            "The 778 batch, training loss is 0.14593403041362762\n",
            "The 779 batch, training loss is 0.09929849952459335\n",
            "The 780 batch, training loss is 0.33048567175865173\n",
            "The 781 batch, training loss is 0.1822335422039032\n",
            "The 782 batch, training loss is 0.14616245031356812\n",
            "The 783 batch, training loss is 0.11986615508794785\n",
            "The 784 batch, training loss is 0.21385550498962402\n",
            "The 785 batch, training loss is 0.09510537981987\n",
            "The 786 batch, training loss is 0.2150474190711975\n",
            "The 787 batch, training loss is 0.11201941221952438\n",
            "The 788 batch, training loss is 0.16379784047603607\n",
            "The 789 batch, training loss is 0.4001120924949646\n",
            "The 790 batch, training loss is 0.14397142827510834\n",
            "The 791 batch, training loss is 0.09534020721912384\n",
            "The 792 batch, training loss is 0.49004030227661133\n",
            "The 793 batch, training loss is 0.29618626832962036\n",
            "The 794 batch, training loss is 0.12728503346443176\n",
            "The 795 batch, training loss is 0.07805975526571274\n",
            "The 796 batch, training loss is 0.18747828900814056\n",
            "The 797 batch, training loss is 0.32245495915412903\n",
            "The 798 batch, training loss is 0.14857615530490875\n",
            "The 799 batch, training loss is 0.32238757610321045\n",
            "The 800 batch, training loss is 0.19896268844604492\n",
            "The 801 batch, training loss is 0.13727080821990967\n",
            "The 802 batch, training loss is 0.14075608551502228\n",
            "The 803 batch, training loss is 0.21667809784412384\n",
            "The 804 batch, training loss is 0.3617134988307953\n",
            "The 805 batch, training loss is 0.16695807874202728\n",
            "The 806 batch, training loss is 0.30805179476737976\n",
            "The 807 batch, training loss is 0.12971222400665283\n",
            "The 808 batch, training loss is 0.20845791697502136\n",
            "The 809 batch, training loss is 0.16005556285381317\n",
            "The 810 batch, training loss is 0.10655668377876282\n",
            "The 811 batch, training loss is 0.13571707904338837\n",
            "The 812 batch, training loss is 0.05773356184363365\n",
            "The 813 batch, training loss is 0.2065609246492386\n",
            "The 814 batch, training loss is 0.11424506455659866\n",
            "The 815 batch, training loss is 0.06383464485406876\n",
            "The 816 batch, training loss is 0.2497078776359558\n",
            "The 817 batch, training loss is 0.21525564789772034\n",
            "The 818 batch, training loss is 0.15734755992889404\n",
            "The 819 batch, training loss is 0.31117263436317444\n",
            "The 820 batch, training loss is 0.23393569886684418\n",
            "The 821 batch, training loss is 0.1986125409603119\n",
            "The 822 batch, training loss is 0.2910242974758148\n",
            "The 823 batch, training loss is 0.17467831075191498\n",
            "The 824 batch, training loss is 0.13646692037582397\n",
            "The 825 batch, training loss is 0.09906087815761566\n",
            "The 826 batch, training loss is 0.1510397344827652\n",
            "The 827 batch, training loss is 0.12971632182598114\n",
            "The 828 batch, training loss is 0.1039063036441803\n",
            "The 829 batch, training loss is 0.10192511230707169\n",
            "The 830 batch, training loss is 0.1463593989610672\n",
            "The 831 batch, training loss is 0.1030936911702156\n",
            "The 832 batch, training loss is 0.12181667983531952\n",
            "The 833 batch, training loss is 0.11378735303878784\n",
            "The 834 batch, training loss is 0.175348162651062\n",
            "The 835 batch, training loss is 0.10797539353370667\n",
            "The 836 batch, training loss is 0.30928462743759155\n",
            "The 837 batch, training loss is 0.08540014177560806\n",
            "The 838 batch, training loss is 0.06611747294664383\n",
            "The 839 batch, training loss is 0.17455752193927765\n",
            "The 840 batch, training loss is 0.2487342357635498\n",
            "The 841 batch, training loss is 0.2088398039340973\n",
            "The 842 batch, training loss is 0.23757940530776978\n",
            "The 843 batch, training loss is 0.32015329599380493\n",
            "The 844 batch, training loss is 0.11639150232076645\n",
            "The 845 batch, training loss is 0.3140377998352051\n",
            "The 846 batch, training loss is 0.2451154738664627\n",
            "The 847 batch, training loss is 0.14524756371974945\n",
            "The 848 batch, training loss is 0.06781969964504242\n",
            "The 849 batch, training loss is 0.15866391360759735\n",
            "The 850 batch, training loss is 0.5372287034988403\n",
            "The 851 batch, training loss is 0.12037885934114456\n",
            "The 852 batch, training loss is 0.17548023164272308\n",
            "The 853 batch, training loss is 0.11011699587106705\n",
            "The 854 batch, training loss is 0.15364059805870056\n",
            "The 855 batch, training loss is 0.09243111312389374\n",
            "The 856 batch, training loss is 0.08056223392486572\n",
            "The 857 batch, training loss is 0.1668764054775238\n",
            "The 858 batch, training loss is 0.18936264514923096\n",
            "The 859 batch, training loss is 0.3041217625141144\n",
            "The 860 batch, training loss is 0.17480827867984772\n",
            "The 861 batch, training loss is 0.13714249432086945\n",
            "The 862 batch, training loss is 0.1293099820613861\n",
            "The 863 batch, training loss is 0.1507519781589508\n",
            "The 864 batch, training loss is 0.13053582608699799\n",
            "The 865 batch, training loss is 0.1752656251192093\n",
            "The 866 batch, training loss is 0.24753597378730774\n",
            "The 867 batch, training loss is 0.3539092540740967\n",
            "The 868 batch, training loss is 0.16661891341209412\n",
            "The 869 batch, training loss is 0.16058605909347534\n",
            "The 870 batch, training loss is 0.1938539296388626\n",
            "The 871 batch, training loss is 0.10107243061065674\n",
            "The 872 batch, training loss is 0.09285416454076767\n",
            "The 873 batch, training loss is 0.1635042279958725\n",
            "The 874 batch, training loss is 0.13353022933006287\n",
            "The 875 batch, training loss is 0.17022748291492462\n",
            "The 876 batch, training loss is 0.240888774394989\n",
            "The 877 batch, training loss is 0.2313556671142578\n",
            "The 878 batch, training loss is 0.15553373098373413\n",
            "The 879 batch, training loss is 0.14482316374778748\n",
            "The 880 batch, training loss is 0.09700185805559158\n",
            "The 881 batch, training loss is 0.17024590075016022\n",
            "The 882 batch, training loss is 0.1226370632648468\n",
            "The 883 batch, training loss is 0.24747179448604584\n",
            "The 884 batch, training loss is 0.25698143243789673\n",
            "The 885 batch, training loss is 0.10767253488302231\n",
            "The 886 batch, training loss is 0.10927366465330124\n",
            "The 887 batch, training loss is 0.16662882268428802\n",
            "The 888 batch, training loss is 0.26250702142715454\n",
            "The 889 batch, training loss is 0.14232602715492249\n",
            "The 890 batch, training loss is 0.08695725351572037\n",
            "The 891 batch, training loss is 0.07494738698005676\n",
            "The 892 batch, training loss is 0.26498767733573914\n",
            "The 893 batch, training loss is 0.10681676864624023\n",
            "The 894 batch, training loss is 0.06282984465360641\n",
            "The 895 batch, training loss is 0.14489208161830902\n",
            "The 896 batch, training loss is 0.1111331507563591\n",
            "The 897 batch, training loss is 0.30451783537864685\n",
            "The 898 batch, training loss is 0.19437448680400848\n",
            "The 899 batch, training loss is 0.2303425371646881\n",
            "The 900 batch, training loss is 0.08944100886583328\n",
            "The 901 batch, training loss is 0.11355162411928177\n",
            "The 902 batch, training loss is 0.10774204879999161\n",
            "The 903 batch, training loss is 0.2773561179637909\n",
            "The 904 batch, training loss is 0.20538416504859924\n",
            "The 905 batch, training loss is 0.2265307605266571\n",
            "The 906 batch, training loss is 0.12430531531572342\n",
            "The 907 batch, training loss is 0.1455981433391571\n",
            "The 908 batch, training loss is 0.1574944406747818\n",
            "The 909 batch, training loss is 0.29939213395118713\n",
            "The 910 batch, training loss is 0.07545080780982971\n",
            "The 911 batch, training loss is 0.22477708756923676\n",
            "The 912 batch, training loss is 0.11922793835401535\n",
            "The 913 batch, training loss is 0.2944146394729614\n",
            "The 914 batch, training loss is 0.17762906849384308\n",
            "The 915 batch, training loss is 0.13644184172153473\n",
            "The 916 batch, training loss is 0.09075325727462769\n",
            "The 917 batch, training loss is 0.14018431305885315\n",
            "The 918 batch, training loss is 0.16695117950439453\n",
            "The 919 batch, training loss is 0.11964521557092667\n",
            "The 920 batch, training loss is 0.19150319695472717\n",
            "The 921 batch, training loss is 0.14432859420776367\n",
            "The 922 batch, training loss is 0.15626747906208038\n",
            "The 923 batch, training loss is 0.1772928386926651\n",
            "The 924 batch, training loss is 0.0916471779346466\n",
            "The 925 batch, training loss is 0.16336646676063538\n",
            "The 926 batch, training loss is 0.14348214864730835\n",
            "The 927 batch, training loss is 0.15307201445102692\n",
            "The 928 batch, training loss is 0.2229216992855072\n",
            "The 929 batch, training loss is 0.18811143934726715\n",
            "The 930 batch, training loss is 0.2011663317680359\n",
            "The 931 batch, training loss is 0.11934801936149597\n",
            "The 932 batch, training loss is 0.04710144177079201\n",
            "The 933 batch, training loss is 0.1290672868490219\n",
            "The 934 batch, training loss is 0.16663436591625214\n",
            "The 935 batch, training loss is 0.15424345433712006\n",
            "The 936 batch, training loss is 0.23712804913520813\n",
            "The 937 batch, training loss is 0.10838963091373444\n",
            "The 6 epoch, training loss is 0.10838963091373444\n",
            "The 0 batch, training loss is 0.25373393297195435\n",
            "The 1 batch, training loss is 0.12021592259407043\n",
            "The 2 batch, training loss is 0.13249531388282776\n",
            "The 3 batch, training loss is 0.09952294826507568\n",
            "The 4 batch, training loss is 0.20067201554775238\n",
            "The 5 batch, training loss is 0.12096869945526123\n",
            "The 6 batch, training loss is 0.16473954916000366\n",
            "The 7 batch, training loss is 0.17933624982833862\n",
            "The 8 batch, training loss is 0.08054675161838531\n",
            "The 9 batch, training loss is 0.15844644606113434\n",
            "The 10 batch, training loss is 0.06482639163732529\n",
            "The 11 batch, training loss is 0.13446232676506042\n",
            "The 12 batch, training loss is 0.12595543265342712\n",
            "The 13 batch, training loss is 0.27298709750175476\n",
            "The 14 batch, training loss is 0.17624446749687195\n",
            "The 15 batch, training loss is 0.5189911127090454\n",
            "The 16 batch, training loss is 0.24541808664798737\n",
            "The 17 batch, training loss is 0.184800386428833\n",
            "The 18 batch, training loss is 0.17533379793167114\n",
            "The 19 batch, training loss is 0.14540156722068787\n",
            "The 20 batch, training loss is 0.11585067212581635\n",
            "The 21 batch, training loss is 0.24398261308670044\n",
            "The 22 batch, training loss is 0.15023334324359894\n",
            "The 23 batch, training loss is 0.28138938546180725\n",
            "The 24 batch, training loss is 0.2516365945339203\n",
            "The 25 batch, training loss is 0.4017294943332672\n",
            "The 26 batch, training loss is 0.3930158019065857\n",
            "The 27 batch, training loss is 0.21419596672058105\n",
            "The 28 batch, training loss is 0.17161597311496735\n",
            "The 29 batch, training loss is 0.1328234076499939\n",
            "The 30 batch, training loss is 0.07857305556535721\n",
            "The 31 batch, training loss is 0.1003243699669838\n",
            "The 32 batch, training loss is 0.07320941984653473\n",
            "The 33 batch, training loss is 0.09599591046571732\n",
            "The 34 batch, training loss is 0.13185741007328033\n",
            "The 35 batch, training loss is 0.08073780685663223\n",
            "The 36 batch, training loss is 0.07452128827571869\n",
            "The 37 batch, training loss is 0.18487872183322906\n",
            "The 38 batch, training loss is 0.12246698141098022\n",
            "The 39 batch, training loss is 0.06593897938728333\n",
            "The 40 batch, training loss is 0.1456902027130127\n",
            "The 41 batch, training loss is 0.12287374585866928\n",
            "The 42 batch, training loss is 0.12939175963401794\n",
            "The 43 batch, training loss is 0.056196168065071106\n",
            "The 44 batch, training loss is 0.20440854132175446\n",
            "The 45 batch, training loss is 0.24328552186489105\n",
            "The 46 batch, training loss is 0.10392303764820099\n",
            "The 47 batch, training loss is 0.030072735622525215\n",
            "The 48 batch, training loss is 0.14832371473312378\n",
            "The 49 batch, training loss is 0.04043222963809967\n",
            "The 50 batch, training loss is 0.1340114325284958\n",
            "The 51 batch, training loss is 0.22346854209899902\n",
            "The 52 batch, training loss is 0.14380554854869843\n",
            "The 53 batch, training loss is 0.0911848247051239\n",
            "The 54 batch, training loss is 0.22980019450187683\n",
            "The 55 batch, training loss is 0.20868520438671112\n",
            "The 56 batch, training loss is 0.09395091235637665\n",
            "The 57 batch, training loss is 0.22910381853580475\n",
            "The 58 batch, training loss is 0.07556135952472687\n",
            "The 59 batch, training loss is 0.15890373289585114\n",
            "The 60 batch, training loss is 0.08662360906600952\n",
            "The 61 batch, training loss is 0.19137410819530487\n",
            "The 62 batch, training loss is 0.16969649493694305\n",
            "The 63 batch, training loss is 0.08026085048913956\n",
            "The 64 batch, training loss is 0.12322372943162918\n",
            "The 65 batch, training loss is 0.1535206288099289\n",
            "The 66 batch, training loss is 0.08171030879020691\n",
            "The 67 batch, training loss is 0.18191702663898468\n",
            "The 68 batch, training loss is 0.19239521026611328\n",
            "The 69 batch, training loss is 0.2110866755247116\n",
            "The 70 batch, training loss is 0.16692447662353516\n",
            "The 71 batch, training loss is 0.20973509550094604\n",
            "The 72 batch, training loss is 0.29754090309143066\n",
            "The 73 batch, training loss is 0.09981132298707962\n",
            "The 74 batch, training loss is 0.07392319291830063\n",
            "The 75 batch, training loss is 0.24451325833797455\n",
            "The 76 batch, training loss is 0.20464368164539337\n",
            "The 77 batch, training loss is 0.07751701772212982\n",
            "The 78 batch, training loss is 0.10424397140741348\n",
            "The 79 batch, training loss is 0.07686498016119003\n",
            "The 80 batch, training loss is 0.10058541595935822\n",
            "The 81 batch, training loss is 0.06885489821434021\n",
            "The 82 batch, training loss is 0.06557640433311462\n",
            "The 83 batch, training loss is 0.16978532075881958\n",
            "The 84 batch, training loss is 0.16998699307441711\n",
            "The 85 batch, training loss is 0.08515480905771255\n",
            "The 86 batch, training loss is 0.10111358016729355\n",
            "The 87 batch, training loss is 0.13106749951839447\n",
            "The 88 batch, training loss is 0.07581707835197449\n",
            "The 89 batch, training loss is 0.18658463656902313\n",
            "The 90 batch, training loss is 0.12642920017242432\n",
            "The 91 batch, training loss is 0.0973896011710167\n",
            "The 92 batch, training loss is 0.13423579931259155\n",
            "The 93 batch, training loss is 0.3032708466053009\n",
            "The 94 batch, training loss is 0.29662832617759705\n",
            "The 95 batch, training loss is 0.06599025428295135\n",
            "The 96 batch, training loss is 0.10682473331689835\n",
            "The 97 batch, training loss is 0.07945884764194489\n",
            "The 98 batch, training loss is 0.06457492709159851\n",
            "The 99 batch, training loss is 0.21863345801830292\n",
            "The 100 batch, training loss is 0.10378225892782211\n",
            "The 101 batch, training loss is 0.1237044408917427\n",
            "The 102 batch, training loss is 0.1007014587521553\n",
            "The 103 batch, training loss is 0.09591978788375854\n",
            "The 104 batch, training loss is 0.08382441103458405\n",
            "The 105 batch, training loss is 0.17069418728351593\n",
            "The 106 batch, training loss is 0.1079215407371521\n",
            "The 107 batch, training loss is 0.21906493604183197\n",
            "The 108 batch, training loss is 0.14279235899448395\n",
            "The 109 batch, training loss is 0.2365708202123642\n",
            "The 110 batch, training loss is 0.04275745898485184\n",
            "The 111 batch, training loss is 0.12826748192310333\n",
            "The 112 batch, training loss is 0.11878370493650436\n",
            "The 113 batch, training loss is 0.21914072334766388\n",
            "The 114 batch, training loss is 0.1890680193901062\n",
            "The 115 batch, training loss is 0.10391764342784882\n",
            "The 116 batch, training loss is 0.16474765539169312\n",
            "The 117 batch, training loss is 0.21228426694869995\n",
            "The 118 batch, training loss is 0.26069098711013794\n",
            "The 119 batch, training loss is 0.20748917758464813\n",
            "The 120 batch, training loss is 0.22288855910301208\n",
            "The 121 batch, training loss is 0.10976395010948181\n",
            "The 122 batch, training loss is 0.08502835780382156\n",
            "The 123 batch, training loss is 0.10937214642763138\n",
            "The 124 batch, training loss is 0.07439916580915451\n",
            "The 125 batch, training loss is 0.11715531349182129\n",
            "The 126 batch, training loss is 0.15943734347820282\n",
            "The 127 batch, training loss is 0.2745511829853058\n",
            "The 128 batch, training loss is 0.25722721219062805\n",
            "The 129 batch, training loss is 0.10611319541931152\n",
            "The 130 batch, training loss is 0.1667255014181137\n",
            "The 131 batch, training loss is 0.17740151286125183\n",
            "The 132 batch, training loss is 0.18131577968597412\n",
            "The 133 batch, training loss is 0.17151039838790894\n",
            "The 134 batch, training loss is 0.15422292053699493\n",
            "The 135 batch, training loss is 0.21329209208488464\n",
            "The 136 batch, training loss is 0.23562052845954895\n",
            "The 137 batch, training loss is 0.14107801020145416\n",
            "The 138 batch, training loss is 0.20399893820285797\n",
            "The 139 batch, training loss is 0.06695787608623505\n",
            "The 140 batch, training loss is 0.08662357181310654\n",
            "The 141 batch, training loss is 0.21180419623851776\n",
            "The 142 batch, training loss is 0.12456383556127548\n",
            "The 143 batch, training loss is 0.19621115922927856\n",
            "The 144 batch, training loss is 0.16392694413661957\n",
            "The 145 batch, training loss is 0.09657730162143707\n",
            "The 146 batch, training loss is 0.2791450619697571\n",
            "The 147 batch, training loss is 0.111335888504982\n",
            "The 148 batch, training loss is 0.17687879502773285\n",
            "The 149 batch, training loss is 0.14968328177928925\n",
            "The 150 batch, training loss is 0.0727607011795044\n",
            "The 151 batch, training loss is 0.2504042088985443\n",
            "The 152 batch, training loss is 0.1279979944229126\n",
            "The 153 batch, training loss is 0.15741626918315887\n",
            "The 154 batch, training loss is 0.18554916977882385\n",
            "The 155 batch, training loss is 0.0510081946849823\n",
            "The 156 batch, training loss is 0.11245786398649216\n",
            "The 157 batch, training loss is 0.18684715032577515\n",
            "The 158 batch, training loss is 0.16259388625621796\n",
            "The 159 batch, training loss is 0.07031146436929703\n",
            "The 160 batch, training loss is 0.18346266448497772\n",
            "The 161 batch, training loss is 0.23938505351543427\n",
            "The 162 batch, training loss is 0.11925004422664642\n",
            "The 163 batch, training loss is 0.1521914005279541\n",
            "The 164 batch, training loss is 0.26475298404693604\n",
            "The 165 batch, training loss is 0.0922580286860466\n",
            "The 166 batch, training loss is 0.19673731923103333\n",
            "The 167 batch, training loss is 0.12568266689777374\n",
            "The 168 batch, training loss is 0.0839882344007492\n",
            "The 169 batch, training loss is 0.08361490815877914\n",
            "The 170 batch, training loss is 0.2343357652425766\n",
            "The 171 batch, training loss is 0.2395532727241516\n",
            "The 172 batch, training loss is 0.10567718744277954\n",
            "The 173 batch, training loss is 0.15609216690063477\n",
            "The 174 batch, training loss is 0.13489209115505219\n",
            "The 175 batch, training loss is 0.27880269289016724\n",
            "The 176 batch, training loss is 0.0608070008456707\n",
            "The 177 batch, training loss is 0.35860270261764526\n",
            "The 178 batch, training loss is 0.26388072967529297\n",
            "The 179 batch, training loss is 0.1504569947719574\n",
            "The 180 batch, training loss is 0.09905029088258743\n",
            "The 181 batch, training loss is 0.11997324973344803\n",
            "The 182 batch, training loss is 0.0799868106842041\n",
            "The 183 batch, training loss is 0.10358499735593796\n",
            "The 184 batch, training loss is 0.2457624226808548\n",
            "The 185 batch, training loss is 0.13059496879577637\n",
            "The 186 batch, training loss is 0.10530054569244385\n",
            "The 187 batch, training loss is 0.09904829412698746\n",
            "The 188 batch, training loss is 0.06247628107666969\n",
            "The 189 batch, training loss is 0.26674073934555054\n",
            "The 190 batch, training loss is 0.13593389093875885\n",
            "The 191 batch, training loss is 0.09892529249191284\n",
            "The 192 batch, training loss is 0.1323152333498001\n",
            "The 193 batch, training loss is 0.17774894833564758\n",
            "The 194 batch, training loss is 0.18057064712047577\n",
            "The 195 batch, training loss is 0.15683870017528534\n",
            "The 196 batch, training loss is 0.0877600759267807\n",
            "The 197 batch, training loss is 0.2686699628829956\n",
            "The 198 batch, training loss is 0.17332212626934052\n",
            "The 199 batch, training loss is 0.12464739382266998\n",
            "The 200 batch, training loss is 0.35852912068367004\n",
            "The 201 batch, training loss is 0.1801256239414215\n",
            "The 202 batch, training loss is 0.3042442202568054\n",
            "The 203 batch, training loss is 0.17652645707130432\n",
            "The 204 batch, training loss is 0.11689906567335129\n",
            "The 205 batch, training loss is 0.14004619419574738\n",
            "The 206 batch, training loss is 0.06362313777208328\n",
            "The 207 batch, training loss is 0.14927981793880463\n",
            "The 208 batch, training loss is 0.1287536770105362\n",
            "The 209 batch, training loss is 0.0756952092051506\n",
            "The 210 batch, training loss is 0.2360374629497528\n",
            "The 211 batch, training loss is 0.2070130705833435\n",
            "The 212 batch, training loss is 0.14220228791236877\n",
            "The 213 batch, training loss is 0.19283683598041534\n",
            "The 214 batch, training loss is 0.21028512716293335\n",
            "The 215 batch, training loss is 0.10787700861692429\n",
            "The 216 batch, training loss is 0.47693657875061035\n",
            "The 217 batch, training loss is 0.05164601653814316\n",
            "The 218 batch, training loss is 0.26416662335395813\n",
            "The 219 batch, training loss is 0.08951809257268906\n",
            "The 220 batch, training loss is 0.3199658989906311\n",
            "The 221 batch, training loss is 0.19241321086883545\n",
            "The 222 batch, training loss is 0.0919402465224266\n",
            "The 223 batch, training loss is 0.1639629453420639\n",
            "The 224 batch, training loss is 0.1371481865644455\n",
            "The 225 batch, training loss is 0.07100659608840942\n",
            "The 226 batch, training loss is 0.1510060876607895\n",
            "The 227 batch, training loss is 0.06510429084300995\n",
            "The 228 batch, training loss is 0.17420245707035065\n",
            "The 229 batch, training loss is 0.21909217536449432\n",
            "The 230 batch, training loss is 0.11693593859672546\n",
            "The 231 batch, training loss is 0.14146427810192108\n",
            "The 232 batch, training loss is 0.11217033863067627\n",
            "The 233 batch, training loss is 0.16577604413032532\n",
            "The 234 batch, training loss is 0.07080304622650146\n",
            "The 235 batch, training loss is 0.07258805632591248\n",
            "The 236 batch, training loss is 0.1832524836063385\n",
            "The 237 batch, training loss is 0.17900945246219635\n",
            "The 238 batch, training loss is 0.256213515996933\n",
            "The 239 batch, training loss is 0.29565131664276123\n",
            "The 240 batch, training loss is 0.09122715890407562\n",
            "The 241 batch, training loss is 0.44787392020225525\n",
            "The 242 batch, training loss is 0.1865815967321396\n",
            "The 243 batch, training loss is 0.06232834607362747\n",
            "The 244 batch, training loss is 0.2256164848804474\n",
            "The 245 batch, training loss is 0.1223539486527443\n",
            "The 246 batch, training loss is 0.11548449099063873\n",
            "The 247 batch, training loss is 0.21090394258499146\n",
            "The 248 batch, training loss is 0.08914517611265182\n",
            "The 249 batch, training loss is 0.2515549957752228\n",
            "The 250 batch, training loss is 0.36340588331222534\n",
            "The 251 batch, training loss is 0.12736310064792633\n",
            "The 252 batch, training loss is 0.13827528059482574\n",
            "The 253 batch, training loss is 0.10456652194261551\n",
            "The 254 batch, training loss is 0.19126872718334198\n",
            "The 255 batch, training loss is 0.11594889312982559\n",
            "The 256 batch, training loss is 0.15533272922039032\n",
            "The 257 batch, training loss is 0.14121633768081665\n",
            "The 258 batch, training loss is 0.29224708676338196\n",
            "The 259 batch, training loss is 0.14511200785636902\n",
            "The 260 batch, training loss is 0.1154424324631691\n",
            "The 261 batch, training loss is 0.22431084513664246\n",
            "The 262 batch, training loss is 0.09349480271339417\n",
            "The 263 batch, training loss is 0.23597224056720734\n",
            "The 264 batch, training loss is 0.2577195465564728\n",
            "The 265 batch, training loss is 0.13284645974636078\n",
            "The 266 batch, training loss is 0.21086075901985168\n",
            "The 267 batch, training loss is 0.12163340300321579\n",
            "The 268 batch, training loss is 0.09336952865123749\n",
            "The 269 batch, training loss is 0.1883268654346466\n",
            "The 270 batch, training loss is 0.116355299949646\n",
            "The 271 batch, training loss is 0.18957585096359253\n",
            "The 272 batch, training loss is 0.10118906199932098\n",
            "The 273 batch, training loss is 0.14524786174297333\n",
            "The 274 batch, training loss is 0.17995202541351318\n",
            "The 275 batch, training loss is 0.14846429228782654\n",
            "The 276 batch, training loss is 0.14401157200336456\n",
            "The 277 batch, training loss is 0.10382037609815598\n",
            "The 278 batch, training loss is 0.10375489294528961\n",
            "The 279 batch, training loss is 0.10589108616113663\n",
            "The 280 batch, training loss is 0.2120799422264099\n",
            "The 281 batch, training loss is 0.0822148323059082\n",
            "The 282 batch, training loss is 0.2444721907377243\n",
            "The 283 batch, training loss is 0.07440638542175293\n",
            "The 284 batch, training loss is 0.10396713018417358\n",
            "The 285 batch, training loss is 0.2307475358247757\n",
            "The 286 batch, training loss is 0.30379194021224976\n",
            "The 287 batch, training loss is 0.1714588701725006\n",
            "The 288 batch, training loss is 0.0948680117726326\n",
            "The 289 batch, training loss is 0.14083397388458252\n",
            "The 290 batch, training loss is 0.1090347096323967\n",
            "The 291 batch, training loss is 0.19264720380306244\n",
            "The 292 batch, training loss is 0.17265217006206512\n",
            "The 293 batch, training loss is 0.07011272013187408\n",
            "The 294 batch, training loss is 0.18221396207809448\n",
            "The 295 batch, training loss is 0.15826204419136047\n",
            "The 296 batch, training loss is 0.2460586577653885\n",
            "The 297 batch, training loss is 0.10092219710350037\n",
            "The 298 batch, training loss is 0.24036745727062225\n",
            "The 299 batch, training loss is 0.10534574836492538\n",
            "The 300 batch, training loss is 0.1954163908958435\n",
            "The 301 batch, training loss is 0.0505366250872612\n",
            "The 302 batch, training loss is 0.07464278489351273\n",
            "The 303 batch, training loss is 0.12020298838615417\n",
            "The 304 batch, training loss is 0.0912095233798027\n",
            "The 305 batch, training loss is 0.27879568934440613\n",
            "The 306 batch, training loss is 0.08163908869028091\n",
            "The 307 batch, training loss is 0.1551581770181656\n",
            "The 308 batch, training loss is 0.10375192761421204\n",
            "The 309 batch, training loss is 0.07813030481338501\n",
            "The 310 batch, training loss is 0.1375780701637268\n",
            "The 311 batch, training loss is 0.10983142256736755\n",
            "The 312 batch, training loss is 0.14915041625499725\n",
            "The 313 batch, training loss is 0.09713644534349442\n",
            "The 314 batch, training loss is 0.32636988162994385\n",
            "The 315 batch, training loss is 0.08112277835607529\n",
            "The 316 batch, training loss is 0.05315779894590378\n",
            "The 317 batch, training loss is 0.09446930140256882\n",
            "The 318 batch, training loss is 0.14181919395923615\n",
            "The 319 batch, training loss is 0.10683939605951309\n",
            "The 320 batch, training loss is 0.15187972784042358\n",
            "The 321 batch, training loss is 0.1291896402835846\n",
            "The 322 batch, training loss is 0.20808294415473938\n",
            "The 323 batch, training loss is 0.0895124301314354\n",
            "The 324 batch, training loss is 0.18588460981845856\n",
            "The 325 batch, training loss is 0.0655609741806984\n",
            "The 326 batch, training loss is 0.14417679607868195\n",
            "The 327 batch, training loss is 0.036958929151296616\n",
            "The 328 batch, training loss is 0.17370553314685822\n",
            "The 329 batch, training loss is 0.048802699893713\n",
            "The 330 batch, training loss is 0.1559169441461563\n",
            "The 331 batch, training loss is 0.19847433269023895\n",
            "The 332 batch, training loss is 0.20143382251262665\n",
            "The 333 batch, training loss is 0.13748784363269806\n",
            "The 334 batch, training loss is 0.05237100273370743\n",
            "The 335 batch, training loss is 0.13427157700061798\n",
            "The 336 batch, training loss is 0.16968420147895813\n",
            "The 337 batch, training loss is 0.16225126385688782\n",
            "The 338 batch, training loss is 0.17875680327415466\n",
            "The 339 batch, training loss is 0.08187577873468399\n",
            "The 340 batch, training loss is 0.08349411934614182\n",
            "The 341 batch, training loss is 0.1346239596605301\n",
            "The 342 batch, training loss is 0.08205050230026245\n",
            "The 343 batch, training loss is 0.10233953595161438\n",
            "The 344 batch, training loss is 0.10254431515932083\n",
            "The 345 batch, training loss is 0.11952529102563858\n",
            "The 346 batch, training loss is 0.14689190685749054\n",
            "The 347 batch, training loss is 0.09119623154401779\n",
            "The 348 batch, training loss is 0.05103609710931778\n",
            "The 349 batch, training loss is 0.1261296421289444\n",
            "The 350 batch, training loss is 0.1497582495212555\n",
            "The 351 batch, training loss is 0.2954637110233307\n",
            "The 352 batch, training loss is 0.06080583482980728\n",
            "The 353 batch, training loss is 0.18302536010742188\n",
            "The 354 batch, training loss is 0.1050722673535347\n",
            "The 355 batch, training loss is 0.08133860677480698\n",
            "The 356 batch, training loss is 0.09429360926151276\n",
            "The 357 batch, training loss is 0.15457354485988617\n",
            "The 358 batch, training loss is 0.1626320779323578\n",
            "The 359 batch, training loss is 0.1835576593875885\n",
            "The 360 batch, training loss is 0.07466328144073486\n",
            "The 361 batch, training loss is 0.259229838848114\n",
            "The 362 batch, training loss is 0.1270548403263092\n",
            "The 363 batch, training loss is 0.13952480256557465\n",
            "The 364 batch, training loss is 0.17554965615272522\n",
            "The 365 batch, training loss is 0.16900502145290375\n",
            "The 366 batch, training loss is 0.16292406618595123\n",
            "The 367 batch, training loss is 0.09530862420797348\n",
            "The 368 batch, training loss is 0.17870087921619415\n",
            "The 369 batch, training loss is 0.06268303096294403\n",
            "The 370 batch, training loss is 0.09596282243728638\n",
            "The 371 batch, training loss is 0.15784020721912384\n",
            "The 372 batch, training loss is 0.1429593712091446\n",
            "The 373 batch, training loss is 0.07674624025821686\n",
            "The 374 batch, training loss is 0.09973224252462387\n",
            "The 375 batch, training loss is 0.3658541142940521\n",
            "The 376 batch, training loss is 0.05328260362148285\n",
            "The 377 batch, training loss is 0.2234136313199997\n",
            "The 378 batch, training loss is 0.13132275640964508\n",
            "The 379 batch, training loss is 0.07586834579706192\n",
            "The 380 batch, training loss is 0.07655143737792969\n",
            "The 381 batch, training loss is 0.17479829490184784\n",
            "The 382 batch, training loss is 0.09648620337247849\n",
            "The 383 batch, training loss is 0.13827505707740784\n",
            "The 384 batch, training loss is 0.23125812411308289\n",
            "The 385 batch, training loss is 0.11293153464794159\n",
            "The 386 batch, training loss is 0.18018485605716705\n",
            "The 387 batch, training loss is 0.09725876152515411\n",
            "The 388 batch, training loss is 0.11256299912929535\n",
            "The 389 batch, training loss is 0.08815161138772964\n",
            "The 390 batch, training loss is 0.24859727919101715\n",
            "The 391 batch, training loss is 0.03677215799689293\n",
            "The 392 batch, training loss is 0.17175042629241943\n",
            "The 393 batch, training loss is 0.1200391873717308\n",
            "The 394 batch, training loss is 0.27530404925346375\n",
            "The 395 batch, training loss is 0.1719880998134613\n",
            "The 396 batch, training loss is 0.2844116985797882\n",
            "The 397 batch, training loss is 0.08043225109577179\n",
            "The 398 batch, training loss is 0.15779376029968262\n",
            "The 399 batch, training loss is 0.08066840469837189\n",
            "The 400 batch, training loss is 0.20312869548797607\n",
            "The 401 batch, training loss is 0.10459260642528534\n",
            "The 402 batch, training loss is 0.048432424664497375\n",
            "The 403 batch, training loss is 0.1403525322675705\n",
            "The 404 batch, training loss is 0.07158368080854416\n",
            "The 405 batch, training loss is 0.12361309677362442\n",
            "The 406 batch, training loss is 0.10899197310209274\n",
            "The 407 batch, training loss is 0.08278562873601913\n",
            "The 408 batch, training loss is 0.24905608594417572\n",
            "The 409 batch, training loss is 0.16084976494312286\n",
            "The 410 batch, training loss is 0.12900857627391815\n",
            "The 411 batch, training loss is 0.12751157581806183\n",
            "The 412 batch, training loss is 0.12443767488002777\n",
            "The 413 batch, training loss is 0.08512129634618759\n",
            "The 414 batch, training loss is 0.21445123851299286\n",
            "The 415 batch, training loss is 0.11161006987094879\n",
            "The 416 batch, training loss is 0.10328949242830276\n",
            "The 417 batch, training loss is 0.10138542950153351\n",
            "The 418 batch, training loss is 0.15186986327171326\n",
            "The 419 batch, training loss is 0.12527811527252197\n",
            "The 420 batch, training loss is 0.12646906077861786\n",
            "The 421 batch, training loss is 0.09529533237218857\n",
            "The 422 batch, training loss is 0.21459834277629852\n",
            "The 423 batch, training loss is 0.2101738154888153\n",
            "The 424 batch, training loss is 0.11540471762418747\n",
            "The 425 batch, training loss is 0.1254538595676422\n",
            "The 426 batch, training loss is 0.13210497796535492\n",
            "The 427 batch, training loss is 0.10181768983602524\n",
            "The 428 batch, training loss is 0.09235066175460815\n",
            "The 429 batch, training loss is 0.1113010123372078\n",
            "The 430 batch, training loss is 0.08555231988430023\n",
            "The 431 batch, training loss is 0.2235967218875885\n",
            "The 432 batch, training loss is 0.06132068485021591\n",
            "The 433 batch, training loss is 0.03832732141017914\n",
            "The 434 batch, training loss is 0.12883080542087555\n",
            "The 435 batch, training loss is 0.18577633798122406\n",
            "The 436 batch, training loss is 0.053809311240911484\n",
            "The 437 batch, training loss is 0.15324166417121887\n",
            "The 438 batch, training loss is 0.13854964077472687\n",
            "The 439 batch, training loss is 0.08454056084156036\n",
            "The 440 batch, training loss is 0.1679387241601944\n",
            "The 441 batch, training loss is 0.1704028695821762\n",
            "The 442 batch, training loss is 0.25150349736213684\n",
            "The 443 batch, training loss is 0.265296071767807\n",
            "The 444 batch, training loss is 0.1519865244626999\n",
            "The 445 batch, training loss is 0.2658936679363251\n",
            "The 446 batch, training loss is 0.18395505845546722\n",
            "The 447 batch, training loss is 0.23615846037864685\n",
            "The 448 batch, training loss is 0.06950236856937408\n",
            "The 449 batch, training loss is 0.149152472615242\n",
            "The 450 batch, training loss is 0.1458466798067093\n",
            "The 451 batch, training loss is 0.18307748436927795\n",
            "The 452 batch, training loss is 0.2139734923839569\n",
            "The 453 batch, training loss is 0.09231192618608475\n",
            "The 454 batch, training loss is 0.053120825439691544\n",
            "The 455 batch, training loss is 0.24105796217918396\n",
            "The 456 batch, training loss is 0.08025971055030823\n",
            "The 457 batch, training loss is 0.16537633538246155\n",
            "The 458 batch, training loss is 0.13198304176330566\n",
            "The 459 batch, training loss is 0.2346971482038498\n",
            "The 460 batch, training loss is 0.08115676790475845\n",
            "The 461 batch, training loss is 0.2578181326389313\n",
            "The 462 batch, training loss is 0.07215656340122223\n",
            "The 463 batch, training loss is 0.08570065349340439\n",
            "The 464 batch, training loss is 0.21959860622882843\n",
            "The 465 batch, training loss is 0.28841283917427063\n",
            "The 466 batch, training loss is 0.12639454007148743\n",
            "The 467 batch, training loss is 0.1032487228512764\n",
            "The 468 batch, training loss is 0.133413165807724\n",
            "The 469 batch, training loss is 0.10854625701904297\n",
            "The 470 batch, training loss is 0.07762585580348969\n",
            "The 471 batch, training loss is 0.1416195034980774\n",
            "The 472 batch, training loss is 0.2805056869983673\n",
            "The 473 batch, training loss is 0.07079417258501053\n",
            "The 474 batch, training loss is 0.05570002272725105\n",
            "The 475 batch, training loss is 0.10038801282644272\n",
            "The 476 batch, training loss is 0.14461708068847656\n",
            "The 477 batch, training loss is 0.09386488050222397\n",
            "The 478 batch, training loss is 0.09564664214849472\n",
            "The 479 batch, training loss is 0.1433321237564087\n",
            "The 480 batch, training loss is 0.11809565126895905\n",
            "The 481 batch, training loss is 0.20489628612995148\n",
            "The 482 batch, training loss is 0.0445437915623188\n",
            "The 483 batch, training loss is 0.39032790064811707\n",
            "The 484 batch, training loss is 0.22038500010967255\n",
            "The 485 batch, training loss is 0.16025404632091522\n",
            "The 486 batch, training loss is 0.06711406260728836\n",
            "The 487 batch, training loss is 0.10765545815229416\n",
            "The 488 batch, training loss is 0.06965019553899765\n",
            "The 489 batch, training loss is 0.08996696770191193\n",
            "The 490 batch, training loss is 0.2470400035381317\n",
            "The 491 batch, training loss is 0.13032996654510498\n",
            "The 492 batch, training loss is 0.11407952010631561\n",
            "The 493 batch, training loss is 0.12806914746761322\n",
            "The 494 batch, training loss is 0.12439610064029694\n",
            "The 495 batch, training loss is 0.1442176103591919\n",
            "The 496 batch, training loss is 0.13055187463760376\n",
            "The 497 batch, training loss is 0.07041166722774506\n",
            "The 498 batch, training loss is 0.2697683572769165\n",
            "The 499 batch, training loss is 0.23711875081062317\n",
            "The 500 batch, training loss is 0.08419881761074066\n",
            "The 501 batch, training loss is 0.11638866364955902\n",
            "The 502 batch, training loss is 0.13239169120788574\n",
            "The 503 batch, training loss is 0.18473264575004578\n",
            "The 504 batch, training loss is 0.23949362337589264\n",
            "The 505 batch, training loss is 0.20280437171459198\n",
            "The 506 batch, training loss is 0.11765021085739136\n",
            "The 507 batch, training loss is 0.0828138217329979\n",
            "The 508 batch, training loss is 0.2556822597980499\n",
            "The 509 batch, training loss is 0.27716970443725586\n",
            "The 510 batch, training loss is 0.0659278929233551\n",
            "The 511 batch, training loss is 0.09112653136253357\n",
            "The 512 batch, training loss is 0.13176938891410828\n",
            "The 513 batch, training loss is 0.12186773866415024\n",
            "The 514 batch, training loss is 0.22758637368679047\n",
            "The 515 batch, training loss is 0.16109660267829895\n",
            "The 516 batch, training loss is 0.08644497394561768\n",
            "The 517 batch, training loss is 0.20915117859840393\n",
            "The 518 batch, training loss is 0.2516903877258301\n",
            "The 519 batch, training loss is 0.10293541848659515\n",
            "The 520 batch, training loss is 0.05120282992720604\n",
            "The 521 batch, training loss is 0.05202766880393028\n",
            "The 522 batch, training loss is 0.28243348002433777\n",
            "The 523 batch, training loss is 0.16766490042209625\n",
            "The 524 batch, training loss is 0.11702629178762436\n",
            "The 525 batch, training loss is 0.16425925493240356\n",
            "The 526 batch, training loss is 0.042749617248773575\n",
            "The 527 batch, training loss is 0.1585521548986435\n",
            "The 528 batch, training loss is 0.02496674284338951\n",
            "The 529 batch, training loss is 0.14748510718345642\n",
            "The 530 batch, training loss is 0.17754776775836945\n",
            "The 531 batch, training loss is 0.2578519284725189\n",
            "The 532 batch, training loss is 0.1362239122390747\n",
            "The 533 batch, training loss is 0.11691983044147491\n",
            "The 534 batch, training loss is 0.15450957417488098\n",
            "The 535 batch, training loss is 0.284865140914917\n",
            "The 536 batch, training loss is 0.17460958659648895\n",
            "The 537 batch, training loss is 0.225469172000885\n",
            "The 538 batch, training loss is 0.07239648699760437\n",
            "The 539 batch, training loss is 0.11572474241256714\n",
            "The 540 batch, training loss is 0.12180991470813751\n",
            "The 541 batch, training loss is 0.06788652390241623\n",
            "The 542 batch, training loss is 0.1835053712129593\n",
            "The 543 batch, training loss is 0.11909575015306473\n",
            "The 544 batch, training loss is 0.08546213805675507\n",
            "The 545 batch, training loss is 0.16434268653392792\n",
            "The 546 batch, training loss is 0.04992131143808365\n",
            "The 547 batch, training loss is 0.18022076785564423\n",
            "The 548 batch, training loss is 0.12059647589921951\n",
            "The 549 batch, training loss is 0.11574270576238632\n",
            "The 550 batch, training loss is 0.10274433344602585\n",
            "The 551 batch, training loss is 0.15890777111053467\n",
            "The 552 batch, training loss is 0.18859829008579254\n",
            "The 553 batch, training loss is 0.14575433731079102\n",
            "The 554 batch, training loss is 0.08028923720121384\n",
            "The 555 batch, training loss is 0.11528753489255905\n",
            "The 556 batch, training loss is 0.13167540729045868\n",
            "The 557 batch, training loss is 0.17254933714866638\n",
            "The 558 batch, training loss is 0.15954986214637756\n",
            "The 559 batch, training loss is 0.11418641358613968\n",
            "The 560 batch, training loss is 0.13823208212852478\n",
            "The 561 batch, training loss is 0.03323565796017647\n",
            "The 562 batch, training loss is 0.12665298581123352\n",
            "The 563 batch, training loss is 0.2295062392950058\n",
            "The 564 batch, training loss is 0.11006848514080048\n",
            "The 565 batch, training loss is 0.14435788989067078\n",
            "The 566 batch, training loss is 0.1890324354171753\n",
            "The 567 batch, training loss is 0.10016756504774094\n",
            "The 568 batch, training loss is 0.19006694853305817\n",
            "The 569 batch, training loss is 0.1269141286611557\n",
            "The 570 batch, training loss is 0.11746518313884735\n",
            "The 571 batch, training loss is 0.1564488559961319\n",
            "The 572 batch, training loss is 0.07805639505386353\n",
            "The 573 batch, training loss is 0.14308229088783264\n",
            "The 574 batch, training loss is 0.1912868320941925\n",
            "The 575 batch, training loss is 0.20026633143424988\n",
            "The 576 batch, training loss is 0.2491338700056076\n",
            "The 577 batch, training loss is 0.11115123331546783\n",
            "The 578 batch, training loss is 0.1607237011194229\n",
            "The 579 batch, training loss is 0.16117843985557556\n",
            "The 580 batch, training loss is 0.10210957378149033\n",
            "The 581 batch, training loss is 0.28311288356781006\n",
            "The 582 batch, training loss is 0.11909808963537216\n",
            "The 583 batch, training loss is 0.11926634609699249\n",
            "The 584 batch, training loss is 0.22343890368938446\n",
            "The 585 batch, training loss is 0.16273419559001923\n",
            "The 586 batch, training loss is 0.18380865454673767\n",
            "The 587 batch, training loss is 0.0954294428229332\n",
            "The 588 batch, training loss is 0.18342894315719604\n",
            "The 589 batch, training loss is 0.12424670159816742\n",
            "The 590 batch, training loss is 0.21251793205738068\n",
            "The 591 batch, training loss is 0.15154802799224854\n",
            "The 592 batch, training loss is 0.1312808096408844\n",
            "The 593 batch, training loss is 0.11756135523319244\n",
            "The 594 batch, training loss is 0.11735344678163528\n",
            "The 595 batch, training loss is 0.11481142789125443\n",
            "The 596 batch, training loss is 0.21162566542625427\n",
            "The 597 batch, training loss is 0.21282504498958588\n",
            "The 598 batch, training loss is 0.2216179370880127\n",
            "The 599 batch, training loss is 0.1452278345823288\n",
            "The 600 batch, training loss is 0.32977014780044556\n",
            "The 601 batch, training loss is 0.1188722550868988\n",
            "The 602 batch, training loss is 0.08363201469182968\n",
            "The 603 batch, training loss is 0.2449423223733902\n",
            "The 604 batch, training loss is 0.24576734006404877\n",
            "The 605 batch, training loss is 0.09314367175102234\n",
            "The 606 batch, training loss is 0.0759790912270546\n",
            "The 607 batch, training loss is 0.13713635504245758\n",
            "The 608 batch, training loss is 0.266144335269928\n",
            "The 609 batch, training loss is 0.19503051042556763\n",
            "The 610 batch, training loss is 0.09316466003656387\n",
            "The 611 batch, training loss is 0.16029182076454163\n",
            "The 612 batch, training loss is 0.1726626753807068\n",
            "The 613 batch, training loss is 0.13103605806827545\n",
            "The 614 batch, training loss is 0.12151145190000534\n",
            "The 615 batch, training loss is 0.10468171536922455\n",
            "The 616 batch, training loss is 0.14964556694030762\n",
            "The 617 batch, training loss is 0.10490597784519196\n",
            "The 618 batch, training loss is 0.2120080292224884\n",
            "The 619 batch, training loss is 0.155486062169075\n",
            "The 620 batch, training loss is 0.12936212122440338\n",
            "The 621 batch, training loss is 0.08471772074699402\n",
            "The 622 batch, training loss is 0.08381237089633942\n",
            "The 623 batch, training loss is 0.14595718681812286\n",
            "The 624 batch, training loss is 0.15858951210975647\n",
            "The 625 batch, training loss is 0.09904198348522186\n",
            "The 626 batch, training loss is 0.14082102477550507\n",
            "The 627 batch, training loss is 0.2247426062822342\n",
            "The 628 batch, training loss is 0.06890598684549332\n",
            "The 629 batch, training loss is 0.15024954080581665\n",
            "The 630 batch, training loss is 0.20920468866825104\n",
            "The 631 batch, training loss is 0.08697792142629623\n",
            "The 632 batch, training loss is 0.09435867518186569\n",
            "The 633 batch, training loss is 0.08335904777050018\n",
            "The 634 batch, training loss is 0.19167426228523254\n",
            "The 635 batch, training loss is 0.1792045533657074\n",
            "The 636 batch, training loss is 0.0957970917224884\n",
            "The 637 batch, training loss is 0.1022929772734642\n",
            "The 638 batch, training loss is 0.13640627264976501\n",
            "The 639 batch, training loss is 0.0868685245513916\n",
            "The 640 batch, training loss is 0.16651375591754913\n",
            "The 641 batch, training loss is 0.13514459133148193\n",
            "The 642 batch, training loss is 0.12531252205371857\n",
            "The 643 batch, training loss is 0.058070745319128036\n",
            "The 644 batch, training loss is 0.10105277597904205\n",
            "The 645 batch, training loss is 0.2938470244407654\n",
            "The 646 batch, training loss is 0.19211414456367493\n",
            "The 647 batch, training loss is 0.10501797497272491\n",
            "The 648 batch, training loss is 0.13260680437088013\n",
            "The 649 batch, training loss is 0.12916724383831024\n",
            "The 650 batch, training loss is 0.2189311981201172\n",
            "The 651 batch, training loss is 0.12575329840183258\n",
            "The 652 batch, training loss is 0.0703343078494072\n",
            "The 653 batch, training loss is 0.14824999868869781\n",
            "The 654 batch, training loss is 0.0603087954223156\n",
            "The 655 batch, training loss is 0.29789772629737854\n",
            "The 656 batch, training loss is 0.17534881830215454\n",
            "The 657 batch, training loss is 0.08810905367136002\n",
            "The 658 batch, training loss is 0.17114518582820892\n",
            "The 659 batch, training loss is 0.0558856837451458\n",
            "The 660 batch, training loss is 0.06362227350473404\n",
            "The 661 batch, training loss is 0.11795172095298767\n",
            "The 662 batch, training loss is 0.17078641057014465\n",
            "The 663 batch, training loss is 0.10149510949850082\n",
            "The 664 batch, training loss is 0.3073562979698181\n",
            "The 665 batch, training loss is 0.13350450992584229\n",
            "The 666 batch, training loss is 0.14973321557044983\n",
            "The 667 batch, training loss is 0.10708395391702652\n",
            "The 668 batch, training loss is 0.21226726472377777\n",
            "The 669 batch, training loss is 0.08753014355897903\n",
            "The 670 batch, training loss is 0.2163284868001938\n",
            "The 671 batch, training loss is 0.07625161111354828\n",
            "The 672 batch, training loss is 0.13186143338680267\n",
            "The 673 batch, training loss is 0.027587195858359337\n",
            "The 674 batch, training loss is 0.214805468916893\n",
            "The 675 batch, training loss is 0.20706512033939362\n",
            "The 676 batch, training loss is 0.28156089782714844\n",
            "The 677 batch, training loss is 0.3200298845767975\n",
            "The 678 batch, training loss is 0.057690586894750595\n",
            "The 679 batch, training loss is 0.18995897471904755\n",
            "The 680 batch, training loss is 0.1295783668756485\n",
            "The 681 batch, training loss is 0.2129719853401184\n",
            "The 682 batch, training loss is 0.08439288288354874\n",
            "The 683 batch, training loss is 0.13130047917366028\n",
            "The 684 batch, training loss is 0.08863239735364914\n",
            "The 685 batch, training loss is 0.08156692236661911\n",
            "The 686 batch, training loss is 0.08623934537172318\n",
            "The 687 batch, training loss is 0.16083396971225739\n",
            "The 688 batch, training loss is 0.06756363064050674\n",
            "The 689 batch, training loss is 0.11116692423820496\n",
            "The 690 batch, training loss is 0.18579864501953125\n",
            "The 691 batch, training loss is 0.12490156292915344\n",
            "The 692 batch, training loss is 0.06330922991037369\n",
            "The 693 batch, training loss is 0.18689797818660736\n",
            "The 694 batch, training loss is 0.11875803023576736\n",
            "The 695 batch, training loss is 0.1268022358417511\n",
            "The 696 batch, training loss is 0.06348446756601334\n",
            "The 697 batch, training loss is 0.2794802486896515\n",
            "The 698 batch, training loss is 0.19234594702720642\n",
            "The 699 batch, training loss is 0.1226232647895813\n",
            "The 700 batch, training loss is 0.08115055412054062\n",
            "The 701 batch, training loss is 0.09149473160505295\n",
            "The 702 batch, training loss is 0.07061439752578735\n",
            "The 703 batch, training loss is 0.10980038344860077\n",
            "The 704 batch, training loss is 0.11182734370231628\n",
            "The 705 batch, training loss is 0.052358388900756836\n",
            "The 706 batch, training loss is 0.2517191171646118\n",
            "The 707 batch, training loss is 0.17168360948562622\n",
            "The 708 batch, training loss is 0.16965587437152863\n",
            "The 709 batch, training loss is 0.11259385198354721\n",
            "The 710 batch, training loss is 0.14255250990390778\n",
            "The 711 batch, training loss is 0.07372570037841797\n",
            "The 712 batch, training loss is 0.37260496616363525\n",
            "The 713 batch, training loss is 0.19758649170398712\n",
            "The 714 batch, training loss is 0.026160836219787598\n",
            "The 715 batch, training loss is 0.07488282024860382\n",
            "The 716 batch, training loss is 0.16350971162319183\n",
            "The 717 batch, training loss is 0.1679466813802719\n",
            "The 718 batch, training loss is 0.08616790920495987\n",
            "The 719 batch, training loss is 0.319244921207428\n",
            "The 720 batch, training loss is 0.1374068409204483\n",
            "The 721 batch, training loss is 0.1775115430355072\n",
            "The 722 batch, training loss is 0.11376508325338364\n",
            "The 723 batch, training loss is 0.06021510437130928\n",
            "The 724 batch, training loss is 0.15249115228652954\n",
            "The 725 batch, training loss is 0.13899902999401093\n",
            "The 726 batch, training loss is 0.1362372189760208\n",
            "The 727 batch, training loss is 0.12054461240768433\n",
            "The 728 batch, training loss is 0.027534404769539833\n",
            "The 729 batch, training loss is 0.09195086359977722\n",
            "The 730 batch, training loss is 0.16730742156505585\n",
            "The 731 batch, training loss is 0.19465260207653046\n",
            "The 732 batch, training loss is 0.10182099044322968\n",
            "The 733 batch, training loss is 0.10931708663702011\n",
            "The 734 batch, training loss is 0.2848736345767975\n",
            "The 735 batch, training loss is 0.06276816129684448\n",
            "The 736 batch, training loss is 0.08292075991630554\n",
            "The 737 batch, training loss is 0.21042479574680328\n",
            "The 738 batch, training loss is 0.08189026266336441\n",
            "The 739 batch, training loss is 0.08996142446994781\n",
            "The 740 batch, training loss is 0.15707014501094818\n",
            "The 741 batch, training loss is 0.09897729009389877\n",
            "The 742 batch, training loss is 0.10972627252340317\n",
            "The 743 batch, training loss is 0.09591352194547653\n",
            "The 744 batch, training loss is 0.26054251194000244\n",
            "The 745 batch, training loss is 0.1316870152950287\n",
            "The 746 batch, training loss is 0.11372911930084229\n",
            "The 747 batch, training loss is 0.12432316690683365\n",
            "The 748 batch, training loss is 0.13572147488594055\n",
            "The 749 batch, training loss is 0.13180123269557953\n",
            "The 750 batch, training loss is 0.14420294761657715\n",
            "The 751 batch, training loss is 0.11050350219011307\n",
            "The 752 batch, training loss is 0.10347076505422592\n",
            "The 753 batch, training loss is 0.17310871183872223\n",
            "The 754 batch, training loss is 0.1187339723110199\n",
            "The 755 batch, training loss is 0.07192777842283249\n",
            "The 756 batch, training loss is 0.17733968794345856\n",
            "The 757 batch, training loss is 0.03801197186112404\n",
            "The 758 batch, training loss is 0.21303066611289978\n",
            "The 759 batch, training loss is 0.15310347080230713\n",
            "The 760 batch, training loss is 0.16726845502853394\n",
            "The 761 batch, training loss is 0.04751008749008179\n",
            "The 762 batch, training loss is 0.03878747671842575\n",
            "The 763 batch, training loss is 0.0384468212723732\n",
            "The 764 batch, training loss is 0.1729184091091156\n",
            "The 765 batch, training loss is 0.10358188301324844\n",
            "The 766 batch, training loss is 0.1839236319065094\n",
            "The 767 batch, training loss is 0.17116278409957886\n",
            "The 768 batch, training loss is 0.1281614899635315\n",
            "The 769 batch, training loss is 0.1393626481294632\n",
            "The 770 batch, training loss is 0.12293168902397156\n",
            "The 771 batch, training loss is 0.11770879477262497\n",
            "The 772 batch, training loss is 0.26837292313575745\n",
            "The 773 batch, training loss is 0.1391979306936264\n",
            "The 774 batch, training loss is 0.1481133997440338\n",
            "The 775 batch, training loss is 0.09464490413665771\n",
            "The 776 batch, training loss is 0.29123109579086304\n",
            "The 777 batch, training loss is 0.3739894926548004\n",
            "The 778 batch, training loss is 0.2204861044883728\n",
            "The 779 batch, training loss is 0.07806071639060974\n",
            "The 780 batch, training loss is 0.05696716904640198\n",
            "The 781 batch, training loss is 0.12108204513788223\n",
            "The 782 batch, training loss is 0.1200619712471962\n",
            "The 783 batch, training loss is 0.12885387241840363\n",
            "The 784 batch, training loss is 0.18183229863643646\n",
            "The 785 batch, training loss is 0.155355304479599\n",
            "The 786 batch, training loss is 0.059418752789497375\n",
            "The 787 batch, training loss is 0.12017945200204849\n",
            "The 788 batch, training loss is 0.08195093274116516\n",
            "The 789 batch, training loss is 0.13403064012527466\n",
            "The 790 batch, training loss is 0.24498021602630615\n",
            "The 791 batch, training loss is 0.1317022144794464\n",
            "The 792 batch, training loss is 0.13638059794902802\n",
            "The 793 batch, training loss is 0.2569388151168823\n",
            "The 794 batch, training loss is 0.2626599073410034\n",
            "The 795 batch, training loss is 0.2805238962173462\n",
            "The 796 batch, training loss is 0.17648756504058838\n",
            "The 797 batch, training loss is 0.2313930094242096\n",
            "The 798 batch, training loss is 0.07852799445390701\n",
            "The 799 batch, training loss is 0.11363586038351059\n",
            "The 800 batch, training loss is 0.1417643278837204\n",
            "The 801 batch, training loss is 0.2013317197561264\n",
            "The 802 batch, training loss is 0.18412010371685028\n",
            "The 803 batch, training loss is 0.15882210433483124\n",
            "The 804 batch, training loss is 0.22858268022537231\n",
            "The 805 batch, training loss is 0.08496005088090897\n",
            "The 806 batch, training loss is 0.26535430550575256\n",
            "The 807 batch, training loss is 0.11009383201599121\n",
            "The 808 batch, training loss is 0.12396933883428574\n",
            "The 809 batch, training loss is 0.1644783914089203\n",
            "The 810 batch, training loss is 0.10736896097660065\n",
            "The 811 batch, training loss is 0.11790335178375244\n",
            "The 812 batch, training loss is 0.11645825952291489\n",
            "The 813 batch, training loss is 0.13759607076644897\n",
            "The 814 batch, training loss is 0.12238229811191559\n",
            "The 815 batch, training loss is 0.11887041479349136\n",
            "The 816 batch, training loss is 0.12728333473205566\n",
            "The 817 batch, training loss is 0.180387482047081\n",
            "The 818 batch, training loss is 0.06780162453651428\n",
            "The 819 batch, training loss is 0.15586650371551514\n",
            "The 820 batch, training loss is 0.17224818468093872\n",
            "The 821 batch, training loss is 0.2908269762992859\n",
            "The 822 batch, training loss is 0.16515585780143738\n",
            "The 823 batch, training loss is 0.11822804063558578\n",
            "The 824 batch, training loss is 0.2306426465511322\n",
            "The 825 batch, training loss is 0.15133801102638245\n",
            "The 826 batch, training loss is 0.1408170908689499\n",
            "The 827 batch, training loss is 0.15122511982917786\n",
            "The 828 batch, training loss is 0.0876590684056282\n",
            "The 829 batch, training loss is 0.08305186778306961\n",
            "The 830 batch, training loss is 0.09866968542337418\n",
            "The 831 batch, training loss is 0.08829697966575623\n",
            "The 832 batch, training loss is 0.1090000569820404\n",
            "The 833 batch, training loss is 0.07341029495000839\n",
            "The 834 batch, training loss is 0.11515507102012634\n",
            "The 835 batch, training loss is 0.23940880596637726\n",
            "The 836 batch, training loss is 0.20425297319889069\n",
            "The 837 batch, training loss is 0.1619168519973755\n",
            "The 838 batch, training loss is 0.07697668671607971\n",
            "The 839 batch, training loss is 0.18741066753864288\n",
            "The 840 batch, training loss is 0.12205100804567337\n",
            "The 841 batch, training loss is 0.1810886263847351\n",
            "The 842 batch, training loss is 0.12946617603302002\n",
            "The 843 batch, training loss is 0.14904911816120148\n",
            "The 844 batch, training loss is 0.18956387042999268\n",
            "The 845 batch, training loss is 0.09398718923330307\n",
            "The 846 batch, training loss is 0.07540958374738693\n",
            "The 847 batch, training loss is 0.12259326130151749\n",
            "The 848 batch, training loss is 0.08656716346740723\n",
            "The 849 batch, training loss is 0.1379881650209427\n",
            "The 850 batch, training loss is 0.167855903506279\n",
            "The 851 batch, training loss is 0.10836497694253922\n",
            "The 852 batch, training loss is 0.10567056387662888\n",
            "The 853 batch, training loss is 0.20443996787071228\n",
            "The 854 batch, training loss is 0.16972596943378448\n",
            "The 855 batch, training loss is 0.17579145729541779\n",
            "The 856 batch, training loss is 0.0715022087097168\n",
            "The 857 batch, training loss is 0.08198459446430206\n",
            "The 858 batch, training loss is 0.2952294945716858\n",
            "The 859 batch, training loss is 0.36869966983795166\n",
            "The 860 batch, training loss is 0.05402369797229767\n",
            "The 861 batch, training loss is 0.1273484081029892\n",
            "The 862 batch, training loss is 0.20886296033859253\n",
            "The 863 batch, training loss is 0.09432435035705566\n",
            "The 864 batch, training loss is 0.15068446099758148\n",
            "The 865 batch, training loss is 0.15074381232261658\n",
            "The 866 batch, training loss is 0.25007620453834534\n",
            "The 867 batch, training loss is 0.11650710552930832\n",
            "The 868 batch, training loss is 0.12097908556461334\n",
            "The 869 batch, training loss is 0.11987648159265518\n",
            "The 870 batch, training loss is 0.13316543400287628\n",
            "The 871 batch, training loss is 0.08260564506053925\n",
            "The 872 batch, training loss is 0.11422907561063766\n",
            "The 873 batch, training loss is 0.13511711359024048\n",
            "The 874 batch, training loss is 0.10756000131368637\n",
            "The 875 batch, training loss is 0.19065776467323303\n",
            "The 876 batch, training loss is 0.19072119891643524\n",
            "The 877 batch, training loss is 0.19992581009864807\n",
            "The 878 batch, training loss is 0.2061983346939087\n",
            "The 879 batch, training loss is 0.1454753428697586\n",
            "The 880 batch, training loss is 0.10228241980075836\n",
            "The 881 batch, training loss is 0.17007236182689667\n",
            "The 882 batch, training loss is 0.2287040650844574\n",
            "The 883 batch, training loss is 0.08440791070461273\n",
            "The 884 batch, training loss is 0.07276942580938339\n",
            "The 885 batch, training loss is 0.2045256346464157\n",
            "The 886 batch, training loss is 0.1595814973115921\n",
            "The 887 batch, training loss is 0.3161138594150543\n",
            "The 888 batch, training loss is 0.12606140971183777\n",
            "The 889 batch, training loss is 0.1998649388551712\n",
            "The 890 batch, training loss is 0.16410984098911285\n",
            "The 891 batch, training loss is 0.13083872199058533\n",
            "The 892 batch, training loss is 0.1506028026342392\n",
            "The 893 batch, training loss is 0.11550868302583694\n",
            "The 894 batch, training loss is 0.2677779793739319\n",
            "The 895 batch, training loss is 0.12422054260969162\n",
            "The 896 batch, training loss is 0.09927141666412354\n",
            "The 897 batch, training loss is 0.18620826303958893\n",
            "The 898 batch, training loss is 0.06486692279577255\n",
            "The 899 batch, training loss is 0.22701109945774078\n",
            "The 900 batch, training loss is 0.12164436280727386\n",
            "The 901 batch, training loss is 0.4605080485343933\n",
            "The 902 batch, training loss is 0.16315796971321106\n",
            "The 903 batch, training loss is 0.07371240854263306\n",
            "The 904 batch, training loss is 0.20818664133548737\n",
            "The 905 batch, training loss is 0.19213323295116425\n",
            "The 906 batch, training loss is 0.1297224462032318\n",
            "The 907 batch, training loss is 0.13693134486675262\n",
            "The 908 batch, training loss is 0.13692378997802734\n",
            "The 909 batch, training loss is 0.07315622270107269\n",
            "The 910 batch, training loss is 0.19641904532909393\n",
            "The 911 batch, training loss is 0.168233260512352\n",
            "The 912 batch, training loss is 0.15311135351657867\n",
            "The 913 batch, training loss is 0.10769414156675339\n",
            "The 914 batch, training loss is 0.18346428871154785\n",
            "The 915 batch, training loss is 0.14770253002643585\n",
            "The 916 batch, training loss is 0.1585199534893036\n",
            "The 917 batch, training loss is 0.10413296520709991\n",
            "The 918 batch, training loss is 0.16576382517814636\n",
            "The 919 batch, training loss is 0.05824706330895424\n",
            "The 920 batch, training loss is 0.22018176317214966\n",
            "The 921 batch, training loss is 0.19990307092666626\n",
            "The 922 batch, training loss is 0.16003668308258057\n",
            "The 923 batch, training loss is 0.18666431307792664\n",
            "The 924 batch, training loss is 0.13492567837238312\n",
            "The 925 batch, training loss is 0.15949031710624695\n",
            "The 926 batch, training loss is 0.2868489921092987\n",
            "The 927 batch, training loss is 0.07290901988744736\n",
            "The 928 batch, training loss is 0.28127405047416687\n",
            "The 929 batch, training loss is 0.13829298317432404\n",
            "The 930 batch, training loss is 0.0907791405916214\n",
            "The 931 batch, training loss is 0.1130843311548233\n",
            "The 932 batch, training loss is 0.09662109613418579\n",
            "The 933 batch, training loss is 0.12795616686344147\n",
            "The 934 batch, training loss is 0.16610510647296906\n",
            "The 935 batch, training loss is 0.16671645641326904\n",
            "The 936 batch, training loss is 0.09240394085645676\n",
            "The 937 batch, training loss is 0.1136249378323555\n",
            "The 7 epoch, training loss is 0.1136249378323555\n",
            "The 0 batch, training loss is 0.10750134289264679\n",
            "The 1 batch, training loss is 0.09507980942726135\n",
            "The 2 batch, training loss is 0.14199887216091156\n",
            "The 3 batch, training loss is 0.1560695916414261\n",
            "The 4 batch, training loss is 0.1102985367178917\n",
            "The 5 batch, training loss is 0.12534578144550323\n",
            "The 6 batch, training loss is 0.06771840900182724\n",
            "The 7 batch, training loss is 0.10348393768072128\n",
            "The 8 batch, training loss is 0.17022693157196045\n",
            "The 9 batch, training loss is 0.09406429529190063\n",
            "The 10 batch, training loss is 0.11978647112846375\n",
            "The 11 batch, training loss is 0.08156772702932358\n",
            "The 12 batch, training loss is 0.12770067155361176\n",
            "The 13 batch, training loss is 0.2149096131324768\n",
            "The 14 batch, training loss is 0.24383066594600677\n",
            "The 15 batch, training loss is 0.11259470134973526\n",
            "The 16 batch, training loss is 0.25112661719322205\n",
            "The 17 batch, training loss is 0.14000950753688812\n",
            "The 18 batch, training loss is 0.10513153672218323\n",
            "The 19 batch, training loss is 0.30168986320495605\n",
            "The 20 batch, training loss is 0.1809621900320053\n",
            "The 21 batch, training loss is 0.12474717199802399\n",
            "The 22 batch, training loss is 0.23067228496074677\n",
            "The 23 batch, training loss is 0.11459249258041382\n",
            "The 24 batch, training loss is 0.06856997311115265\n",
            "The 25 batch, training loss is 0.058549195528030396\n",
            "The 26 batch, training loss is 0.20748403668403625\n",
            "The 27 batch, training loss is 0.11507861316204071\n",
            "The 28 batch, training loss is 0.06213456392288208\n",
            "The 29 batch, training loss is 0.08060295879840851\n",
            "The 30 batch, training loss is 0.08043444901704788\n",
            "The 31 batch, training loss is 0.09694922715425491\n",
            "The 32 batch, training loss is 0.17869704961776733\n",
            "The 33 batch, training loss is 0.18837180733680725\n",
            "The 34 batch, training loss is 0.18724577128887177\n",
            "The 35 batch, training loss is 0.1185247153043747\n",
            "The 36 batch, training loss is 0.07959278672933578\n",
            "The 37 batch, training loss is 0.11566536128520966\n",
            "The 38 batch, training loss is 0.25971630215644836\n",
            "The 39 batch, training loss is 0.15386559069156647\n",
            "The 40 batch, training loss is 0.04948938637971878\n",
            "The 41 batch, training loss is 0.21364042162895203\n",
            "The 42 batch, training loss is 0.15081475675106049\n",
            "The 43 batch, training loss is 0.20357054471969604\n",
            "The 44 batch, training loss is 0.06543903797864914\n",
            "The 45 batch, training loss is 0.15348529815673828\n",
            "The 46 batch, training loss is 0.13263839483261108\n",
            "The 47 batch, training loss is 0.09180478751659393\n",
            "The 48 batch, training loss is 0.3187171220779419\n",
            "The 49 batch, training loss is 0.20394663512706757\n",
            "The 50 batch, training loss is 0.09477246552705765\n",
            "The 51 batch, training loss is 0.21388539671897888\n",
            "The 52 batch, training loss is 0.18967925012111664\n",
            "The 53 batch, training loss is 0.15704728662967682\n",
            "The 54 batch, training loss is 0.18745765089988708\n",
            "The 55 batch, training loss is 0.10779377073049545\n",
            "The 56 batch, training loss is 0.16782063245773315\n",
            "The 57 batch, training loss is 0.18802334368228912\n",
            "The 58 batch, training loss is 0.11670217663049698\n",
            "The 59 batch, training loss is 0.10597027838230133\n",
            "The 60 batch, training loss is 0.05550382658839226\n",
            "The 61 batch, training loss is 0.14322017133235931\n",
            "The 62 batch, training loss is 0.11894002556800842\n",
            "The 63 batch, training loss is 0.15379923582077026\n",
            "The 64 batch, training loss is 0.2423839569091797\n",
            "The 65 batch, training loss is 0.07700113952159882\n",
            "The 66 batch, training loss is 0.12894266843795776\n",
            "The 67 batch, training loss is 0.14357981085777283\n",
            "The 68 batch, training loss is 0.08257179707288742\n",
            "The 69 batch, training loss is 0.08230879157781601\n",
            "The 70 batch, training loss is 0.07865329086780548\n",
            "The 71 batch, training loss is 0.19502201676368713\n",
            "The 72 batch, training loss is 0.11178185045719147\n",
            "The 73 batch, training loss is 0.2024785429239273\n",
            "The 74 batch, training loss is 0.1730812042951584\n",
            "The 75 batch, training loss is 0.14811201393604279\n",
            "The 76 batch, training loss is 0.22671839594841003\n",
            "The 77 batch, training loss is 0.1630176305770874\n",
            "The 78 batch, training loss is 0.0725129023194313\n",
            "The 79 batch, training loss is 0.09689030796289444\n",
            "The 80 batch, training loss is 0.09969598799943924\n",
            "The 81 batch, training loss is 0.10466907918453217\n",
            "The 82 batch, training loss is 0.049468886107206345\n",
            "The 83 batch, training loss is 0.09739477187395096\n",
            "The 84 batch, training loss is 0.02434242144227028\n",
            "The 85 batch, training loss is 0.15514005720615387\n",
            "The 86 batch, training loss is 0.10841921716928482\n",
            "The 87 batch, training loss is 0.09231030195951462\n",
            "The 88 batch, training loss is 0.05242542549967766\n",
            "The 89 batch, training loss is 0.07482839375734329\n",
            "The 90 batch, training loss is 0.13231588900089264\n",
            "The 91 batch, training loss is 0.08050571382045746\n",
            "The 92 batch, training loss is 0.16391026973724365\n",
            "The 93 batch, training loss is 0.15129339694976807\n",
            "The 94 batch, training loss is 0.27702826261520386\n",
            "The 95 batch, training loss is 0.10888802260160446\n",
            "The 96 batch, training loss is 0.040487416088581085\n",
            "The 97 batch, training loss is 0.25786858797073364\n",
            "The 98 batch, training loss is 0.03626272827386856\n",
            "The 99 batch, training loss is 0.1658177524805069\n",
            "The 100 batch, training loss is 0.08208609372377396\n",
            "The 101 batch, training loss is 0.05891186743974686\n",
            "The 102 batch, training loss is 0.23185747861862183\n",
            "The 103 batch, training loss is 0.06249332055449486\n",
            "The 104 batch, training loss is 0.03893984854221344\n",
            "The 105 batch, training loss is 0.07595588266849518\n",
            "The 106 batch, training loss is 0.0849280133843422\n",
            "The 107 batch, training loss is 0.1357591599225998\n",
            "The 108 batch, training loss is 0.043050531297922134\n",
            "The 109 batch, training loss is 0.11982519179582596\n",
            "The 110 batch, training loss is 0.06771770119667053\n",
            "The 111 batch, training loss is 0.09073330461978912\n",
            "The 112 batch, training loss is 0.23248551785945892\n",
            "The 113 batch, training loss is 0.11779099702835083\n",
            "The 114 batch, training loss is 0.07708567380905151\n",
            "The 115 batch, training loss is 0.17621122300624847\n",
            "The 116 batch, training loss is 0.21090449392795563\n",
            "The 117 batch, training loss is 0.11638342589139938\n",
            "The 118 batch, training loss is 0.06356489658355713\n",
            "The 119 batch, training loss is 0.06010798364877701\n",
            "The 120 batch, training loss is 0.08011280745267868\n",
            "The 121 batch, training loss is 0.13665412366390228\n",
            "The 122 batch, training loss is 0.13408763706684113\n",
            "The 123 batch, training loss is 0.21115770936012268\n",
            "The 124 batch, training loss is 0.13389864563941956\n",
            "The 125 batch, training loss is 0.05756146460771561\n",
            "The 126 batch, training loss is 0.1774580031633377\n",
            "The 127 batch, training loss is 0.09452936053276062\n",
            "The 128 batch, training loss is 0.18892152607440948\n",
            "The 129 batch, training loss is 0.19937339425086975\n",
            "The 130 batch, training loss is 0.11876533925533295\n",
            "The 131 batch, training loss is 0.054731085896492004\n",
            "The 132 batch, training loss is 0.12511992454528809\n",
            "The 133 batch, training loss is 0.11081710457801819\n",
            "The 134 batch, training loss is 0.1736418902873993\n",
            "The 135 batch, training loss is 0.19516931474208832\n",
            "The 136 batch, training loss is 0.14470313489437103\n",
            "The 137 batch, training loss is 0.07633226364850998\n",
            "The 138 batch, training loss is 0.20584867894649506\n",
            "The 139 batch, training loss is 0.08521193265914917\n",
            "The 140 batch, training loss is 0.03633595630526543\n",
            "The 141 batch, training loss is 0.05786256119608879\n",
            "The 142 batch, training loss is 0.06673979759216309\n",
            "The 143 batch, training loss is 0.20846129953861237\n",
            "The 144 batch, training loss is 0.17880231142044067\n",
            "The 145 batch, training loss is 0.10174142569303513\n",
            "The 146 batch, training loss is 0.05347602441906929\n",
            "The 147 batch, training loss is 0.07444804906845093\n",
            "The 148 batch, training loss is 0.053663913160562515\n",
            "The 149 batch, training loss is 0.16716165840625763\n",
            "The 150 batch, training loss is 0.2664119303226471\n",
            "The 151 batch, training loss is 0.07557430118322372\n",
            "The 152 batch, training loss is 0.1941915601491928\n",
            "The 153 batch, training loss is 0.28076493740081787\n",
            "The 154 batch, training loss is 0.08889401704072952\n",
            "The 155 batch, training loss is 0.12795743346214294\n",
            "The 156 batch, training loss is 0.12685419619083405\n",
            "The 157 batch, training loss is 0.11112990230321884\n",
            "The 158 batch, training loss is 0.14776618778705597\n",
            "The 159 batch, training loss is 0.4526480436325073\n",
            "The 160 batch, training loss is 0.11750876903533936\n",
            "The 161 batch, training loss is 0.03325134888291359\n",
            "The 162 batch, training loss is 0.07300379872322083\n",
            "The 163 batch, training loss is 0.1561235934495926\n",
            "The 164 batch, training loss is 0.23310963809490204\n",
            "The 165 batch, training loss is 0.1747816503047943\n",
            "The 166 batch, training loss is 0.10214119404554367\n",
            "The 167 batch, training loss is 0.16612720489501953\n",
            "The 168 batch, training loss is 0.17276005446910858\n",
            "The 169 batch, training loss is 0.20914685726165771\n",
            "The 170 batch, training loss is 0.1169983446598053\n",
            "The 171 batch, training loss is 0.13492822647094727\n",
            "The 172 batch, training loss is 0.1617201566696167\n",
            "The 173 batch, training loss is 0.28746768832206726\n",
            "The 174 batch, training loss is 0.09791284054517746\n",
            "The 175 batch, training loss is 0.2352760285139084\n",
            "The 176 batch, training loss is 0.14021435379981995\n",
            "The 177 batch, training loss is 0.17504377663135529\n",
            "The 178 batch, training loss is 0.09683598577976227\n",
            "The 179 batch, training loss is 0.06512879580259323\n",
            "The 180 batch, training loss is 0.11965499818325043\n",
            "The 181 batch, training loss is 0.057058535516262054\n",
            "The 182 batch, training loss is 0.17814788222312927\n",
            "The 183 batch, training loss is 0.11699780821800232\n",
            "The 184 batch, training loss is 0.32842499017715454\n",
            "The 185 batch, training loss is 0.29135367274284363\n",
            "The 186 batch, training loss is 0.051435865461826324\n",
            "The 187 batch, training loss is 0.1795283704996109\n",
            "The 188 batch, training loss is 0.25548797845840454\n",
            "The 189 batch, training loss is 0.2776815593242645\n",
            "The 190 batch, training loss is 0.09018555283546448\n",
            "The 191 batch, training loss is 0.27866533398628235\n",
            "The 192 batch, training loss is 0.1311943233013153\n",
            "The 193 batch, training loss is 0.03549935296177864\n",
            "The 194 batch, training loss is 0.08719014376401901\n",
            "The 195 batch, training loss is 0.15487785637378693\n",
            "The 196 batch, training loss is 0.18614786863327026\n",
            "The 197 batch, training loss is 0.08108007907867432\n",
            "The 198 batch, training loss is 0.11763491481542587\n",
            "The 199 batch, training loss is 0.07588926702737808\n",
            "The 200 batch, training loss is 0.13998952507972717\n",
            "The 201 batch, training loss is 0.20953528583049774\n",
            "The 202 batch, training loss is 0.13999782502651215\n",
            "The 203 batch, training loss is 0.22357177734375\n",
            "The 204 batch, training loss is 0.14838044345378876\n",
            "The 205 batch, training loss is 0.2943064570426941\n",
            "The 206 batch, training loss is 0.03602758049964905\n",
            "The 207 batch, training loss is 0.1392773538827896\n",
            "The 208 batch, training loss is 0.07448757439851761\n",
            "The 209 batch, training loss is 0.16726914048194885\n",
            "The 210 batch, training loss is 0.05131526291370392\n",
            "The 211 batch, training loss is 0.09971277415752411\n",
            "The 212 batch, training loss is 0.18289366364479065\n",
            "The 213 batch, training loss is 0.07578067481517792\n",
            "The 214 batch, training loss is 0.09945690631866455\n",
            "The 215 batch, training loss is 0.1967673897743225\n",
            "The 216 batch, training loss is 0.10328186303377151\n",
            "The 217 batch, training loss is 0.11571715027093887\n",
            "The 218 batch, training loss is 0.2148921936750412\n",
            "The 219 batch, training loss is 0.17741437256336212\n",
            "The 220 batch, training loss is 0.0913420096039772\n",
            "The 221 batch, training loss is 0.1200987845659256\n",
            "The 222 batch, training loss is 0.05414411425590515\n",
            "The 223 batch, training loss is 0.08662769198417664\n",
            "The 224 batch, training loss is 0.13923001289367676\n",
            "The 225 batch, training loss is 0.09697366505861282\n",
            "The 226 batch, training loss is 0.09374357759952545\n",
            "The 227 batch, training loss is 0.2702217996120453\n",
            "The 228 batch, training loss is 0.22341004014015198\n",
            "The 229 batch, training loss is 0.11994241923093796\n",
            "The 230 batch, training loss is 0.26286250352859497\n",
            "The 231 batch, training loss is 0.138546884059906\n",
            "The 232 batch, training loss is 0.06605639308691025\n",
            "The 233 batch, training loss is 0.09705506265163422\n",
            "The 234 batch, training loss is 0.10694833844900131\n",
            "The 235 batch, training loss is 0.13362228870391846\n",
            "The 236 batch, training loss is 0.06742293387651443\n",
            "The 237 batch, training loss is 0.19524860382080078\n",
            "The 238 batch, training loss is 0.14031271636486053\n",
            "The 239 batch, training loss is 0.13050872087478638\n",
            "The 240 batch, training loss is 0.1217631846666336\n",
            "The 241 batch, training loss is 0.12915664911270142\n",
            "The 242 batch, training loss is 0.15266481041908264\n",
            "The 243 batch, training loss is 0.27639374136924744\n",
            "The 244 batch, training loss is 0.08407723158597946\n",
            "The 245 batch, training loss is 0.07759987562894821\n",
            "The 246 batch, training loss is 0.10490294545888901\n",
            "The 247 batch, training loss is 0.16232632100582123\n",
            "The 248 batch, training loss is 0.06025877222418785\n",
            "The 249 batch, training loss is 0.10154308378696442\n",
            "The 250 batch, training loss is 0.09222550690174103\n",
            "The 251 batch, training loss is 0.18640655279159546\n",
            "The 252 batch, training loss is 0.10725457221269608\n",
            "The 253 batch, training loss is 0.18868574500083923\n",
            "The 254 batch, training loss is 0.12881089746952057\n",
            "The 255 batch, training loss is 0.198136568069458\n",
            "The 256 batch, training loss is 0.33750858902931213\n",
            "The 257 batch, training loss is 0.14754441380500793\n",
            "The 258 batch, training loss is 0.061199843883514404\n",
            "The 259 batch, training loss is 0.18402524292469025\n",
            "The 260 batch, training loss is 0.28131765127182007\n",
            "The 261 batch, training loss is 0.08289971947669983\n",
            "The 262 batch, training loss is 0.07767342031002045\n",
            "The 263 batch, training loss is 0.04841245710849762\n",
            "The 264 batch, training loss is 0.17540957033634186\n",
            "The 265 batch, training loss is 0.2738887667655945\n",
            "The 266 batch, training loss is 0.23428501188755035\n",
            "The 267 batch, training loss is 0.13346171379089355\n",
            "The 268 batch, training loss is 0.14092685282230377\n",
            "The 269 batch, training loss is 0.19716596603393555\n",
            "The 270 batch, training loss is 0.15854890644550323\n",
            "The 271 batch, training loss is 0.13081125915050507\n",
            "The 272 batch, training loss is 0.12700675427913666\n",
            "The 273 batch, training loss is 0.07545319944620132\n",
            "The 274 batch, training loss is 0.1704077124595642\n",
            "The 275 batch, training loss is 0.12773573398590088\n",
            "The 276 batch, training loss is 0.05229320749640465\n",
            "The 277 batch, training loss is 0.27491846680641174\n",
            "The 278 batch, training loss is 0.10889241844415665\n",
            "The 279 batch, training loss is 0.11863294243812561\n",
            "The 280 batch, training loss is 0.11017690598964691\n",
            "The 281 batch, training loss is 0.29780739545822144\n",
            "The 282 batch, training loss is 0.22819027304649353\n",
            "The 283 batch, training loss is 0.16592824459075928\n",
            "The 284 batch, training loss is 0.18908877670764923\n",
            "The 285 batch, training loss is 0.17351530492305756\n",
            "The 286 batch, training loss is 0.08410006761550903\n",
            "The 287 batch, training loss is 0.15570665895938873\n",
            "The 288 batch, training loss is 0.08455733954906464\n",
            "The 289 batch, training loss is 0.06397489458322525\n",
            "The 290 batch, training loss is 0.25763508677482605\n",
            "The 291 batch, training loss is 0.2186194360256195\n",
            "The 292 batch, training loss is 0.09282352030277252\n",
            "The 293 batch, training loss is 0.11597342789173126\n",
            "The 294 batch, training loss is 0.04885256662964821\n",
            "The 295 batch, training loss is 0.026505522429943085\n",
            "The 296 batch, training loss is 0.11212942749261856\n",
            "The 297 batch, training loss is 0.21359165012836456\n",
            "The 298 batch, training loss is 0.17154629528522491\n",
            "The 299 batch, training loss is 0.1058335155248642\n",
            "The 300 batch, training loss is 0.11487841606140137\n",
            "The 301 batch, training loss is 0.13338667154312134\n",
            "The 302 batch, training loss is 0.12091585993766785\n",
            "The 303 batch, training loss is 0.1925252079963684\n",
            "The 304 batch, training loss is 0.2446737289428711\n",
            "The 305 batch, training loss is 0.13868911564350128\n",
            "The 306 batch, training loss is 0.06626007705926895\n",
            "The 307 batch, training loss is 0.048905689269304276\n",
            "The 308 batch, training loss is 0.04712807387113571\n",
            "The 309 batch, training loss is 0.11512203514575958\n",
            "The 310 batch, training loss is 0.06596089154481888\n",
            "The 311 batch, training loss is 0.06845588982105255\n",
            "The 312 batch, training loss is 0.1620675027370453\n",
            "The 313 batch, training loss is 0.1401941031217575\n",
            "The 314 batch, training loss is 0.16507604718208313\n",
            "The 315 batch, training loss is 0.1811138391494751\n",
            "The 316 batch, training loss is 0.11236327886581421\n",
            "The 317 batch, training loss is 0.12963411211967468\n",
            "The 318 batch, training loss is 0.134660542011261\n",
            "The 319 batch, training loss is 0.146916925907135\n",
            "The 320 batch, training loss is 0.089134581387043\n",
            "The 321 batch, training loss is 0.1656772792339325\n",
            "The 322 batch, training loss is 0.132610023021698\n",
            "The 323 batch, training loss is 0.18146471679210663\n",
            "The 324 batch, training loss is 0.04881634935736656\n",
            "The 325 batch, training loss is 0.128246009349823\n",
            "The 326 batch, training loss is 0.09297902882099152\n",
            "The 327 batch, training loss is 0.07144369184970856\n",
            "The 328 batch, training loss is 0.06266511976718903\n",
            "The 329 batch, training loss is 0.11623236536979675\n",
            "The 330 batch, training loss is 0.08213568478822708\n",
            "The 331 batch, training loss is 0.13855302333831787\n",
            "The 332 batch, training loss is 0.225735142827034\n",
            "The 333 batch, training loss is 0.3165806829929352\n",
            "The 334 batch, training loss is 0.07035807520151138\n",
            "The 335 batch, training loss is 0.2454742044210434\n",
            "The 336 batch, training loss is 0.10470231622457504\n",
            "The 337 batch, training loss is 0.08214980363845825\n",
            "The 338 batch, training loss is 0.05655346438288689\n",
            "The 339 batch, training loss is 0.257272869348526\n",
            "The 340 batch, training loss is 0.09096717089414597\n",
            "The 341 batch, training loss is 0.09123849123716354\n",
            "The 342 batch, training loss is 0.13560806214809418\n",
            "The 343 batch, training loss is 0.25140342116355896\n",
            "The 344 batch, training loss is 0.16122718155384064\n",
            "The 345 batch, training loss is 0.07040213793516159\n",
            "The 346 batch, training loss is 0.08158334344625473\n",
            "The 347 batch, training loss is 0.17400872707366943\n",
            "The 348 batch, training loss is 0.2579658329486847\n",
            "The 349 batch, training loss is 0.15083011984825134\n",
            "The 350 batch, training loss is 0.07644399255514145\n",
            "The 351 batch, training loss is 0.11676612496376038\n",
            "The 352 batch, training loss is 0.07420264929533005\n",
            "The 353 batch, training loss is 0.10756443440914154\n",
            "The 354 batch, training loss is 0.1746685653924942\n",
            "The 355 batch, training loss is 0.05862324312329292\n",
            "The 356 batch, training loss is 0.1084195002913475\n",
            "The 357 batch, training loss is 0.09921624511480331\n",
            "The 358 batch, training loss is 0.08554865419864655\n",
            "The 359 batch, training loss is 0.11832436919212341\n",
            "The 360 batch, training loss is 0.12953650951385498\n",
            "The 361 batch, training loss is 0.0875653550028801\n",
            "The 362 batch, training loss is 0.08085235208272934\n",
            "The 363 batch, training loss is 0.08767767995595932\n",
            "The 364 batch, training loss is 0.11462874710559845\n",
            "The 365 batch, training loss is 0.17999017238616943\n",
            "The 366 batch, training loss is 0.20964494347572327\n",
            "The 367 batch, training loss is 0.14882776141166687\n",
            "The 368 batch, training loss is 0.05307979881763458\n",
            "The 369 batch, training loss is 0.14505791664123535\n",
            "The 370 batch, training loss is 0.058546632528305054\n",
            "The 371 batch, training loss is 0.11851009726524353\n",
            "The 372 batch, training loss is 0.11249670386314392\n",
            "The 373 batch, training loss is 0.1391909122467041\n",
            "The 374 batch, training loss is 0.053841833025217056\n",
            "The 375 batch, training loss is 0.10070828348398209\n",
            "The 376 batch, training loss is 0.05511581897735596\n",
            "The 377 batch, training loss is 0.06441853195428848\n",
            "The 378 batch, training loss is 0.1509847193956375\n",
            "The 379 batch, training loss is 0.22259290516376495\n",
            "The 380 batch, training loss is 0.18445350229740143\n",
            "The 381 batch, training loss is 0.12119428813457489\n",
            "The 382 batch, training loss is 0.10671652108430862\n",
            "The 383 batch, training loss is 0.12406238913536072\n",
            "The 384 batch, training loss is 0.1521635800600052\n",
            "The 385 batch, training loss is 0.12671414017677307\n",
            "The 386 batch, training loss is 0.179233118891716\n",
            "The 387 batch, training loss is 0.0963437408208847\n",
            "The 388 batch, training loss is 0.05337677150964737\n",
            "The 389 batch, training loss is 0.0536014623939991\n",
            "The 390 batch, training loss is 0.07786490768194199\n",
            "The 391 batch, training loss is 0.0936511904001236\n",
            "The 392 batch, training loss is 0.16601234674453735\n",
            "The 393 batch, training loss is 0.316229909658432\n",
            "The 394 batch, training loss is 0.06574490666389465\n",
            "The 395 batch, training loss is 0.10857654362916946\n",
            "The 396 batch, training loss is 0.13128922879695892\n",
            "The 397 batch, training loss is 0.08848027139902115\n",
            "The 398 batch, training loss is 0.05403425544500351\n",
            "The 399 batch, training loss is 0.054543737322092056\n",
            "The 400 batch, training loss is 0.07183226197957993\n",
            "The 401 batch, training loss is 0.18087269365787506\n",
            "The 402 batch, training loss is 0.1740996092557907\n",
            "The 403 batch, training loss is 0.118545763194561\n",
            "The 404 batch, training loss is 0.2024904489517212\n",
            "The 405 batch, training loss is 0.08776748925447464\n",
            "The 406 batch, training loss is 0.1509004682302475\n",
            "The 407 batch, training loss is 0.234190434217453\n",
            "The 408 batch, training loss is 0.17423370480537415\n",
            "The 409 batch, training loss is 0.08283715695142746\n",
            "The 410 batch, training loss is 0.13418947160243988\n",
            "The 411 batch, training loss is 0.1703723520040512\n",
            "The 412 batch, training loss is 0.2570720911026001\n",
            "The 413 batch, training loss is 0.061056170612573624\n",
            "The 414 batch, training loss is 0.25148463249206543\n",
            "The 415 batch, training loss is 0.13807544112205505\n",
            "The 416 batch, training loss is 0.13892583549022675\n",
            "The 417 batch, training loss is 0.15405569970607758\n",
            "The 418 batch, training loss is 0.12805064022541046\n",
            "The 419 batch, training loss is 0.1501116305589676\n",
            "The 420 batch, training loss is 0.22102881968021393\n",
            "The 421 batch, training loss is 0.08093900978565216\n",
            "The 422 batch, training loss is 0.04390014708042145\n",
            "The 423 batch, training loss is 0.17350341379642487\n",
            "The 424 batch, training loss is 0.0806255042552948\n",
            "The 425 batch, training loss is 0.11549896001815796\n",
            "The 426 batch, training loss is 0.049186017364263535\n",
            "The 427 batch, training loss is 0.14251524209976196\n",
            "The 428 batch, training loss is 0.19661134481430054\n",
            "The 429 batch, training loss is 0.06716981530189514\n",
            "The 430 batch, training loss is 0.06807663291692734\n",
            "The 431 batch, training loss is 0.08894959092140198\n",
            "The 432 batch, training loss is 0.18567439913749695\n",
            "The 433 batch, training loss is 0.09918920695781708\n",
            "The 434 batch, training loss is 0.04328785091638565\n",
            "The 435 batch, training loss is 0.15253622829914093\n",
            "The 436 batch, training loss is 0.13231928646564484\n",
            "The 437 batch, training loss is 0.30229318141937256\n",
            "The 438 batch, training loss is 0.18249207735061646\n",
            "The 439 batch, training loss is 0.18596051633358002\n",
            "The 440 batch, training loss is 0.1273888647556305\n",
            "The 441 batch, training loss is 0.08773232251405716\n",
            "The 442 batch, training loss is 0.22304192185401917\n",
            "The 443 batch, training loss is 0.1424010992050171\n",
            "The 444 batch, training loss is 0.12710437178611755\n",
            "The 445 batch, training loss is 0.12658023834228516\n",
            "The 446 batch, training loss is 0.05386213958263397\n",
            "The 447 batch, training loss is 0.06481660902500153\n",
            "The 448 batch, training loss is 0.2055608034133911\n",
            "The 449 batch, training loss is 0.10848164558410645\n",
            "The 450 batch, training loss is 0.08250271528959274\n",
            "The 451 batch, training loss is 0.1300480216741562\n",
            "The 452 batch, training loss is 0.4202583134174347\n",
            "The 453 batch, training loss is 0.11098939180374146\n",
            "The 454 batch, training loss is 0.0827166885137558\n",
            "The 455 batch, training loss is 0.13462784886360168\n",
            "The 456 batch, training loss is 0.12216752767562866\n",
            "The 457 batch, training loss is 0.10612311214208603\n",
            "The 458 batch, training loss is 0.06945674866437912\n",
            "The 459 batch, training loss is 0.06216476112604141\n",
            "The 460 batch, training loss is 0.07688450813293457\n",
            "The 461 batch, training loss is 0.11468879878520966\n",
            "The 462 batch, training loss is 0.06259714812040329\n",
            "The 463 batch, training loss is 0.45068931579589844\n",
            "The 464 batch, training loss is 0.214570552110672\n",
            "The 465 batch, training loss is 0.07317619025707245\n",
            "The 466 batch, training loss is 0.077263742685318\n",
            "The 467 batch, training loss is 0.07075410336256027\n",
            "The 468 batch, training loss is 0.0954885333776474\n",
            "The 469 batch, training loss is 0.23164419829845428\n",
            "The 470 batch, training loss is 0.12758375704288483\n",
            "The 471 batch, training loss is 0.06076708808541298\n",
            "The 472 batch, training loss is 0.08079012483358383\n",
            "The 473 batch, training loss is 0.09483381360769272\n",
            "The 474 batch, training loss is 0.14601773023605347\n",
            "The 475 batch, training loss is 0.07127237319946289\n",
            "The 476 batch, training loss is 0.09385408461093903\n",
            "The 477 batch, training loss is 0.12987469136714935\n",
            "The 478 batch, training loss is 0.04984438046813011\n",
            "The 479 batch, training loss is 0.05853983387351036\n",
            "The 480 batch, training loss is 0.21503768861293793\n",
            "The 481 batch, training loss is 0.10899879783391953\n",
            "The 482 batch, training loss is 0.16674669086933136\n",
            "The 483 batch, training loss is 0.1274898797273636\n",
            "The 484 batch, training loss is 0.23995070159435272\n",
            "The 485 batch, training loss is 0.05404527485370636\n",
            "The 486 batch, training loss is 0.20642560720443726\n",
            "The 487 batch, training loss is 0.04643074795603752\n",
            "The 488 batch, training loss is 0.020699303597211838\n",
            "The 489 batch, training loss is 0.07553216069936752\n",
            "The 490 batch, training loss is 0.12721754610538483\n",
            "The 491 batch, training loss is 0.11865444481372833\n",
            "The 492 batch, training loss is 0.11884800344705582\n",
            "The 493 batch, training loss is 0.1905573010444641\n",
            "The 494 batch, training loss is 0.1931009143590927\n",
            "The 495 batch, training loss is 0.11734886467456818\n",
            "The 496 batch, training loss is 0.19960953295230865\n",
            "The 497 batch, training loss is 0.13648514449596405\n",
            "The 498 batch, training loss is 0.1407797932624817\n",
            "The 499 batch, training loss is 0.15332795679569244\n",
            "The 500 batch, training loss is 0.08624295145273209\n",
            "The 501 batch, training loss is 0.09107903391122818\n",
            "The 502 batch, training loss is 0.1448257863521576\n",
            "The 503 batch, training loss is 0.05423567071557045\n",
            "The 504 batch, training loss is 0.11124631762504578\n",
            "The 505 batch, training loss is 0.16468527913093567\n",
            "The 506 batch, training loss is 0.13448379933834076\n",
            "The 507 batch, training loss is 0.12375899404287338\n",
            "The 508 batch, training loss is 0.21183328330516815\n",
            "The 509 batch, training loss is 0.08031869679689407\n",
            "The 510 batch, training loss is 0.20003736019134521\n",
            "The 511 batch, training loss is 0.08288048952817917\n",
            "The 512 batch, training loss is 0.19979727268218994\n",
            "The 513 batch, training loss is 0.15309593081474304\n",
            "The 514 batch, training loss is 0.15744435787200928\n",
            "The 515 batch, training loss is 0.22003579139709473\n",
            "The 516 batch, training loss is 0.13416700065135956\n",
            "The 517 batch, training loss is 0.14919789135456085\n",
            "The 518 batch, training loss is 0.23536373674869537\n",
            "The 519 batch, training loss is 0.11634541302919388\n",
            "The 520 batch, training loss is 0.08009408414363861\n",
            "The 521 batch, training loss is 0.14077486097812653\n",
            "The 522 batch, training loss is 0.041006892919540405\n",
            "The 523 batch, training loss is 0.1059301570057869\n",
            "The 524 batch, training loss is 0.20767395198345184\n",
            "The 525 batch, training loss is 0.1271561235189438\n",
            "The 526 batch, training loss is 0.1032932847738266\n",
            "The 527 batch, training loss is 0.11359138041734695\n",
            "The 528 batch, training loss is 0.1220903992652893\n",
            "The 529 batch, training loss is 0.13853134214878082\n",
            "The 530 batch, training loss is 0.22560754418373108\n",
            "The 531 batch, training loss is 0.1182088777422905\n",
            "The 532 batch, training loss is 0.04993218183517456\n",
            "The 533 batch, training loss is 0.0989302322268486\n",
            "The 534 batch, training loss is 0.31839731335639954\n",
            "The 535 batch, training loss is 0.05962098389863968\n",
            "The 536 batch, training loss is 0.11000190675258636\n",
            "The 537 batch, training loss is 0.0636211484670639\n",
            "The 538 batch, training loss is 0.11517421156167984\n",
            "The 539 batch, training loss is 0.09076043218374252\n",
            "The 540 batch, training loss is 0.16781459748744965\n",
            "The 541 batch, training loss is 0.10314476490020752\n",
            "The 542 batch, training loss is 0.19423142075538635\n",
            "The 543 batch, training loss is 0.110462486743927\n",
            "The 544 batch, training loss is 0.07730661332607269\n",
            "The 545 batch, training loss is 0.18360328674316406\n",
            "The 546 batch, training loss is 0.07351891696453094\n",
            "The 547 batch, training loss is 0.20448444783687592\n",
            "The 548 batch, training loss is 0.1620972752571106\n",
            "The 549 batch, training loss is 0.1300486922264099\n",
            "The 550 batch, training loss is 0.21924225986003876\n",
            "The 551 batch, training loss is 0.0715794488787651\n",
            "The 552 batch, training loss is 0.14026372134685516\n",
            "The 553 batch, training loss is 0.18822933733463287\n",
            "The 554 batch, training loss is 0.16813744604587555\n",
            "The 555 batch, training loss is 0.19933556020259857\n",
            "The 556 batch, training loss is 0.2381027191877365\n",
            "The 557 batch, training loss is 0.1406964659690857\n",
            "The 558 batch, training loss is 0.0949777290225029\n",
            "The 559 batch, training loss is 0.10312306135892868\n",
            "The 560 batch, training loss is 0.08251973986625671\n",
            "The 561 batch, training loss is 0.23676198720932007\n",
            "The 562 batch, training loss is 0.10741133242845535\n",
            "The 563 batch, training loss is 0.2223825305700302\n",
            "The 564 batch, training loss is 0.09213054180145264\n",
            "The 565 batch, training loss is 0.056280191987752914\n",
            "The 566 batch, training loss is 0.15254805982112885\n",
            "The 567 batch, training loss is 0.05102364346385002\n",
            "The 568 batch, training loss is 0.0493304468691349\n",
            "The 569 batch, training loss is 0.13326334953308105\n",
            "The 570 batch, training loss is 0.2516031563282013\n",
            "The 571 batch, training loss is 0.06261134892702103\n",
            "The 572 batch, training loss is 0.09746898710727692\n",
            "The 573 batch, training loss is 0.16515657305717468\n",
            "The 574 batch, training loss is 0.28535810112953186\n",
            "The 575 batch, training loss is 0.04040123522281647\n",
            "The 576 batch, training loss is 0.07307901233434677\n",
            "The 577 batch, training loss is 0.1499880701303482\n",
            "The 578 batch, training loss is 0.28659436106681824\n",
            "The 579 batch, training loss is 0.06574156135320663\n",
            "The 580 batch, training loss is 0.11800514906644821\n",
            "The 581 batch, training loss is 0.1957128942012787\n",
            "The 582 batch, training loss is 0.12050101161003113\n",
            "The 583 batch, training loss is 0.1580924689769745\n",
            "The 584 batch, training loss is 0.218862846493721\n",
            "The 585 batch, training loss is 0.14370286464691162\n",
            "The 586 batch, training loss is 0.07698764652013779\n",
            "The 587 batch, training loss is 0.10775179415941238\n",
            "The 588 batch, training loss is 0.10357224196195602\n",
            "The 589 batch, training loss is 0.12681832909584045\n",
            "The 590 batch, training loss is 0.09557260572910309\n",
            "The 591 batch, training loss is 0.5112836956977844\n",
            "The 592 batch, training loss is 0.10229820758104324\n",
            "The 593 batch, training loss is 0.1571272760629654\n",
            "The 594 batch, training loss is 0.1116584837436676\n",
            "The 595 batch, training loss is 0.11506788432598114\n",
            "The 596 batch, training loss is 0.07722043246030807\n",
            "The 597 batch, training loss is 0.11533094942569733\n",
            "The 598 batch, training loss is 0.10196500271558762\n",
            "The 599 batch, training loss is 0.09775030612945557\n",
            "The 600 batch, training loss is 0.06976697593927383\n",
            "The 601 batch, training loss is 0.10931447148323059\n",
            "The 602 batch, training loss is 0.07505468279123306\n",
            "The 603 batch, training loss is 0.09327679127454758\n",
            "The 604 batch, training loss is 0.13864077627658844\n",
            "The 605 batch, training loss is 0.07637243717908859\n",
            "The 606 batch, training loss is 0.10276060551404953\n",
            "The 607 batch, training loss is 0.10166411846876144\n",
            "The 608 batch, training loss is 0.11207649856805801\n",
            "The 609 batch, training loss is 0.18857046961784363\n",
            "The 610 batch, training loss is 0.12838205695152283\n",
            "The 611 batch, training loss is 0.23316650092601776\n",
            "The 612 batch, training loss is 0.18534696102142334\n",
            "The 613 batch, training loss is 0.05003604292869568\n",
            "The 614 batch, training loss is 0.05604676902294159\n",
            "The 615 batch, training loss is 0.15885624289512634\n",
            "The 616 batch, training loss is 0.15318171679973602\n",
            "The 617 batch, training loss is 0.08016067743301392\n",
            "The 618 batch, training loss is 0.1208139955997467\n",
            "The 619 batch, training loss is 0.21966446936130524\n",
            "The 620 batch, training loss is 0.06332755088806152\n",
            "The 621 batch, training loss is 0.08236993849277496\n",
            "The 622 batch, training loss is 0.09934870153665543\n",
            "The 623 batch, training loss is 0.046480678021907806\n",
            "The 624 batch, training loss is 0.06327580660581589\n",
            "The 625 batch, training loss is 0.037859272211790085\n",
            "The 626 batch, training loss is 0.13158637285232544\n",
            "The 627 batch, training loss is 0.05662669986486435\n",
            "The 628 batch, training loss is 0.18192267417907715\n",
            "The 629 batch, training loss is 0.1787935346364975\n",
            "The 630 batch, training loss is 0.1104440838098526\n",
            "The 631 batch, training loss is 0.15862447023391724\n",
            "The 632 batch, training loss is 0.0720217376947403\n",
            "The 633 batch, training loss is 0.09000205993652344\n",
            "The 634 batch, training loss is 0.1005164235830307\n",
            "The 635 batch, training loss is 0.1560547947883606\n",
            "The 636 batch, training loss is 0.12135796248912811\n",
            "The 637 batch, training loss is 0.14542454481124878\n",
            "The 638 batch, training loss is 0.09312667697668076\n",
            "The 639 batch, training loss is 0.18959136307239532\n",
            "The 640 batch, training loss is 0.15565046668052673\n",
            "The 641 batch, training loss is 0.08689570426940918\n",
            "The 642 batch, training loss is 0.128780797123909\n",
            "The 643 batch, training loss is 0.15087473392486572\n",
            "The 644 batch, training loss is 0.1703559160232544\n",
            "The 645 batch, training loss is 0.11970344185829163\n",
            "The 646 batch, training loss is 0.10810128599405289\n",
            "The 647 batch, training loss is 0.23427444696426392\n",
            "The 648 batch, training loss is 0.05531233176589012\n",
            "The 649 batch, training loss is 0.10351988673210144\n",
            "The 650 batch, training loss is 0.14786647260189056\n",
            "The 651 batch, training loss is 0.24492885172367096\n",
            "The 652 batch, training loss is 0.09372454881668091\n",
            "The 653 batch, training loss is 0.09751809388399124\n",
            "The 654 batch, training loss is 0.0879087969660759\n",
            "The 655 batch, training loss is 0.10048539936542511\n",
            "The 656 batch, training loss is 0.07164499163627625\n",
            "The 657 batch, training loss is 0.13588140904903412\n",
            "The 658 batch, training loss is 0.13131183385849\n",
            "The 659 batch, training loss is 0.08897064626216888\n",
            "The 660 batch, training loss is 0.09413459897041321\n",
            "The 661 batch, training loss is 0.2320135235786438\n",
            "The 662 batch, training loss is 0.10666387528181076\n",
            "The 663 batch, training loss is 0.05936980992555618\n",
            "The 664 batch, training loss is 0.041905857622623444\n",
            "The 665 batch, training loss is 0.06497197598218918\n",
            "The 666 batch, training loss is 0.24984660744667053\n",
            "The 667 batch, training loss is 0.0596095472574234\n",
            "The 668 batch, training loss is 0.25144487619400024\n",
            "The 669 batch, training loss is 0.17488306760787964\n",
            "The 670 batch, training loss is 0.1774061769247055\n",
            "The 671 batch, training loss is 0.12533342838287354\n",
            "The 672 batch, training loss is 0.029299583286046982\n",
            "The 673 batch, training loss is 0.11737281084060669\n",
            "The 674 batch, training loss is 0.1388881355524063\n",
            "The 675 batch, training loss is 0.0677291750907898\n",
            "The 676 batch, training loss is 0.03789129853248596\n",
            "The 677 batch, training loss is 0.19167914986610413\n",
            "The 678 batch, training loss is 0.1272679567337036\n",
            "The 679 batch, training loss is 0.08052730560302734\n",
            "The 680 batch, training loss is 0.29243892431259155\n",
            "The 681 batch, training loss is 0.15109290182590485\n",
            "The 682 batch, training loss is 0.2915278375148773\n",
            "The 683 batch, training loss is 0.15603022277355194\n",
            "The 684 batch, training loss is 0.08443684875965118\n",
            "The 685 batch, training loss is 0.10581754148006439\n",
            "The 686 batch, training loss is 0.09967262297868729\n",
            "The 687 batch, training loss is 0.08400214463472366\n",
            "The 688 batch, training loss is 0.11078795045614243\n",
            "The 689 batch, training loss is 0.06410420686006546\n",
            "The 690 batch, training loss is 0.20548023283481598\n",
            "The 691 batch, training loss is 0.11791813373565674\n",
            "The 692 batch, training loss is 0.18947090208530426\n",
            "The 693 batch, training loss is 0.12332768738269806\n",
            "The 694 batch, training loss is 0.13251051306724548\n",
            "The 695 batch, training loss is 0.18861901760101318\n",
            "The 696 batch, training loss is 0.03066154755651951\n",
            "The 697 batch, training loss is 0.11208499222993851\n",
            "The 698 batch, training loss is 0.15290655195713043\n",
            "The 699 batch, training loss is 0.06524061411619186\n",
            "The 700 batch, training loss is 0.11378053575754166\n",
            "The 701 batch, training loss is 0.15475577116012573\n",
            "The 702 batch, training loss is 0.2827382981777191\n",
            "The 703 batch, training loss is 0.08819208294153214\n",
            "The 704 batch, training loss is 0.05142904072999954\n",
            "The 705 batch, training loss is 0.2914188802242279\n",
            "The 706 batch, training loss is 0.28022611141204834\n",
            "The 707 batch, training loss is 0.0970415249466896\n",
            "The 708 batch, training loss is 0.19834403693675995\n",
            "The 709 batch, training loss is 0.16666890680789948\n",
            "The 710 batch, training loss is 0.1622222661972046\n",
            "The 711 batch, training loss is 0.155986025929451\n",
            "The 712 batch, training loss is 0.33703523874282837\n",
            "The 713 batch, training loss is 0.08886070549488068\n",
            "The 714 batch, training loss is 0.13851533830165863\n",
            "The 715 batch, training loss is 0.05422857403755188\n",
            "The 716 batch, training loss is 0.07048030197620392\n",
            "The 717 batch, training loss is 0.18003447353839874\n",
            "The 718 batch, training loss is 0.12045680731534958\n",
            "The 719 batch, training loss is 0.11715973913669586\n",
            "The 720 batch, training loss is 0.27754154801368713\n",
            "The 721 batch, training loss is 0.1576899141073227\n",
            "The 722 batch, training loss is 0.1535710096359253\n",
            "The 723 batch, training loss is 0.1851770281791687\n",
            "The 724 batch, training loss is 0.10575376451015472\n",
            "The 725 batch, training loss is 0.2521399259567261\n",
            "The 726 batch, training loss is 0.13934443891048431\n",
            "The 727 batch, training loss is 0.12196115404367447\n",
            "The 728 batch, training loss is 0.19380304217338562\n",
            "The 729 batch, training loss is 0.057489607483148575\n",
            "The 730 batch, training loss is 0.0863179862499237\n",
            "The 731 batch, training loss is 0.0964096337556839\n",
            "The 732 batch, training loss is 0.17605353891849518\n",
            "The 733 batch, training loss is 0.1750037968158722\n",
            "The 734 batch, training loss is 0.16393736004829407\n",
            "The 735 batch, training loss is 0.12889569997787476\n",
            "The 736 batch, training loss is 0.09866400808095932\n",
            "The 737 batch, training loss is 0.06350640207529068\n",
            "The 738 batch, training loss is 0.07187756896018982\n",
            "The 739 batch, training loss is 0.20775455236434937\n",
            "The 740 batch, training loss is 0.11724939197301865\n",
            "The 741 batch, training loss is 0.09276288002729416\n",
            "The 742 batch, training loss is 0.20369945466518402\n",
            "The 743 batch, training loss is 0.11788713186979294\n",
            "The 744 batch, training loss is 0.09642521291971207\n",
            "The 745 batch, training loss is 0.13842934370040894\n",
            "The 746 batch, training loss is 0.110286183655262\n",
            "The 747 batch, training loss is 0.1104048490524292\n",
            "The 748 batch, training loss is 0.07172208279371262\n",
            "The 749 batch, training loss is 0.06412170827388763\n",
            "The 750 batch, training loss is 0.10079304128885269\n",
            "The 751 batch, training loss is 0.11985968053340912\n",
            "The 752 batch, training loss is 0.11815842241048813\n",
            "The 753 batch, training loss is 0.11785051226615906\n",
            "The 754 batch, training loss is 0.12746405601501465\n",
            "The 755 batch, training loss is 0.31361401081085205\n",
            "The 756 batch, training loss is 0.146853506565094\n",
            "The 757 batch, training loss is 0.12368800491094589\n",
            "The 758 batch, training loss is 0.12174893915653229\n",
            "The 759 batch, training loss is 0.09045848995447159\n",
            "The 760 batch, training loss is 0.1023518517613411\n",
            "The 761 batch, training loss is 0.24250800907611847\n",
            "The 762 batch, training loss is 0.19128799438476562\n",
            "The 763 batch, training loss is 0.11130063980817795\n",
            "The 764 batch, training loss is 0.2559368908405304\n",
            "The 765 batch, training loss is 0.12586089968681335\n",
            "The 766 batch, training loss is 0.13646642863750458\n",
            "The 767 batch, training loss is 0.23287785053253174\n",
            "The 768 batch, training loss is 0.19982382655143738\n",
            "The 769 batch, training loss is 0.12190597504377365\n",
            "The 770 batch, training loss is 0.05963263288140297\n",
            "The 771 batch, training loss is 0.14118853211402893\n",
            "The 772 batch, training loss is 0.15621690452098846\n",
            "The 773 batch, training loss is 0.1165783479809761\n",
            "The 774 batch, training loss is 0.18158861994743347\n",
            "The 775 batch, training loss is 0.10427886247634888\n",
            "The 776 batch, training loss is 0.1768300086259842\n",
            "The 777 batch, training loss is 0.06536884605884552\n",
            "The 778 batch, training loss is 0.12559856474399567\n",
            "The 779 batch, training loss is 0.13120409846305847\n",
            "The 780 batch, training loss is 0.22746595740318298\n",
            "The 781 batch, training loss is 0.2142067551612854\n",
            "The 782 batch, training loss is 0.2073710560798645\n",
            "The 783 batch, training loss is 0.16869507730007172\n",
            "The 784 batch, training loss is 0.08513924479484558\n",
            "The 785 batch, training loss is 0.17020325362682343\n",
            "The 786 batch, training loss is 0.14813074469566345\n",
            "The 787 batch, training loss is 0.19879737496376038\n",
            "The 788 batch, training loss is 0.1044035330414772\n",
            "The 789 batch, training loss is 0.14828358590602875\n",
            "The 790 batch, training loss is 0.17540627717971802\n",
            "The 791 batch, training loss is 0.09741649031639099\n",
            "The 792 batch, training loss is 0.12402630597352982\n",
            "The 793 batch, training loss is 0.09337170422077179\n",
            "The 794 batch, training loss is 0.16629068553447723\n",
            "The 795 batch, training loss is 0.10136006772518158\n",
            "The 796 batch, training loss is 0.1764484941959381\n",
            "The 797 batch, training loss is 0.08968513458967209\n",
            "The 798 batch, training loss is 0.06692205369472504\n",
            "The 799 batch, training loss is 0.05496656894683838\n",
            "The 800 batch, training loss is 0.03697706758975983\n",
            "The 801 batch, training loss is 0.17006568610668182\n",
            "The 802 batch, training loss is 0.08306694775819778\n",
            "The 803 batch, training loss is 0.2471020668745041\n",
            "The 804 batch, training loss is 0.17039138078689575\n",
            "The 805 batch, training loss is 0.07854142040014267\n",
            "The 806 batch, training loss is 0.10703926533460617\n",
            "The 807 batch, training loss is 0.08462224155664444\n",
            "The 808 batch, training loss is 0.20730160176753998\n",
            "The 809 batch, training loss is 0.15714052319526672\n",
            "The 810 batch, training loss is 0.1445622444152832\n",
            "The 811 batch, training loss is 0.08455386757850647\n",
            "The 812 batch, training loss is 0.08059737086296082\n",
            "The 813 batch, training loss is 0.1916399896144867\n",
            "The 814 batch, training loss is 0.1546597182750702\n",
            "The 815 batch, training loss is 0.11786967515945435\n",
            "The 816 batch, training loss is 0.19494237005710602\n",
            "The 817 batch, training loss is 0.12224742770195007\n",
            "The 818 batch, training loss is 0.0998648852109909\n",
            "The 819 batch, training loss is 0.10838785022497177\n",
            "The 820 batch, training loss is 0.07462354749441147\n",
            "The 821 batch, training loss is 0.1780652403831482\n",
            "The 822 batch, training loss is 0.1815950572490692\n",
            "The 823 batch, training loss is 0.09872828423976898\n",
            "The 824 batch, training loss is 0.19855695962905884\n",
            "The 825 batch, training loss is 0.0849175676703453\n",
            "The 826 batch, training loss is 0.07841803133487701\n",
            "The 827 batch, training loss is 0.17518596351146698\n",
            "The 828 batch, training loss is 0.10539565980434418\n",
            "The 829 batch, training loss is 0.09469691663980484\n",
            "The 830 batch, training loss is 0.11452855914831161\n",
            "The 831 batch, training loss is 0.09737280011177063\n",
            "The 832 batch, training loss is 0.2136879861354828\n",
            "The 833 batch, training loss is 0.1023963913321495\n",
            "The 834 batch, training loss is 0.08694052696228027\n",
            "The 835 batch, training loss is 0.3513053059577942\n",
            "The 836 batch, training loss is 0.1919700652360916\n",
            "The 837 batch, training loss is 0.14189785718917847\n",
            "The 838 batch, training loss is 0.09809467941522598\n",
            "The 839 batch, training loss is 0.12429913133382797\n",
            "The 840 batch, training loss is 0.018532205373048782\n",
            "The 841 batch, training loss is 0.0973164513707161\n",
            "The 842 batch, training loss is 0.09353917092084885\n",
            "The 843 batch, training loss is 0.12336264550685883\n",
            "The 844 batch, training loss is 0.08430597186088562\n",
            "The 845 batch, training loss is 0.12647230923175812\n",
            "The 846 batch, training loss is 0.14145833253860474\n",
            "The 847 batch, training loss is 0.12568050622940063\n",
            "The 848 batch, training loss is 0.20485112071037292\n",
            "The 849 batch, training loss is 0.14984333515167236\n",
            "The 850 batch, training loss is 0.17736269533634186\n",
            "The 851 batch, training loss is 0.06774482131004333\n",
            "The 852 batch, training loss is 0.08592477440834045\n",
            "The 853 batch, training loss is 0.13798099756240845\n",
            "The 854 batch, training loss is 0.055932749062776566\n",
            "The 855 batch, training loss is 0.1876024752855301\n",
            "The 856 batch, training loss is 0.11449331045150757\n",
            "The 857 batch, training loss is 0.1008043885231018\n",
            "The 858 batch, training loss is 0.16283081471920013\n",
            "The 859 batch, training loss is 0.06999216228723526\n",
            "The 860 batch, training loss is 0.10788832604885101\n",
            "The 861 batch, training loss is 0.12939892709255219\n",
            "The 862 batch, training loss is 0.1120537668466568\n",
            "The 863 batch, training loss is 0.23846378922462463\n",
            "The 864 batch, training loss is 0.05884581431746483\n",
            "The 865 batch, training loss is 0.06286785006523132\n",
            "The 866 batch, training loss is 0.09378905594348907\n",
            "The 867 batch, training loss is 0.09795190393924713\n",
            "The 868 batch, training loss is 0.10301525890827179\n",
            "The 869 batch, training loss is 0.15021292865276337\n",
            "The 870 batch, training loss is 0.14071610569953918\n",
            "The 871 batch, training loss is 0.3582475185394287\n",
            "The 872 batch, training loss is 0.24967220425605774\n",
            "The 873 batch, training loss is 0.06207921728491783\n",
            "The 874 batch, training loss is 0.0901188850402832\n",
            "The 875 batch, training loss is 0.07977970689535141\n",
            "The 876 batch, training loss is 0.12223497033119202\n",
            "The 877 batch, training loss is 0.16745802760124207\n",
            "The 878 batch, training loss is 0.06427691131830215\n",
            "The 879 batch, training loss is 0.15647736191749573\n",
            "The 880 batch, training loss is 0.10786283761262894\n",
            "The 881 batch, training loss is 0.10366185009479523\n",
            "The 882 batch, training loss is 0.27386799454689026\n",
            "The 883 batch, training loss is 0.09538164734840393\n",
            "The 884 batch, training loss is 0.06949658691883087\n",
            "The 885 batch, training loss is 0.23213914036750793\n",
            "The 886 batch, training loss is 0.04304736107587814\n",
            "The 887 batch, training loss is 0.15448592603206635\n",
            "The 888 batch, training loss is 0.04975517466664314\n",
            "The 889 batch, training loss is 0.06775927543640137\n",
            "The 890 batch, training loss is 0.06707707047462463\n",
            "The 891 batch, training loss is 0.052887193858623505\n",
            "The 892 batch, training loss is 0.19466722011566162\n",
            "The 893 batch, training loss is 0.15595725178718567\n",
            "The 894 batch, training loss is 0.13531696796417236\n",
            "The 895 batch, training loss is 0.07413699477910995\n",
            "The 896 batch, training loss is 0.10346143692731857\n",
            "The 897 batch, training loss is 0.12830398976802826\n",
            "The 898 batch, training loss is 0.279861718416214\n",
            "The 899 batch, training loss is 0.1322629600763321\n",
            "The 900 batch, training loss is 0.27353909611701965\n",
            "The 901 batch, training loss is 0.11970305442810059\n",
            "The 902 batch, training loss is 0.13000763952732086\n",
            "The 903 batch, training loss is 0.14451272785663605\n",
            "The 904 batch, training loss is 0.2545958161354065\n",
            "The 905 batch, training loss is 0.12709610164165497\n",
            "The 906 batch, training loss is 0.10428726673126221\n",
            "The 907 batch, training loss is 0.15356992185115814\n",
            "The 908 batch, training loss is 0.25112590193748474\n",
            "The 909 batch, training loss is 0.21184830367565155\n",
            "The 910 batch, training loss is 0.08256855607032776\n",
            "The 911 batch, training loss is 0.24132686853408813\n",
            "The 912 batch, training loss is 0.12604062259197235\n",
            "The 913 batch, training loss is 0.09357894212007523\n",
            "The 914 batch, training loss is 0.08433030545711517\n",
            "The 915 batch, training loss is 0.0779949277639389\n",
            "The 916 batch, training loss is 0.1333802491426468\n",
            "The 917 batch, training loss is 0.09799342602491379\n",
            "The 918 batch, training loss is 0.1732337325811386\n",
            "The 919 batch, training loss is 0.17585591971874237\n",
            "The 920 batch, training loss is 0.15219947695732117\n",
            "The 921 batch, training loss is 0.08943621069192886\n",
            "The 922 batch, training loss is 0.13335378468036652\n",
            "The 923 batch, training loss is 0.21080291271209717\n",
            "The 924 batch, training loss is 0.051699601113796234\n",
            "The 925 batch, training loss is 0.10559577494859695\n",
            "The 926 batch, training loss is 0.06332522630691528\n",
            "The 927 batch, training loss is 0.20849157869815826\n",
            "The 928 batch, training loss is 0.14699454605579376\n",
            "The 929 batch, training loss is 0.09276992827653885\n",
            "The 930 batch, training loss is 0.10792258381843567\n",
            "The 931 batch, training loss is 0.09992469847202301\n",
            "The 932 batch, training loss is 0.1811555176973343\n",
            "The 933 batch, training loss is 0.08542313426733017\n",
            "The 934 batch, training loss is 0.08224643021821976\n",
            "The 935 batch, training loss is 0.09341718256473541\n",
            "The 936 batch, training loss is 0.17441849410533905\n",
            "The 937 batch, training loss is 0.032354652881622314\n",
            "The 8 epoch, training loss is 0.032354652881622314\n",
            "The 0 batch, training loss is 0.17332996428012848\n",
            "The 1 batch, training loss is 0.11592855304479599\n",
            "The 2 batch, training loss is 0.075973741710186\n",
            "The 3 batch, training loss is 0.09309594333171844\n",
            "The 4 batch, training loss is 0.08947541564702988\n",
            "The 5 batch, training loss is 0.11621364206075668\n",
            "The 6 batch, training loss is 0.07651088386774063\n",
            "The 7 batch, training loss is 0.1509188711643219\n",
            "The 8 batch, training loss is 0.0753760114312172\n",
            "The 9 batch, training loss is 0.15573139488697052\n",
            "The 10 batch, training loss is 0.10515391826629639\n",
            "The 11 batch, training loss is 0.18177422881126404\n",
            "The 12 batch, training loss is 0.19324639439582825\n",
            "The 13 batch, training loss is 0.09798498451709747\n",
            "The 14 batch, training loss is 0.08152413368225098\n",
            "The 15 batch, training loss is 0.19706426560878754\n",
            "The 16 batch, training loss is 0.10196929425001144\n",
            "The 17 batch, training loss is 0.2366846650838852\n",
            "The 18 batch, training loss is 0.09119319915771484\n",
            "The 19 batch, training loss is 0.1583985537290573\n",
            "The 20 batch, training loss is 0.1516149342060089\n",
            "The 21 batch, training loss is 0.13923470675945282\n",
            "The 22 batch, training loss is 0.20258326828479767\n",
            "The 23 batch, training loss is 0.04716718569397926\n",
            "The 24 batch, training loss is 0.08819276839494705\n",
            "The 25 batch, training loss is 0.12305083870887756\n",
            "The 26 batch, training loss is 0.07663805037736893\n",
            "The 27 batch, training loss is 0.11074036359786987\n",
            "The 28 batch, training loss is 0.12718258798122406\n",
            "The 29 batch, training loss is 0.09924358874559402\n",
            "The 30 batch, training loss is 0.08851028978824615\n",
            "The 31 batch, training loss is 0.055944133549928665\n",
            "The 32 batch, training loss is 0.08277115225791931\n",
            "The 33 batch, training loss is 0.11323405057191849\n",
            "The 34 batch, training loss is 0.08969955891370773\n",
            "The 35 batch, training loss is 0.21630549430847168\n",
            "The 36 batch, training loss is 0.1654357761144638\n",
            "The 37 batch, training loss is 0.21295501291751862\n",
            "The 38 batch, training loss is 0.16545641422271729\n",
            "The 39 batch, training loss is 0.16260334849357605\n",
            "The 40 batch, training loss is 0.09462547302246094\n",
            "The 41 batch, training loss is 0.06978572905063629\n",
            "The 42 batch, training loss is 0.2561968266963959\n",
            "The 43 batch, training loss is 0.1327114701271057\n",
            "The 44 batch, training loss is 0.1992376297712326\n",
            "The 45 batch, training loss is 0.189900740981102\n",
            "The 46 batch, training loss is 0.08602691441774368\n",
            "The 47 batch, training loss is 0.0804944708943367\n",
            "The 48 batch, training loss is 0.13578464090824127\n",
            "The 49 batch, training loss is 0.11458595097064972\n",
            "The 50 batch, training loss is 0.08606100082397461\n",
            "The 51 batch, training loss is 0.11252032965421677\n",
            "The 52 batch, training loss is 0.13652417063713074\n",
            "The 53 batch, training loss is 0.12918253242969513\n",
            "The 54 batch, training loss is 0.2840757966041565\n",
            "The 55 batch, training loss is 0.12640829384326935\n",
            "The 56 batch, training loss is 0.1587621122598648\n",
            "The 57 batch, training loss is 0.26433712244033813\n",
            "The 58 batch, training loss is 0.1583782136440277\n",
            "The 59 batch, training loss is 0.15629149973392487\n",
            "The 60 batch, training loss is 0.15310324728488922\n",
            "The 61 batch, training loss is 0.20584441721439362\n",
            "The 62 batch, training loss is 0.09517209231853485\n",
            "The 63 batch, training loss is 0.07870949059724808\n",
            "The 64 batch, training loss is 0.0889384001493454\n",
            "The 65 batch, training loss is 0.1345810890197754\n",
            "The 66 batch, training loss is 0.10638972371816635\n",
            "The 67 batch, training loss is 0.07747246325016022\n",
            "The 68 batch, training loss is 0.08435157686471939\n",
            "The 69 batch, training loss is 0.12307219207286835\n",
            "The 70 batch, training loss is 0.09446411579847336\n",
            "The 71 batch, training loss is 0.30517372488975525\n",
            "The 72 batch, training loss is 0.15985043346881866\n",
            "The 73 batch, training loss is 0.14102642238140106\n",
            "The 74 batch, training loss is 0.23102325201034546\n",
            "The 75 batch, training loss is 0.19817385077476501\n",
            "The 76 batch, training loss is 0.13417112827301025\n",
            "The 77 batch, training loss is 0.12890520691871643\n",
            "The 78 batch, training loss is 0.17002396285533905\n",
            "The 79 batch, training loss is 0.16471709311008453\n",
            "The 80 batch, training loss is 0.09181466698646545\n",
            "The 81 batch, training loss is 0.19172172248363495\n",
            "The 82 batch, training loss is 0.12127652019262314\n",
            "The 83 batch, training loss is 0.19674745202064514\n",
            "The 84 batch, training loss is 0.07634900510311127\n",
            "The 85 batch, training loss is 0.10417746752500534\n",
            "The 86 batch, training loss is 0.06333399564027786\n",
            "The 87 batch, training loss is 0.10592922568321228\n",
            "The 88 batch, training loss is 0.09066526591777802\n",
            "The 89 batch, training loss is 0.18327201902866364\n",
            "The 90 batch, training loss is 0.06373874098062515\n",
            "The 91 batch, training loss is 0.20754535496234894\n",
            "The 92 batch, training loss is 0.07415757328271866\n",
            "The 93 batch, training loss is 0.15638750791549683\n",
            "The 94 batch, training loss is 0.20044302940368652\n",
            "The 95 batch, training loss is 0.12455730885267258\n",
            "The 96 batch, training loss is 0.046552568674087524\n",
            "The 97 batch, training loss is 0.09199749678373337\n",
            "The 98 batch, training loss is 0.2211330085992813\n",
            "The 99 batch, training loss is 0.11829372495412827\n",
            "The 100 batch, training loss is 0.05499887838959694\n",
            "The 101 batch, training loss is 0.11150062829256058\n",
            "The 102 batch, training loss is 0.07469133287668228\n",
            "The 103 batch, training loss is 0.07449325174093246\n",
            "The 104 batch, training loss is 0.13350103795528412\n",
            "The 105 batch, training loss is 0.12527231872081757\n",
            "The 106 batch, training loss is 0.24134749174118042\n",
            "The 107 batch, training loss is 0.07411825656890869\n",
            "The 108 batch, training loss is 0.1393684446811676\n",
            "The 109 batch, training loss is 0.3372240960597992\n",
            "The 110 batch, training loss is 0.16780366003513336\n",
            "The 111 batch, training loss is 0.1522080898284912\n",
            "The 112 batch, training loss is 0.11601085960865021\n",
            "The 113 batch, training loss is 0.12017348408699036\n",
            "The 114 batch, training loss is 0.07286945730447769\n",
            "The 115 batch, training loss is 0.10937296599149704\n",
            "The 116 batch, training loss is 0.1740606278181076\n",
            "The 117 batch, training loss is 0.1699192374944687\n",
            "The 118 batch, training loss is 0.1554100066423416\n",
            "The 119 batch, training loss is 0.1660957634449005\n",
            "The 120 batch, training loss is 0.14983375370502472\n",
            "The 121 batch, training loss is 0.12069426476955414\n",
            "The 122 batch, training loss is 0.10089761763811111\n",
            "The 123 batch, training loss is 0.09991913288831711\n",
            "The 124 batch, training loss is 0.11858132481575012\n",
            "The 125 batch, training loss is 0.26865696907043457\n",
            "The 126 batch, training loss is 0.13726793229579926\n",
            "The 127 batch, training loss is 0.07065810263156891\n",
            "The 128 batch, training loss is 0.17914344370365143\n",
            "The 129 batch, training loss is 0.19743949174880981\n",
            "The 130 batch, training loss is 0.061784449964761734\n",
            "The 131 batch, training loss is 0.04406912624835968\n",
            "The 132 batch, training loss is 0.10112477838993073\n",
            "The 133 batch, training loss is 0.21067087352275848\n",
            "The 134 batch, training loss is 0.1897420585155487\n",
            "The 135 batch, training loss is 0.06907010823488235\n",
            "The 136 batch, training loss is 0.11041843891143799\n",
            "The 137 batch, training loss is 0.18899978697299957\n",
            "The 138 batch, training loss is 0.06631079316139221\n",
            "The 139 batch, training loss is 0.06492685526609421\n",
            "The 140 batch, training loss is 0.060619156807661057\n",
            "The 141 batch, training loss is 0.04901602864265442\n",
            "The 142 batch, training loss is 0.14550398290157318\n",
            "The 143 batch, training loss is 0.08737760782241821\n",
            "The 144 batch, training loss is 0.06384699791669846\n",
            "The 145 batch, training loss is 0.16110309958457947\n",
            "The 146 batch, training loss is 0.05677098408341408\n",
            "The 147 batch, training loss is 0.05329889804124832\n",
            "The 148 batch, training loss is 0.2256062626838684\n",
            "The 149 batch, training loss is 0.09680396318435669\n",
            "The 150 batch, training loss is 0.08065204322338104\n",
            "The 151 batch, training loss is 0.12806111574172974\n",
            "The 152 batch, training loss is 0.04652087762951851\n",
            "The 153 batch, training loss is 0.16213743388652802\n",
            "The 154 batch, training loss is 0.131075918674469\n",
            "The 155 batch, training loss is 0.20670640468597412\n",
            "The 156 batch, training loss is 0.08574296534061432\n",
            "The 157 batch, training loss is 0.10601184517145157\n",
            "The 158 batch, training loss is 0.17107461392879486\n",
            "The 159 batch, training loss is 0.1178775355219841\n",
            "The 160 batch, training loss is 0.060979872941970825\n",
            "The 161 batch, training loss is 0.09733422100543976\n",
            "The 162 batch, training loss is 0.13850516080856323\n",
            "The 163 batch, training loss is 0.11400418728590012\n",
            "The 164 batch, training loss is 0.06277799606323242\n",
            "The 165 batch, training loss is 0.05555438622832298\n",
            "The 166 batch, training loss is 0.10096980631351471\n",
            "The 167 batch, training loss is 0.08768074214458466\n",
            "The 168 batch, training loss is 0.07826061546802521\n",
            "The 169 batch, training loss is 0.1401435285806656\n",
            "The 170 batch, training loss is 0.1325121372938156\n",
            "The 171 batch, training loss is 0.09456855803728104\n",
            "The 172 batch, training loss is 0.16313649713993073\n",
            "The 173 batch, training loss is 0.13237427175045013\n",
            "The 174 batch, training loss is 0.04736601561307907\n",
            "The 175 batch, training loss is 0.0283241905272007\n",
            "The 176 batch, training loss is 0.15530379116535187\n",
            "The 177 batch, training loss is 0.15160997211933136\n",
            "The 178 batch, training loss is 0.22787077724933624\n",
            "The 179 batch, training loss is 0.09117309749126434\n",
            "The 180 batch, training loss is 0.22480927407741547\n",
            "The 181 batch, training loss is 0.1404554396867752\n",
            "The 182 batch, training loss is 0.07558468729257584\n",
            "The 183 batch, training loss is 0.21892149746418\n",
            "The 184 batch, training loss is 0.13279125094413757\n",
            "The 185 batch, training loss is 0.05048142746090889\n",
            "The 186 batch, training loss is 0.16394691169261932\n",
            "The 187 batch, training loss is 0.09998928755521774\n",
            "The 188 batch, training loss is 0.10008501261472702\n",
            "The 189 batch, training loss is 0.10891875624656677\n",
            "The 190 batch, training loss is 0.08068852126598358\n",
            "The 191 batch, training loss is 0.11856435984373093\n",
            "The 192 batch, training loss is 0.12346427142620087\n",
            "The 193 batch, training loss is 0.16860248148441315\n",
            "The 194 batch, training loss is 0.037502262741327286\n",
            "The 195 batch, training loss is 0.15258461236953735\n",
            "The 196 batch, training loss is 0.13189402222633362\n",
            "The 197 batch, training loss is 0.1329532265663147\n",
            "The 198 batch, training loss is 0.12441013008356094\n",
            "The 199 batch, training loss is 0.2538473904132843\n",
            "The 200 batch, training loss is 0.13542625308036804\n",
            "The 201 batch, training loss is 0.08754858374595642\n",
            "The 202 batch, training loss is 0.16446571052074432\n",
            "The 203 batch, training loss is 0.13663147389888763\n",
            "The 204 batch, training loss is 0.08167994022369385\n",
            "The 205 batch, training loss is 0.12614043056964874\n",
            "The 206 batch, training loss is 0.09983397275209427\n",
            "The 207 batch, training loss is 0.0786098837852478\n",
            "The 208 batch, training loss is 0.06808032840490341\n",
            "The 209 batch, training loss is 0.2478589564561844\n",
            "The 210 batch, training loss is 0.20308277010917664\n",
            "The 211 batch, training loss is 0.10088513046503067\n",
            "The 212 batch, training loss is 0.1670408844947815\n",
            "The 213 batch, training loss is 0.07728300988674164\n",
            "The 214 batch, training loss is 0.09265022724866867\n",
            "The 215 batch, training loss is 0.08308784663677216\n",
            "The 216 batch, training loss is 0.17351271212100983\n",
            "The 217 batch, training loss is 0.1122790202498436\n",
            "The 218 batch, training loss is 0.13621056079864502\n",
            "The 219 batch, training loss is 0.13223469257354736\n",
            "The 220 batch, training loss is 0.09200551360845566\n",
            "The 221 batch, training loss is 0.08167850226163864\n",
            "The 222 batch, training loss is 0.19858665764331818\n",
            "The 223 batch, training loss is 0.11518939584493637\n",
            "The 224 batch, training loss is 0.07011923938989639\n",
            "The 225 batch, training loss is 0.1394384205341339\n",
            "The 226 batch, training loss is 0.03978151082992554\n",
            "The 227 batch, training loss is 0.09077554941177368\n",
            "The 228 batch, training loss is 0.06572631746530533\n",
            "The 229 batch, training loss is 0.1752394437789917\n",
            "The 230 batch, training loss is 0.11834688484668732\n",
            "The 231 batch, training loss is 0.11304757744073868\n",
            "The 232 batch, training loss is 0.13478335738182068\n",
            "The 233 batch, training loss is 0.0858878642320633\n",
            "The 234 batch, training loss is 0.2277495563030243\n",
            "The 235 batch, training loss is 0.1239599660038948\n",
            "The 236 batch, training loss is 0.11273200064897537\n",
            "The 237 batch, training loss is 0.14003264904022217\n",
            "The 238 batch, training loss is 0.1236623153090477\n",
            "The 239 batch, training loss is 0.06098977476358414\n",
            "The 240 batch, training loss is 0.055665574967861176\n",
            "The 241 batch, training loss is 0.17554321885108948\n",
            "The 242 batch, training loss is 0.1403355449438095\n",
            "The 243 batch, training loss is 0.10453978925943375\n",
            "The 244 batch, training loss is 0.19169723987579346\n",
            "The 245 batch, training loss is 0.08665512502193451\n",
            "The 246 batch, training loss is 0.14296531677246094\n",
            "The 247 batch, training loss is 0.24255219101905823\n",
            "The 248 batch, training loss is 0.04852208122611046\n",
            "The 249 batch, training loss is 0.17752327024936676\n",
            "The 250 batch, training loss is 0.1080724224448204\n",
            "The 251 batch, training loss is 0.0408109650015831\n",
            "The 252 batch, training loss is 0.058178722858428955\n",
            "The 253 batch, training loss is 0.1165250763297081\n",
            "The 254 batch, training loss is 0.13203929364681244\n",
            "The 255 batch, training loss is 0.1005348265171051\n",
            "The 256 batch, training loss is 0.04518328607082367\n",
            "The 257 batch, training loss is 0.12665580213069916\n",
            "The 258 batch, training loss is 0.11486830562353134\n",
            "The 259 batch, training loss is 0.09942697733640671\n",
            "The 260 batch, training loss is 0.05551709607243538\n",
            "The 261 batch, training loss is 0.07988286018371582\n",
            "The 262 batch, training loss is 0.1494172066450119\n",
            "The 263 batch, training loss is 0.09628910571336746\n",
            "The 264 batch, training loss is 0.11053259670734406\n",
            "The 265 batch, training loss is 0.1520390659570694\n",
            "The 266 batch, training loss is 0.10300938785076141\n",
            "The 267 batch, training loss is 0.11114264279603958\n",
            "The 268 batch, training loss is 0.24230045080184937\n",
            "The 269 batch, training loss is 0.1599365621805191\n",
            "The 270 batch, training loss is 0.20034290850162506\n",
            "The 271 batch, training loss is 0.1511419713497162\n",
            "The 272 batch, training loss is 0.2599920928478241\n",
            "The 273 batch, training loss is 0.08938141912221909\n",
            "The 274 batch, training loss is 0.06292811781167984\n",
            "The 275 batch, training loss is 0.11923343688249588\n",
            "The 276 batch, training loss is 0.11272028833627701\n",
            "The 277 batch, training loss is 0.41647449135780334\n",
            "The 278 batch, training loss is 0.04126541316509247\n",
            "The 279 batch, training loss is 0.1333201825618744\n",
            "The 280 batch, training loss is 0.21745561063289642\n",
            "The 281 batch, training loss is 0.05390257015824318\n",
            "The 282 batch, training loss is 0.05188116803765297\n",
            "The 283 batch, training loss is 0.22817565500736237\n",
            "The 284 batch, training loss is 0.13594567775726318\n",
            "The 285 batch, training loss is 0.10222110897302628\n",
            "The 286 batch, training loss is 0.15243710577487946\n",
            "The 287 batch, training loss is 0.16407491266727448\n",
            "The 288 batch, training loss is 0.054487742483615875\n",
            "The 289 batch, training loss is 0.07666070759296417\n",
            "The 290 batch, training loss is 0.07560724020004272\n",
            "The 291 batch, training loss is 0.1500001847743988\n",
            "The 292 batch, training loss is 0.062207549810409546\n",
            "The 293 batch, training loss is 0.07202348113059998\n",
            "The 294 batch, training loss is 0.03686904534697533\n",
            "The 295 batch, training loss is 0.23179501295089722\n",
            "The 296 batch, training loss is 0.10026218742132187\n",
            "The 297 batch, training loss is 0.1394033133983612\n",
            "The 298 batch, training loss is 0.1367182582616806\n",
            "The 299 batch, training loss is 0.06704499572515488\n",
            "The 300 batch, training loss is 0.10779105126857758\n",
            "The 301 batch, training loss is 0.03836581110954285\n",
            "The 302 batch, training loss is 0.07508455216884613\n",
            "The 303 batch, training loss is 0.07903485745191574\n",
            "The 304 batch, training loss is 0.10362695902585983\n",
            "The 305 batch, training loss is 0.175546795129776\n",
            "The 306 batch, training loss is 0.2124805450439453\n",
            "The 307 batch, training loss is 0.14287641644477844\n",
            "The 308 batch, training loss is 0.2732936143875122\n",
            "The 309 batch, training loss is 0.046594005078077316\n",
            "The 310 batch, training loss is 0.09118738770484924\n",
            "The 311 batch, training loss is 0.08051815629005432\n",
            "The 312 batch, training loss is 0.10134638100862503\n",
            "The 313 batch, training loss is 0.24787521362304688\n",
            "The 314 batch, training loss is 0.08292065560817719\n",
            "The 315 batch, training loss is 0.09403645992279053\n",
            "The 316 batch, training loss is 0.08095113188028336\n",
            "The 317 batch, training loss is 0.05990293622016907\n",
            "The 318 batch, training loss is 0.06480152159929276\n",
            "The 319 batch, training loss is 0.13769274950027466\n",
            "The 320 batch, training loss is 0.15515035390853882\n",
            "The 321 batch, training loss is 0.09974030405282974\n",
            "The 322 batch, training loss is 0.05674879625439644\n",
            "The 323 batch, training loss is 0.0652114674448967\n",
            "The 324 batch, training loss is 0.11739490181207657\n",
            "The 325 batch, training loss is 0.10111591219902039\n",
            "The 326 batch, training loss is 0.1393519937992096\n",
            "The 327 batch, training loss is 0.0998646467924118\n",
            "The 328 batch, training loss is 0.21255192160606384\n",
            "The 329 batch, training loss is 0.11125724762678146\n",
            "The 330 batch, training loss is 0.08694229274988174\n",
            "The 331 batch, training loss is 0.2066059410572052\n",
            "The 332 batch, training loss is 0.37291693687438965\n",
            "The 333 batch, training loss is 0.22885121405124664\n",
            "The 334 batch, training loss is 0.060127682983875275\n",
            "The 335 batch, training loss is 0.05134161189198494\n",
            "The 336 batch, training loss is 0.13519707322120667\n",
            "The 337 batch, training loss is 0.06795062869787216\n",
            "The 338 batch, training loss is 0.02904663421213627\n",
            "The 339 batch, training loss is 0.04365208372473717\n",
            "The 340 batch, training loss is 0.14102843403816223\n",
            "The 341 batch, training loss is 0.04960253834724426\n",
            "The 342 batch, training loss is 0.12770628929138184\n",
            "The 343 batch, training loss is 0.07102671265602112\n",
            "The 344 batch, training loss is 0.13796350359916687\n",
            "The 345 batch, training loss is 0.08044423907995224\n",
            "The 346 batch, training loss is 0.254800409078598\n",
            "The 347 batch, training loss is 0.1903969943523407\n",
            "The 348 batch, training loss is 0.1050371378660202\n",
            "The 349 batch, training loss is 0.09928102046251297\n",
            "The 350 batch, training loss is 0.1430494338274002\n",
            "The 351 batch, training loss is 0.07566703855991364\n",
            "The 352 batch, training loss is 0.07321012020111084\n",
            "The 353 batch, training loss is 0.0942191556096077\n",
            "The 354 batch, training loss is 0.1279611736536026\n",
            "The 355 batch, training loss is 0.02365271933376789\n",
            "The 356 batch, training loss is 0.1922839879989624\n",
            "The 357 batch, training loss is 0.06406968086957932\n",
            "The 358 batch, training loss is 0.07738564163446426\n",
            "The 359 batch, training loss is 0.034260790795087814\n",
            "The 360 batch, training loss is 0.23637770116329193\n",
            "The 361 batch, training loss is 0.05101834610104561\n",
            "The 362 batch, training loss is 0.2712550759315491\n",
            "The 363 batch, training loss is 0.1493486613035202\n",
            "The 364 batch, training loss is 0.10924390703439713\n",
            "The 365 batch, training loss is 0.10173710435628891\n",
            "The 366 batch, training loss is 0.2745024561882019\n",
            "The 367 batch, training loss is 0.18715088069438934\n",
            "The 368 batch, training loss is 0.06132284179329872\n",
            "The 369 batch, training loss is 0.07063654810190201\n",
            "The 370 batch, training loss is 0.06794095784425735\n",
            "The 371 batch, training loss is 0.10482092946767807\n",
            "The 372 batch, training loss is 0.1577940732240677\n",
            "The 373 batch, training loss is 0.07098694145679474\n",
            "The 374 batch, training loss is 0.11549253016710281\n",
            "The 375 batch, training loss is 0.05687274411320686\n",
            "The 376 batch, training loss is 0.1556612104177475\n",
            "The 377 batch, training loss is 0.07190918177366257\n",
            "The 378 batch, training loss is 0.24272528290748596\n",
            "The 379 batch, training loss is 0.09433943778276443\n",
            "The 380 batch, training loss is 0.16459426283836365\n",
            "The 381 batch, training loss is 0.15505826473236084\n",
            "The 382 batch, training loss is 0.051402267068624496\n",
            "The 383 batch, training loss is 0.062475673854351044\n",
            "The 384 batch, training loss is 0.16276758909225464\n",
            "The 385 batch, training loss is 0.25622808933258057\n",
            "The 386 batch, training loss is 0.06368701905012131\n",
            "The 387 batch, training loss is 0.05647958442568779\n",
            "The 388 batch, training loss is 0.12406812608242035\n",
            "The 389 batch, training loss is 0.17722190916538239\n",
            "The 390 batch, training loss is 0.05055438354611397\n",
            "The 391 batch, training loss is 0.26537737250328064\n",
            "The 392 batch, training loss is 0.20013126730918884\n",
            "The 393 batch, training loss is 0.15432408452033997\n",
            "The 394 batch, training loss is 0.054676998406648636\n",
            "The 395 batch, training loss is 0.08872512727975845\n",
            "The 396 batch, training loss is 0.07381250709295273\n",
            "The 397 batch, training loss is 0.07867688685655594\n",
            "The 398 batch, training loss is 0.249167799949646\n",
            "The 399 batch, training loss is 0.06514319032430649\n",
            "The 400 batch, training loss is 0.08250153064727783\n",
            "The 401 batch, training loss is 0.1175447329878807\n",
            "The 402 batch, training loss is 0.15146778523921967\n",
            "The 403 batch, training loss is 0.19791465997695923\n",
            "The 404 batch, training loss is 0.08643828332424164\n",
            "The 405 batch, training loss is 0.06981963664293289\n",
            "The 406 batch, training loss is 0.18511618673801422\n",
            "The 407 batch, training loss is 0.1487014889717102\n",
            "The 408 batch, training loss is 0.0513126477599144\n",
            "The 409 batch, training loss is 0.07096754759550095\n",
            "The 410 batch, training loss is 0.17649967968463898\n",
            "The 411 batch, training loss is 0.058720704168081284\n",
            "The 412 batch, training loss is 0.16794869303703308\n",
            "The 413 batch, training loss is 0.11723210662603378\n",
            "The 414 batch, training loss is 0.09122108668088913\n",
            "The 415 batch, training loss is 0.05051882192492485\n",
            "The 416 batch, training loss is 0.08035817742347717\n",
            "The 417 batch, training loss is 0.20277848839759827\n",
            "The 418 batch, training loss is 0.0664161816239357\n",
            "The 419 batch, training loss is 0.08395104110240936\n",
            "The 420 batch, training loss is 0.3522065579891205\n",
            "The 421 batch, training loss is 0.19414760172367096\n",
            "The 422 batch, training loss is 0.05120474472641945\n",
            "The 423 batch, training loss is 0.08966218680143356\n",
            "The 424 batch, training loss is 0.07997822761535645\n",
            "The 425 batch, training loss is 0.0829978883266449\n",
            "The 426 batch, training loss is 0.1446777880191803\n",
            "The 427 batch, training loss is 0.12505878508090973\n",
            "The 428 batch, training loss is 0.059695981442928314\n",
            "The 429 batch, training loss is 0.06731030344963074\n",
            "The 430 batch, training loss is 0.14978145062923431\n",
            "The 431 batch, training loss is 0.18642862141132355\n",
            "The 432 batch, training loss is 0.0840374007821083\n",
            "The 433 batch, training loss is 0.05161702260375023\n",
            "The 434 batch, training loss is 0.11959090083837509\n",
            "The 435 batch, training loss is 0.12504051625728607\n",
            "The 436 batch, training loss is 0.05477089434862137\n",
            "The 437 batch, training loss is 0.12424059957265854\n",
            "The 438 batch, training loss is 0.041434355080127716\n",
            "The 439 batch, training loss is 0.16251236200332642\n",
            "The 440 batch, training loss is 0.16361886262893677\n",
            "The 441 batch, training loss is 0.12548111379146576\n",
            "The 442 batch, training loss is 0.16958025097846985\n",
            "The 443 batch, training loss is 0.15179526805877686\n",
            "The 444 batch, training loss is 0.11046462506055832\n",
            "The 445 batch, training loss is 0.06810357421636581\n",
            "The 446 batch, training loss is 0.07782059907913208\n",
            "The 447 batch, training loss is 0.15381421148777008\n",
            "The 448 batch, training loss is 0.06639915704727173\n",
            "The 449 batch, training loss is 0.1013103649020195\n",
            "The 450 batch, training loss is 0.09532324969768524\n",
            "The 451 batch, training loss is 0.3217715322971344\n",
            "The 452 batch, training loss is 0.13996385037899017\n",
            "The 453 batch, training loss is 0.10847997665405273\n",
            "The 454 batch, training loss is 0.10867542773485184\n",
            "The 455 batch, training loss is 0.10950934886932373\n",
            "The 456 batch, training loss is 0.04136236757040024\n",
            "The 457 batch, training loss is 0.06350968033075333\n",
            "The 458 batch, training loss is 0.1271476447582245\n",
            "The 459 batch, training loss is 0.1343778669834137\n",
            "The 460 batch, training loss is 0.3099617063999176\n",
            "The 461 batch, training loss is 0.27345794439315796\n",
            "The 462 batch, training loss is 0.14118768274784088\n",
            "The 463 batch, training loss is 0.3017430603504181\n",
            "The 464 batch, training loss is 0.18243180215358734\n",
            "The 465 batch, training loss is 0.06092403829097748\n",
            "The 466 batch, training loss is 0.14689753949642181\n",
            "The 467 batch, training loss is 0.07896208018064499\n",
            "The 468 batch, training loss is 0.16138507425785065\n",
            "The 469 batch, training loss is 0.15973925590515137\n",
            "The 470 batch, training loss is 0.14850331842899323\n",
            "The 471 batch, training loss is 0.10358335077762604\n",
            "The 472 batch, training loss is 0.1069611981511116\n",
            "The 473 batch, training loss is 0.24057219922542572\n",
            "The 474 batch, training loss is 0.1295856237411499\n",
            "The 475 batch, training loss is 0.1054578348994255\n",
            "The 476 batch, training loss is 0.2733563184738159\n",
            "The 477 batch, training loss is 0.13852816820144653\n",
            "The 478 batch, training loss is 0.08830555528402328\n",
            "The 479 batch, training loss is 0.07370567321777344\n",
            "The 480 batch, training loss is 0.06552459299564362\n",
            "The 481 batch, training loss is 0.21629424393177032\n",
            "The 482 batch, training loss is 0.09571874886751175\n",
            "The 483 batch, training loss is 0.15158259868621826\n",
            "The 484 batch, training loss is 0.18520183861255646\n",
            "The 485 batch, training loss is 0.10885768383741379\n",
            "The 486 batch, training loss is 0.18654265999794006\n",
            "The 487 batch, training loss is 0.13255655765533447\n",
            "The 488 batch, training loss is 0.0635322779417038\n",
            "The 489 batch, training loss is 0.0765405148267746\n",
            "The 490 batch, training loss is 0.1801607608795166\n",
            "The 491 batch, training loss is 0.14506112039089203\n",
            "The 492 batch, training loss is 0.16748180985450745\n",
            "The 493 batch, training loss is 0.14045025408267975\n",
            "The 494 batch, training loss is 0.2288329303264618\n",
            "The 495 batch, training loss is 0.07225500047206879\n",
            "The 496 batch, training loss is 0.12150336056947708\n",
            "The 497 batch, training loss is 0.14911498129367828\n",
            "The 498 batch, training loss is 0.12689608335494995\n",
            "The 499 batch, training loss is 0.16080935299396515\n",
            "The 500 batch, training loss is 0.05802982300519943\n",
            "The 501 batch, training loss is 0.0930723249912262\n",
            "The 502 batch, training loss is 0.08612028509378433\n",
            "The 503 batch, training loss is 0.23557063937187195\n",
            "The 504 batch, training loss is 0.1783791184425354\n",
            "The 505 batch, training loss is 0.04280790314078331\n",
            "The 506 batch, training loss is 0.1411641538143158\n",
            "The 507 batch, training loss is 0.07348491251468658\n",
            "The 508 batch, training loss is 0.039869774132966995\n",
            "The 509 batch, training loss is 0.09734288603067398\n",
            "The 510 batch, training loss is 0.06978203356266022\n",
            "The 511 batch, training loss is 0.053407538682222366\n",
            "The 512 batch, training loss is 0.0667719617486\n",
            "The 513 batch, training loss is 0.07230164110660553\n",
            "The 514 batch, training loss is 0.09429282695055008\n",
            "The 515 batch, training loss is 0.2184470146894455\n",
            "The 516 batch, training loss is 0.07984951138496399\n",
            "The 517 batch, training loss is 0.04194975271821022\n",
            "The 518 batch, training loss is 0.0633421391248703\n",
            "The 519 batch, training loss is 0.09390546381473541\n",
            "The 520 batch, training loss is 0.19126971065998077\n",
            "The 521 batch, training loss is 0.15188437700271606\n",
            "The 522 batch, training loss is 0.17534126341342926\n",
            "The 523 batch, training loss is 0.11853105574846268\n",
            "The 524 batch, training loss is 0.07977431267499924\n",
            "The 525 batch, training loss is 0.08288776129484177\n",
            "The 526 batch, training loss is 0.1314152181148529\n",
            "The 527 batch, training loss is 0.08208819478750229\n",
            "The 528 batch, training loss is 0.0911361500620842\n",
            "The 529 batch, training loss is 0.1450449824333191\n",
            "The 530 batch, training loss is 0.09840593487024307\n",
            "The 531 batch, training loss is 0.08100147545337677\n",
            "The 532 batch, training loss is 0.16823387145996094\n",
            "The 533 batch, training loss is 0.19636382162570953\n",
            "The 534 batch, training loss is 0.12087144702672958\n",
            "The 535 batch, training loss is 0.036340583115816116\n",
            "The 536 batch, training loss is 0.10276731103658676\n",
            "The 537 batch, training loss is 0.1426708698272705\n",
            "The 538 batch, training loss is 0.05896763131022453\n",
            "The 539 batch, training loss is 0.06763514131307602\n",
            "The 540 batch, training loss is 0.06141996756196022\n",
            "The 541 batch, training loss is 0.17763619124889374\n",
            "The 542 batch, training loss is 0.18559877574443817\n",
            "The 543 batch, training loss is 0.0495273731648922\n",
            "The 544 batch, training loss is 0.12271855771541595\n",
            "The 545 batch, training loss is 0.18574663996696472\n",
            "The 546 batch, training loss is 0.12129746377468109\n",
            "The 547 batch, training loss is 0.12507888674736023\n",
            "The 548 batch, training loss is 0.03749468922615051\n",
            "The 549 batch, training loss is 0.0789303332567215\n",
            "The 550 batch, training loss is 0.21798953413963318\n",
            "The 551 batch, training loss is 0.2602636516094208\n",
            "The 552 batch, training loss is 0.10130603611469269\n",
            "The 553 batch, training loss is 0.20333661139011383\n",
            "The 554 batch, training loss is 0.06838516891002655\n",
            "The 555 batch, training loss is 0.12943391501903534\n",
            "The 556 batch, training loss is 0.17221426963806152\n",
            "The 557 batch, training loss is 0.07968172430992126\n",
            "The 558 batch, training loss is 0.09654383361339569\n",
            "The 559 batch, training loss is 0.07783322036266327\n",
            "The 560 batch, training loss is 0.06638741493225098\n",
            "The 561 batch, training loss is 0.18255920708179474\n",
            "The 562 batch, training loss is 0.09489615261554718\n",
            "The 563 batch, training loss is 0.07122229784727097\n",
            "The 564 batch, training loss is 0.1194058284163475\n",
            "The 565 batch, training loss is 0.07956898957490921\n",
            "The 566 batch, training loss is 0.19195638597011566\n",
            "The 567 batch, training loss is 0.28272727131843567\n",
            "The 568 batch, training loss is 0.1107596829533577\n",
            "The 569 batch, training loss is 0.26955947279930115\n",
            "The 570 batch, training loss is 0.2965511679649353\n",
            "The 571 batch, training loss is 0.03935976326465607\n",
            "The 572 batch, training loss is 0.11385776847600937\n",
            "The 573 batch, training loss is 0.10196642577648163\n",
            "The 574 batch, training loss is 0.0493302159011364\n",
            "The 575 batch, training loss is 0.1489759236574173\n",
            "The 576 batch, training loss is 0.0780324712395668\n",
            "The 577 batch, training loss is 0.051750361919403076\n",
            "The 578 batch, training loss is 0.0986393392086029\n",
            "The 579 batch, training loss is 0.1849294751882553\n",
            "The 580 batch, training loss is 0.10739519447088242\n",
            "The 581 batch, training loss is 0.054620977491140366\n",
            "The 582 batch, training loss is 0.17903268337249756\n",
            "The 583 batch, training loss is 0.0510890819132328\n",
            "The 584 batch, training loss is 0.14684513211250305\n",
            "The 585 batch, training loss is 0.06640838086605072\n",
            "The 586 batch, training loss is 0.2035464346408844\n",
            "The 587 batch, training loss is 0.09026596695184708\n",
            "The 588 batch, training loss is 0.34437552094459534\n",
            "The 589 batch, training loss is 0.16031955182552338\n",
            "The 590 batch, training loss is 0.12003101408481598\n",
            "The 591 batch, training loss is 0.09595229476690292\n",
            "The 592 batch, training loss is 0.09410972148180008\n",
            "The 593 batch, training loss is 0.1245577484369278\n",
            "The 594 batch, training loss is 0.033198848366737366\n",
            "The 595 batch, training loss is 0.1622980386018753\n",
            "The 596 batch, training loss is 0.14328257739543915\n",
            "The 597 batch, training loss is 0.06816687434911728\n",
            "The 598 batch, training loss is 0.07317332178354263\n",
            "The 599 batch, training loss is 0.06298323720693588\n",
            "The 600 batch, training loss is 0.15308555960655212\n",
            "The 601 batch, training loss is 0.05577220767736435\n",
            "The 602 batch, training loss is 0.1478416621685028\n",
            "The 603 batch, training loss is 0.03678997978568077\n",
            "The 604 batch, training loss is 0.10165867209434509\n",
            "The 605 batch, training loss is 0.22724319994449615\n",
            "The 606 batch, training loss is 0.1434866487979889\n",
            "The 607 batch, training loss is 0.0834687203168869\n",
            "The 608 batch, training loss is 0.11950311064720154\n",
            "The 609 batch, training loss is 0.21537154912948608\n",
            "The 610 batch, training loss is 0.1681005358695984\n",
            "The 611 batch, training loss is 0.2265111356973648\n",
            "The 612 batch, training loss is 0.20163069665431976\n",
            "The 613 batch, training loss is 0.07960664480924606\n",
            "The 614 batch, training loss is 0.10353848338127136\n",
            "The 615 batch, training loss is 0.14881715178489685\n",
            "The 616 batch, training loss is 0.11929735541343689\n",
            "The 617 batch, training loss is 0.08154942095279694\n",
            "The 618 batch, training loss is 0.05556977167725563\n",
            "The 619 batch, training loss is 0.11597734689712524\n",
            "The 620 batch, training loss is 0.09282508492469788\n",
            "The 621 batch, training loss is 0.2121819257736206\n",
            "The 622 batch, training loss is 0.14398427307605743\n",
            "The 623 batch, training loss is 0.1423056423664093\n",
            "The 624 batch, training loss is 0.1832771599292755\n",
            "The 625 batch, training loss is 0.04561919718980789\n",
            "The 626 batch, training loss is 0.11894465982913971\n",
            "The 627 batch, training loss is 0.06277742236852646\n",
            "The 628 batch, training loss is 0.09890025109052658\n",
            "The 629 batch, training loss is 0.23297080397605896\n",
            "The 630 batch, training loss is 0.25028133392333984\n",
            "The 631 batch, training loss is 0.05609351396560669\n",
            "The 632 batch, training loss is 0.036173973232507706\n",
            "The 633 batch, training loss is 0.24043743312358856\n",
            "The 634 batch, training loss is 0.14875705540180206\n",
            "The 635 batch, training loss is 0.12592566013336182\n",
            "The 636 batch, training loss is 0.09436087310314178\n",
            "The 637 batch, training loss is 0.13860751688480377\n",
            "The 638 batch, training loss is 0.10927363485097885\n",
            "The 639 batch, training loss is 0.12493017315864563\n",
            "The 640 batch, training loss is 0.1434202939271927\n",
            "The 641 batch, training loss is 0.15798845887184143\n",
            "The 642 batch, training loss is 0.0940769612789154\n",
            "The 643 batch, training loss is 0.14438651502132416\n",
            "The 644 batch, training loss is 0.25949445366859436\n",
            "The 645 batch, training loss is 0.1577700674533844\n",
            "The 646 batch, training loss is 0.10224825888872147\n",
            "The 647 batch, training loss is 0.059772174805402756\n",
            "The 648 batch, training loss is 0.12967026233673096\n",
            "The 649 batch, training loss is 0.1136900782585144\n",
            "The 650 batch, training loss is 0.3578505516052246\n",
            "The 651 batch, training loss is 0.10479473322629929\n",
            "The 652 batch, training loss is 0.03817950561642647\n",
            "The 653 batch, training loss is 0.12162672728300095\n",
            "The 654 batch, training loss is 0.11723990738391876\n",
            "The 655 batch, training loss is 0.06875838339328766\n",
            "The 656 batch, training loss is 0.09100712090730667\n",
            "The 657 batch, training loss is 0.022448424249887466\n",
            "The 658 batch, training loss is 0.2014884352684021\n",
            "The 659 batch, training loss is 0.08053573220968246\n",
            "The 660 batch, training loss is 0.14906956255435944\n",
            "The 661 batch, training loss is 0.11490319669246674\n",
            "The 662 batch, training loss is 0.15706707537174225\n",
            "The 663 batch, training loss is 0.20033235847949982\n",
            "The 664 batch, training loss is 0.19759026169776917\n",
            "The 665 batch, training loss is 0.16549289226531982\n",
            "The 666 batch, training loss is 0.14994068443775177\n",
            "The 667 batch, training loss is 0.0684962347149849\n",
            "The 668 batch, training loss is 0.06594917178153992\n",
            "The 669 batch, training loss is 0.07189181447029114\n",
            "The 670 batch, training loss is 0.15116813778877258\n",
            "The 671 batch, training loss is 0.05795431509613991\n",
            "The 672 batch, training loss is 0.1424979269504547\n",
            "The 673 batch, training loss is 0.12072239816188812\n",
            "The 674 batch, training loss is 0.07957905530929565\n",
            "The 675 batch, training loss is 0.2806437313556671\n",
            "The 676 batch, training loss is 0.13682222366333008\n",
            "The 677 batch, training loss is 0.1551147848367691\n",
            "The 678 batch, training loss is 0.12656185030937195\n",
            "The 679 batch, training loss is 0.09668316692113876\n",
            "The 680 batch, training loss is 0.08932872861623764\n",
            "The 681 batch, training loss is 0.11804899573326111\n",
            "The 682 batch, training loss is 0.1054203063249588\n",
            "The 683 batch, training loss is 0.11214516311883926\n",
            "The 684 batch, training loss is 0.26462337374687195\n",
            "The 685 batch, training loss is 0.07199349254369736\n",
            "The 686 batch, training loss is 0.13021597266197205\n",
            "The 687 batch, training loss is 0.08091728389263153\n",
            "The 688 batch, training loss is 0.07367193698883057\n",
            "The 689 batch, training loss is 0.15715017914772034\n",
            "The 690 batch, training loss is 0.07842276990413666\n",
            "The 691 batch, training loss is 0.06144680082798004\n",
            "The 692 batch, training loss is 0.17740997672080994\n",
            "The 693 batch, training loss is 0.28240567445755005\n",
            "The 694 batch, training loss is 0.08807189762592316\n",
            "The 695 batch, training loss is 0.10356001555919647\n",
            "The 696 batch, training loss is 0.1304045170545578\n",
            "The 697 batch, training loss is 0.12200707942247391\n",
            "The 698 batch, training loss is 0.13404539227485657\n",
            "The 699 batch, training loss is 0.11469852924346924\n",
            "The 700 batch, training loss is 0.2712419033050537\n",
            "The 701 batch, training loss is 0.14580267667770386\n",
            "The 702 batch, training loss is 0.07635753601789474\n",
            "The 703 batch, training loss is 0.24434582889080048\n",
            "The 704 batch, training loss is 0.19545523822307587\n",
            "The 705 batch, training loss is 0.05214935913681984\n",
            "The 706 batch, training loss is 0.05811242014169693\n",
            "The 707 batch, training loss is 0.05322451516985893\n",
            "The 708 batch, training loss is 0.06270048767328262\n",
            "The 709 batch, training loss is 0.13460083305835724\n",
            "The 710 batch, training loss is 0.16311754286289215\n",
            "The 711 batch, training loss is 0.0969589501619339\n",
            "The 712 batch, training loss is 0.06632301211357117\n",
            "The 713 batch, training loss is 0.026441209018230438\n",
            "The 714 batch, training loss is 0.06024739518761635\n",
            "The 715 batch, training loss is 0.10504387319087982\n",
            "The 716 batch, training loss is 0.14864255487918854\n",
            "The 717 batch, training loss is 0.1411796510219574\n",
            "The 718 batch, training loss is 0.11831330507993698\n",
            "The 719 batch, training loss is 0.0610860213637352\n",
            "The 720 batch, training loss is 0.03617291525006294\n",
            "The 721 batch, training loss is 0.13531769812107086\n",
            "The 722 batch, training loss is 0.1284809708595276\n",
            "The 723 batch, training loss is 0.13029742240905762\n",
            "The 724 batch, training loss is 0.05951036140322685\n",
            "The 725 batch, training loss is 0.05734403431415558\n",
            "The 726 batch, training loss is 0.06922270357608795\n",
            "The 727 batch, training loss is 0.2104770988225937\n",
            "The 728 batch, training loss is 0.1292499452829361\n",
            "The 729 batch, training loss is 0.24033860862255096\n",
            "The 730 batch, training loss is 0.07956453412771225\n",
            "The 731 batch, training loss is 0.10765817761421204\n",
            "The 732 batch, training loss is 0.16842888295650482\n",
            "The 733 batch, training loss is 0.0979483500123024\n",
            "The 734 batch, training loss is 0.2290273904800415\n",
            "The 735 batch, training loss is 0.10343580693006516\n",
            "The 736 batch, training loss is 0.10858306288719177\n",
            "The 737 batch, training loss is 0.10444192588329315\n",
            "The 738 batch, training loss is 0.057733289897441864\n",
            "The 739 batch, training loss is 0.12545906007289886\n",
            "The 740 batch, training loss is 0.07083533704280853\n",
            "The 741 batch, training loss is 0.080008365213871\n",
            "The 742 batch, training loss is 0.16355809569358826\n",
            "The 743 batch, training loss is 0.07187999784946442\n",
            "The 744 batch, training loss is 0.15141914784908295\n",
            "The 745 batch, training loss is 0.040400248020887375\n",
            "The 746 batch, training loss is 0.05179106444120407\n",
            "The 747 batch, training loss is 0.05622900649905205\n",
            "The 748 batch, training loss is 0.0772135779261589\n",
            "The 749 batch, training loss is 0.10122706741094589\n",
            "The 750 batch, training loss is 0.09903454780578613\n",
            "The 751 batch, training loss is 0.07015782594680786\n",
            "The 752 batch, training loss is 0.04007498547434807\n",
            "The 753 batch, training loss is 0.04540756717324257\n",
            "The 754 batch, training loss is 0.10342936962842941\n",
            "The 755 batch, training loss is 0.15739300847053528\n",
            "The 756 batch, training loss is 0.2592669725418091\n",
            "The 757 batch, training loss is 0.0531647615134716\n",
            "The 758 batch, training loss is 0.14557835459709167\n",
            "The 759 batch, training loss is 0.08465559780597687\n",
            "The 760 batch, training loss is 0.07872603088617325\n",
            "The 761 batch, training loss is 0.12015653401613235\n",
            "The 762 batch, training loss is 0.13634347915649414\n",
            "The 763 batch, training loss is 0.0917147770524025\n",
            "The 764 batch, training loss is 0.04218330979347229\n",
            "The 765 batch, training loss is 0.05021722987294197\n",
            "The 766 batch, training loss is 0.1416683793067932\n",
            "The 767 batch, training loss is 0.17272788286209106\n",
            "The 768 batch, training loss is 0.159567192196846\n",
            "The 769 batch, training loss is 0.04270100221037865\n",
            "The 770 batch, training loss is 0.07408680021762848\n",
            "The 771 batch, training loss is 0.09506720304489136\n",
            "The 772 batch, training loss is 0.10522660613059998\n",
            "The 773 batch, training loss is 0.10742916166782379\n",
            "The 774 batch, training loss is 0.1474747359752655\n",
            "The 775 batch, training loss is 0.07820668071508408\n",
            "The 776 batch, training loss is 0.06796899437904358\n",
            "The 777 batch, training loss is 0.20456376671791077\n",
            "The 778 batch, training loss is 0.04834303259849548\n",
            "The 779 batch, training loss is 0.06776390224695206\n",
            "The 780 batch, training loss is 0.10892121493816376\n",
            "The 781 batch, training loss is 0.08952715247869492\n",
            "The 782 batch, training loss is 0.13308608531951904\n",
            "The 783 batch, training loss is 0.06701917201280594\n",
            "The 784 batch, training loss is 0.04638410732150078\n",
            "The 785 batch, training loss is 0.21660181879997253\n",
            "The 786 batch, training loss is 0.15041078627109528\n",
            "The 787 batch, training loss is 0.05447350814938545\n",
            "The 788 batch, training loss is 0.30831778049468994\n",
            "The 789 batch, training loss is 0.17129157483577728\n",
            "The 790 batch, training loss is 0.1386052817106247\n",
            "The 791 batch, training loss is 0.0745600014925003\n",
            "The 792 batch, training loss is 0.10036274045705795\n",
            "The 793 batch, training loss is 0.12775447964668274\n",
            "The 794 batch, training loss is 0.1643235832452774\n",
            "The 795 batch, training loss is 0.15281891822814941\n",
            "The 796 batch, training loss is 0.04007340222597122\n",
            "The 797 batch, training loss is 0.11769703030586243\n",
            "The 798 batch, training loss is 0.15783502161502838\n",
            "The 799 batch, training loss is 0.11677198112010956\n",
            "The 800 batch, training loss is 0.2343568652868271\n",
            "The 801 batch, training loss is 0.19842448830604553\n",
            "The 802 batch, training loss is 0.077723428606987\n",
            "The 803 batch, training loss is 0.09846659004688263\n",
            "The 804 batch, training loss is 0.18504293262958527\n",
            "The 805 batch, training loss is 0.08731325715780258\n",
            "The 806 batch, training loss is 0.08389182388782501\n",
            "The 807 batch, training loss is 0.11344212293624878\n",
            "The 808 batch, training loss is 0.07236982136964798\n",
            "The 809 batch, training loss is 0.09991204738616943\n",
            "The 810 batch, training loss is 0.0717906728386879\n",
            "The 811 batch, training loss is 0.06080286577343941\n",
            "The 812 batch, training loss is 0.10029998421669006\n",
            "The 813 batch, training loss is 0.0646083801984787\n",
            "The 814 batch, training loss is 0.15741313993930817\n",
            "The 815 batch, training loss is 0.05184235796332359\n",
            "The 816 batch, training loss is 0.06274960190057755\n",
            "The 817 batch, training loss is 0.11758559197187424\n",
            "The 818 batch, training loss is 0.09074980765581131\n",
            "The 819 batch, training loss is 0.3091214597225189\n",
            "The 820 batch, training loss is 0.09051571041345596\n",
            "The 821 batch, training loss is 0.06253557652235031\n",
            "The 822 batch, training loss is 0.06451216340065002\n",
            "The 823 batch, training loss is 0.05749651417136192\n",
            "The 824 batch, training loss is 0.04470156878232956\n",
            "The 825 batch, training loss is 0.05849422514438629\n",
            "The 826 batch, training loss is 0.11892081797122955\n",
            "The 827 batch, training loss is 0.042162902653217316\n",
            "The 828 batch, training loss is 0.06669845432043076\n",
            "The 829 batch, training loss is 0.049944791942834854\n",
            "The 830 batch, training loss is 0.1868596374988556\n",
            "The 831 batch, training loss is 0.1822369247674942\n",
            "The 832 batch, training loss is 0.11818942427635193\n",
            "The 833 batch, training loss is 0.099161297082901\n",
            "The 834 batch, training loss is 0.38144248723983765\n",
            "The 835 batch, training loss is 0.08109709620475769\n",
            "The 836 batch, training loss is 0.09100913256406784\n",
            "The 837 batch, training loss is 0.14446581900119781\n",
            "The 838 batch, training loss is 0.10666586458683014\n",
            "The 839 batch, training loss is 0.1612958163022995\n",
            "The 840 batch, training loss is 0.05172184109687805\n",
            "The 841 batch, training loss is 0.10700713098049164\n",
            "The 842 batch, training loss is 0.11324246972799301\n",
            "The 843 batch, training loss is 0.055429596453905106\n",
            "The 844 batch, training loss is 0.12542076408863068\n",
            "The 845 batch, training loss is 0.0653914362192154\n",
            "The 846 batch, training loss is 0.0476870983839035\n",
            "The 847 batch, training loss is 0.06718699634075165\n",
            "The 848 batch, training loss is 0.06228502467274666\n",
            "The 849 batch, training loss is 0.07024648785591125\n",
            "The 850 batch, training loss is 0.13544657826423645\n",
            "The 851 batch, training loss is 0.18398649990558624\n",
            "The 852 batch, training loss is 0.174557626247406\n",
            "The 853 batch, training loss is 0.11596395075321198\n",
            "The 854 batch, training loss is 0.09786131978034973\n",
            "The 855 batch, training loss is 0.15818066895008087\n",
            "The 856 batch, training loss is 0.10605455935001373\n",
            "The 857 batch, training loss is 0.1369941383600235\n",
            "The 858 batch, training loss is 0.14539843797683716\n",
            "The 859 batch, training loss is 0.058079950511455536\n",
            "The 860 batch, training loss is 0.14584587514400482\n",
            "The 861 batch, training loss is 0.06972048431634903\n",
            "The 862 batch, training loss is 0.12258629500865936\n",
            "The 863 batch, training loss is 0.11667811870574951\n",
            "The 864 batch, training loss is 0.05769171565771103\n",
            "The 865 batch, training loss is 0.08121971040964127\n",
            "The 866 batch, training loss is 0.11311398446559906\n",
            "The 867 batch, training loss is 0.08850317448377609\n",
            "The 868 batch, training loss is 0.2608890235424042\n",
            "The 869 batch, training loss is 0.11477882415056229\n",
            "The 870 batch, training loss is 0.07985804975032806\n",
            "The 871 batch, training loss is 0.06203180178999901\n",
            "The 872 batch, training loss is 0.07832535356283188\n",
            "The 873 batch, training loss is 0.1647719293832779\n",
            "The 874 batch, training loss is 0.2019275426864624\n",
            "The 875 batch, training loss is 0.026777319610118866\n",
            "The 876 batch, training loss is 0.16141057014465332\n",
            "The 877 batch, training loss is 0.13354122638702393\n",
            "The 878 batch, training loss is 0.048471178859472275\n",
            "The 879 batch, training loss is 0.10313146561384201\n",
            "The 880 batch, training loss is 0.11434227973222733\n",
            "The 881 batch, training loss is 0.08661933243274689\n",
            "The 882 batch, training loss is 0.2811512351036072\n",
            "The 883 batch, training loss is 0.10279019176959991\n",
            "The 884 batch, training loss is 0.11343742907047272\n",
            "The 885 batch, training loss is 0.12042306363582611\n",
            "The 886 batch, training loss is 0.15479446947574615\n",
            "The 887 batch, training loss is 0.13820818066596985\n",
            "The 888 batch, training loss is 0.1251680999994278\n",
            "The 889 batch, training loss is 0.054538775235414505\n",
            "The 890 batch, training loss is 0.13411244750022888\n",
            "The 891 batch, training loss is 0.07811424881219864\n",
            "The 892 batch, training loss is 0.06797759234905243\n",
            "The 893 batch, training loss is 0.20473067462444305\n",
            "The 894 batch, training loss is 0.15995150804519653\n",
            "The 895 batch, training loss is 0.10235080122947693\n",
            "The 896 batch, training loss is 0.09206774085760117\n",
            "The 897 batch, training loss is 0.15692387521266937\n",
            "The 898 batch, training loss is 0.13255684077739716\n",
            "The 899 batch, training loss is 0.01699245348572731\n",
            "The 900 batch, training loss is 0.12792523205280304\n",
            "The 901 batch, training loss is 0.15454617142677307\n",
            "The 902 batch, training loss is 0.08129838109016418\n",
            "The 903 batch, training loss is 0.07039213925600052\n",
            "The 904 batch, training loss is 0.12529078125953674\n",
            "The 905 batch, training loss is 0.11259623616933823\n",
            "The 906 batch, training loss is 0.07100733369588852\n",
            "The 907 batch, training loss is 0.1031905934214592\n",
            "The 908 batch, training loss is 0.13984504342079163\n",
            "The 909 batch, training loss is 0.027060814201831818\n",
            "The 910 batch, training loss is 0.15461376309394836\n",
            "The 911 batch, training loss is 0.11931369453668594\n",
            "The 912 batch, training loss is 0.06903086602687836\n",
            "The 913 batch, training loss is 0.062499526888132095\n",
            "The 914 batch, training loss is 0.10343832522630692\n",
            "The 915 batch, training loss is 0.16724218428134918\n",
            "The 916 batch, training loss is 0.07480902224779129\n",
            "The 917 batch, training loss is 0.08848898857831955\n",
            "The 918 batch, training loss is 0.05449733883142471\n",
            "The 919 batch, training loss is 0.1781284660100937\n",
            "The 920 batch, training loss is 0.049299683421850204\n",
            "The 921 batch, training loss is 0.05869627743959427\n",
            "The 922 batch, training loss is 0.06545673310756683\n",
            "The 923 batch, training loss is 0.1053750142455101\n",
            "The 924 batch, training loss is 0.14600501954555511\n",
            "The 925 batch, training loss is 0.09328062832355499\n",
            "The 926 batch, training loss is 0.07094011455774307\n",
            "The 927 batch, training loss is 0.049462925642728806\n",
            "The 928 batch, training loss is 0.14533740282058716\n",
            "The 929 batch, training loss is 0.11434447765350342\n",
            "The 930 batch, training loss is 0.09114328771829605\n",
            "The 931 batch, training loss is 0.07663853466510773\n",
            "The 932 batch, training loss is 0.14681123197078705\n",
            "The 933 batch, training loss is 0.12299476563930511\n",
            "The 934 batch, training loss is 0.09283477067947388\n",
            "The 935 batch, training loss is 0.13858945667743683\n",
            "The 936 batch, training loss is 0.10792171210050583\n",
            "The 937 batch, training loss is 0.17972110211849213\n",
            "The 9 epoch, training loss is 0.17972110211849213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss over epoch\n",
        "plt.plot(losses)\n",
        "plt.title(\"training loss over time\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "BarSPTPmnmqu",
        "outputId": "c6f2955b-7a85-4a78-d676-4a76d0ca7408"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training procedure"
      ],
      "metadata": {
        "id": "o2CT-KZiJuoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test\n",
        "\n",
        "Notice:\n",
        "*   You'd need to turn on eval mode first (model.eval()) and then turn off when you finish cal (model.train())\n",
        "*   No gradient descent in the Testing\n",
        "```\n",
        "with torch.no_grad():\n",
        "```\n",
        "*   No \"enumerate\" bc you don't need batch_idx\n",
        "\n"
      ],
      "metadata": {
        "id": "cZc6SdpxJvOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import reduce"
      ],
      "metadata": {
        "id": "jPwZaAdB0fwI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(x, model):\n",
        "  overall = 0 # Number of model making predictions\n",
        "  accurate = 0 # Number of model making accurate predictions\n",
        "  model.eval() # Set up the model in eval mode (where things such as dropout and normalization may behave differently.)\n",
        "  with torch.no_grad():\n",
        "    for data, label in x: # Notice that x is a dataloader but of tensor dtype\n",
        "      data = rearrange(data,'b c h w -> (b c) (h w)') # x is flattened. [batch, all_else]\n",
        "      logits = model(data)  # returning non-normalized probabilities\n",
        "      # max_values = reduce(logits, \"n c ->n\", \"max\") # Notice that this einops method does not work here bc you are looking for the \"labels\", which are not returned (but only max values)\n",
        "      _, prediction = logits.max(1) # You actually want the max value of 2nd dim of logits (batch_size, classes): max of each item (max out of all 10 classes).\n",
        "      overall += prediction.size(0) # for each batch, you add in the number of \"batch_size\" predictions to denominator\n",
        "      accurate += (prediction==label).sum() #  (prediction=label) -> produce boolean; .sum() turn boolean to 1 (yes) and 0 (no) and sum up to a scalar.\n",
        "  model.train()\n",
        "  accuracy = float(accurate/overall) # it seems originally they were tensor before float()\n",
        "  print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9tbCcCoLdd82"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the test function\n",
        "test(test_dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H77gGDd-vmT",
        "outputId": "892ec3a3-1179-4fe0-b3d5-04b68484e5d4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 96.23%\n"
          ]
        }
      ]
    }
  ]
}
